{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01100277-afdf-493f-a01f-3c494dd91eb5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A Comprehensive Introduction to PyTorch**\n",
    "\n",
    "PyTorch is an open-source machine learning library, primarily developed by Facebook's AI Research lab (FAIR). It has rapidly gained popularity in both the research community and industry for its flexibility, Python-friendly interface, and powerful capabilities. At its core, PyTorch serves two main purposes:\n",
    "\n",
    "1.  **A NumPy-like library with strong GPU acceleration:** It provides multi-dimensional arrays called Tensors (similar to NumPy's ndarrays) that can be processed on CPUs or NVIDIA GPUs, enabling massive speedups for numerical computations.\n",
    "2.  **A deep learning platform built for flexibility and speed:** It offers a rich set of tools and modules to define, train, and deploy neural networks and other machine learning models with a high degree of control.\n",
    "\n",
    "**Key Characteristics & Philosophy:**\n",
    "\n",
    "* **Pythonic:** PyTorch is designed to feel native to Python. Its syntax and structure are intuitive for Python developers, making the learning curve smoother. You can use standard Python control flow, data structures, and debugging tools.\n",
    "* **Dynamic Computation Graphs (Define-by-Run):** This is a hallmark of PyTorch. Unlike static graph frameworks (like older versions of TensorFlow) where you define the entire computation graph upfront, PyTorch builds the graph dynamically as operations are executed.\n",
    "    * **Benefits:**\n",
    "        * **Easier Debugging:** You can use standard Python debuggers (e.g., `pdb`) to inspect values and a_grad_fn flow at any point.\n",
    "        * **Flexibility:** Allows for dynamic model architectures where the structure of the network can change during runtime (e.g., Recurrent Neural Networks with varying sequence lengths, conditional computations).\n",
    "        * **Natural Control Flow:** Python's `if` statements, `for` loops, and other control structures can be used naturally within the model definition and execution.\n",
    "* **Imperative Programming Style:** Operations are executed immediately, and their results are available right away. This makes the code more straightforward to write, understand, and debug.\n",
    "* **Strong GPU Acceleration:** PyTorch provides seamless integration with NVIDIA GPUs using CUDA, allowing for significant acceleration of tensor operations and model training.\n",
    "* **Extensive Ecosystem and Community:** PyTorch boasts a vibrant community and a rich ecosystem of supporting libraries and tools for various domains like computer vision (`torchvision`), audio processing (`torchaudio`), and natural language processing (e.g., Hugging Face Transformers).\n",
    "* **Transition to Production:** While initially known more for research, PyTorch has made significant strides in production deployment with tools like TorchScript (for creating serializable and optimizable models) and support for ONNX (Open Neural Network Exchange) for interoperability with other frameworks.\n",
    "\n",
    "---\n",
    "\n",
    "**Core Components of PyTorch:**\n",
    "\n",
    "Let's delve into the fundamental building blocks of PyTorch:\n",
    "\n",
    "**1. Tensors (`torch.Tensor`)**\n",
    "\n",
    "Tensors are the central data structure in PyTorch, analogous to NumPy's `ndarray`, but with added capabilities for GPU computation and automatic differentiation.\n",
    "\n",
    "* **Definition:** Multi-dimensional arrays that can hold numerical data of various types.\n",
    "* **Creation:**\n",
    "    * From Python lists or NumPy arrays: `torch.tensor()`, `torch.from_numpy()`\n",
    "    * Specific values: `torch.zeros()`, `torch.ones()`, `torch.eye()`\n",
    "    * Random values: `torch.rand()`, `torch.randn()`\n",
    "    * Ranges: `torch.arange()`, `torch.linspace()`\n",
    "* **Data Types:** PyTorch supports various numerical data types, such as `torch.float32` (32-bit floating point, common default), `torch.float64` (double precision), `torch.int64` (long integer), `torch.bool`, etc. You can specify the `dtype` during creation or cast tensors using methods like `.float()`, `.long()`.\n",
    "* **Attributes:**\n",
    "    * `shape` or `size()`: Dimensions of the tensor.\n",
    "    * `dtype`: Data type of the elements.\n",
    "    * `device`: The device (CPU or GPU) where the tensor's data is stored.\n",
    "* **Operations:** PyTorch offers a vast range of operations on tensors:\n",
    "    * **Mathematical:** Element-wise addition, subtraction, multiplication, division. Matrix operations like dot products (`torch.dot`), matrix multiplication (`torch.matmul` or `@` operator), transposing (`.T` or `torch.transpose`).\n",
    "    * **Reshaping:** `view()`, `reshape()`, `squeeze()`, `unsqueeze()`.\n",
    "    * **Indexing and Slicing:** Similar to NumPy arrays, allowing for powerful data manipulation.\n",
    "    * **Reduction Operations:** `sum()`, `mean()`, `std()`, `max()`, `min()`, `argmax()`, `argmin()`.\n",
    "* **GPU Acceleration:** Tensors can be effortlessly moved between CPU and GPU.\n",
    "    * `tensor.to(device)` (where `device` can be `torch.device(\"cuda\")` or `torch.device(\"cpu\")`)\n",
    "    * `tensor.cuda()` (shorthand for moving to the default GPU)\n",
    "    * `tensor.cpu()` (shorthand for moving to CPU)\n",
    "* **NumPy Bridge:** PyTorch tensors and NumPy arrays can be converted to each other efficiently (often sharing the underlying memory if on CPU, which means changes in one affect the other).\n",
    "    * `tensor.numpy()`: Converts a CPU tensor to a NumPy array.\n",
    "    * `torch.from_numpy(ndarray)`: Converts a NumPy array to a PyTorch tensor.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Autograd: Automatic Differentiation (`torch.autograd`)**\n",
    "\n",
    "This is PyTorch's automatic differentiation engine, crucial for training neural networks via backpropagation.\n",
    "\n",
    "* **Concept:** `autograd` tracks operations performed on tensors and automatically computes gradients of a scalar value (typically the loss) with respect to any tensors that were involved in its computation and require gradients.\n",
    "* **`requires_grad` Attribute:**\n",
    "    * If a tensor has `requires_grad=True`, PyTorch will track all operations involving it to build a backward computation graph.\n",
    "    * Tensors created by the user have `requires_grad=False` by default. Learnable parameters of a neural network typically have `requires_grad=True`.\n",
    "    * You can set it during creation: `torch.randn(3, 3, requires_grad=True)`.\n",
    "    * Or in-place: `tensor.requires_grad_(True)`.\n",
    "* **`grad_fn` Attribute:**\n",
    "    * When an operation is performed on tensors with `requires_grad=True`, the resulting tensor will have a `grad_fn` attribute. This attribute references the function that created the tensor and holds a reference to the inputs, forming the backward graph. Leaf nodes (tensors created by the user) have `grad_fn=None`.\n",
    "* **`.backward()` Method:**\n",
    "    * When you have a scalar tensor (e.g., the loss from your model), you can call `.backward()` on it.\n",
    "    * This triggers `autograd` to compute the gradients of this scalar with respect to all tensors in the computation graph that have `requires_grad=True`.\n",
    "    * Gradients are accumulated by default (i.e., added to any existing gradients). You typically need to zero out gradients before each backward pass in a training loop using `optimizer.zero_grad()`.\n",
    "* **`.grad` Attribute:**\n",
    "    * After `.backward()` is called, the computed gradients are stored in the `.grad` attribute of the respective leaf tensors (those with `requires_grad=True` that were part of the computation).\n",
    "* **Context Managers for Gradient Calculation:**\n",
    "    * `torch.no_grad()`: A context manager used to temporarily disable gradient tracking. This is useful during inference (when you don't need gradients) or when updating model parameters directly, as it reduces memory consumption and speeds up computations.\n",
    "    * `torch.enable_grad()`: A context manager to re-enable gradient tracking if it was disabled.\n",
    "* **`detach()` Method:**\n",
    "    * `tensor.detach()` creates a new tensor that shares the same data but is detached from the current computation graph (it won't require gradients, and no operations on it will be tracked).\n",
    "\n",
    "---\n",
    "\n",
    "**3. Neural Network Module (`torch.nn`)**\n",
    "\n",
    "`torch.nn` is the cornerstone for building neural networks in PyTorch. It provides a set of powerful tools, including a base class for defining custom models and a wide array of pre-built layers, loss functions, and utility functions.\n",
    "\n",
    "* **`nn.Module`:**\n",
    "    * The base class for all neural network modules (layers, or entire models).\n",
    "    * You create custom models by subclassing `nn.Module`.\n",
    "    * **Key methods to override:**\n",
    "        * `__init__(self, ...)`: This is where you define and initialize the layers, parameters, and other components of your model. You must call `super().__init__()`.\n",
    "        * `forward(self, input, ...)`: This method defines the forward pass of your model. It takes input tensor(s) and returns output tensor(s). The actual computation graph is built dynamically when this method is called.\n",
    "    * `nn.Module` automatically tracks its learnable parameters (tensors created with `requires_grad=True` and registered as attributes, or parameters of sub-modules). You can access them via `model.parameters()` or `model.named_parameters()`.\n",
    "    * Modules can contain other `nn.Module` instances, allowing for easy nesting and modular design of complex architectures.\n",
    "\n",
    "* **Common Layers (found in `torch.nn`):**\n",
    "    * **Linear Layers:** `nn.Linear(in_features, out_features, bias=True)` – applies a linear transformation ($y = xA^T + b$).\n",
    "    * **Convolutional Layers:** For processing grid-like data (e.g., images).\n",
    "        * `nn.Conv1d`, `nn.Conv2d`, `nn.Conv3d`\n",
    "    * **Recurrent Layers:** For processing sequential data.\n",
    "        * `nn.RNN`, `nn.LSTM` (Long Short-Term Memory), `nn.GRU` (Gated Recurrent Unit)\n",
    "    * **Activation Functions:** Introduce non-linearity into the model.\n",
    "        * `nn.ReLU`, `nn.LeakyReLU`, `nn.Sigmoid`, `nn.Tanh`, `nn.Softmax`, `nn.LogSoftmax`, `nn.GELU`, etc. These are often used as functions from `torch.nn.functional` as well.\n",
    "    * **Pooling Layers:** Reduce spatial dimensions.\n",
    "        * `nn.MaxPool1d`, `nn.MaxPool2d`, `nn.AvgPool2d`, `nn.AdaptiveAvgPool2d`\n",
    "    * **Normalization Layers:** Stabilize training and improve generalization.\n",
    "        * `nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.LayerNorm`, `nn.GroupNorm`\n",
    "    * **Dropout Layers:** Regularization technique to prevent overfitting.\n",
    "        * `nn.Dropout`, `nn.Dropout2d`\n",
    "    * **Embedding Layers:** `nn.Embedding(num_embeddings, embedding_dim)` – for representing categorical data (e.g., words) as dense vectors.\n",
    "\n",
    "* **Loss Functions (also typically `nn.Module` subclasses):**\n",
    "    * Used to measure the discrepancy between the model's predictions and the true targets.\n",
    "    * `nn.MSELoss()`: Mean Squared Error, for regression tasks.\n",
    "    * `nn.CrossEntropyLoss()`: Combines `nn.LogSoftmax` and `nn.NLLLoss`. Commonly used for multi-class classification.\n",
    "    * `nn.BCELoss()`: Binary Cross Entropy, for binary classification (output usually passed through a sigmoid).\n",
    "    * `nn.BCEWithLogitsLoss()`: Combines a Sigmoid layer and BCELoss in one more numerically stable step.\n",
    "    * `nn.NLLLoss()`: Negative Log Likelihood Loss.\n",
    "    * Many others like `L1Loss`, `SmoothL1Loss`, `KLDivLoss`, etc.\n",
    "\n",
    "* **Containers:**\n",
    "    * `nn.Sequential(*args)`: A container that stacks modules in the order they are passed to the constructor. The input is passed sequentially through each module.\n",
    "    * `nn.ModuleList([modules])`: Holds submodules in a Python list. Useful when you need an iterable list of modules, and ensures modules are properly registered.\n",
    "    * `nn.ModuleDict({name: module})`: Holds submodules in a Python dictionary. Useful for organizing modules with string keys.\n",
    "\n",
    "* **`torch.nn.functional` (often imported as `F`):**\n",
    "    * This module contains functional versions of many layers and operations (e.g., `F.relu`, `F.conv2d`, `F.cross_entropy`). These are stateless functions, meaning they don't hold any parameters themselves. `nn.Module` layers often wrap these functional versions and manage their learnable parameters.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Optimizers (`torch.optim`)**\n",
    "\n",
    "Optimizers are algorithms used to update the learnable parameters (weights and biases) of your model in order to minimize the loss function.\n",
    "\n",
    "* **How they work:** They use the gradients computed by `autograd` to adjust the parameters in the direction that reduces the loss.\n",
    "* **Initialization:** You typically initialize an optimizer by providing it with the model's parameters (e.g., `model.parameters()`) and a learning rate (`lr`).\n",
    "* **Common Optimizers:**\n",
    "    * `optim.SGD(params, lr, momentum=0, weight_decay=0)`: Stochastic Gradient Descent, optionally with momentum and weight decay (L2 regularization).\n",
    "    * `optim.Adam(params, lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)`: Adaptive Moment Estimation, a popular and often effective optimizer.\n",
    "    * `optim.AdamW(params, lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)`: Adam with decoupled weight decay, often preferred over standard Adam's L2 regularization.\n",
    "    * `optim.RMSprop(params, lr, alpha=0.99, weight_decay=0)`\n",
    "    * `optim.Adagrad(params, lr, weight_decay=0)`\n",
    "* **Key Optimizer Methods:**\n",
    "    * `optimizer.zero_grad()`: Clears the gradients of all optimized parameters. This must be called before `loss.backward()` in each training iteration because gradients accumulate by default.\n",
    "    * `optimizer.step()`: Updates the parameters based on their accumulated gradients and the chosen optimization algorithm. This is called after `loss.backward()`.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Data Handling (`torch.utils.data`)**\n",
    "\n",
    "PyTorch provides tools to efficiently load and process data, which is essential for training deep learning models.\n",
    "\n",
    "* **`Dataset`:**\n",
    "    * An abstract class representing a dataset. To create a custom dataset, you need to subclass `torch.utils.data.Dataset` and override two methods:\n",
    "        * `__len__(self)`: Should return the total number of samples in the dataset.\n",
    "        * `__getitem__(self, idx)`: Should return the sample (e.g., an image tensor and its label) at the given index `idx`.\n",
    "* **`DataLoader`:**\n",
    "    * Wraps a `Dataset` (or an iterable) and provides an iterator to easily access data in batches.\n",
    "    * **Key Features and Parameters:**\n",
    "        * `batch_size`: Number of samples per batch.\n",
    "        * `shuffle=True/False`: Whether to shuffle the data at the beginning of each epoch. Crucial for training.\n",
    "        * `num_workers`: Number of subprocesses to use for data loading. Using multiple workers can significantly speed up data preprocessing and loading by performing it in parallel with model training.\n",
    "        * `pin_memory=True/False`: If `True`, the DataLoader will copy tensors into CUDA pinned memory before returning them, which can speed up data transfer to the GPU.\n",
    "        * `collate_fn`: A callable used to customize how samples are batched together (e.g., for handling sequences of varying lengths).\n",
    "* **Pre-built Datasets:**\n",
    "    * `torchvision.datasets`: Contains popular computer vision datasets like MNIST, CIFAR10, ImageNet, with built-in downloading and preprocessing.\n",
    "    * `torchaudio.datasets`: For audio datasets.\n",
    "    * `torchtext.datasets` (Note: `torchtext`'s API has evolved; many now prefer Hugging Face `datasets` for NLP).\n",
    "\n",
    "---\n",
    "\n",
    "**6. A Typical PyTorch Training Loop (Conceptual Outline)**\n",
    "\n",
    "The process of training a model in PyTorch generally follows these steps:\n",
    "\n",
    "1.  **Define the Model:** Create a class that inherits from `nn.Module`, defining layers in `__init__` and the forward pass logic in `forward`.\n",
    "2.  **Instantiate Model, Loss Function, and Optimizer:**\n",
    "    * `model = YourModelClass(...)`\n",
    "    * `criterion = nn.YourLossFunction()`\n",
    "    * `optimizer = optim.YourOptimizer(model.parameters(), lr=your_learning_rate)`\n",
    "3.  **Prepare Data:** Use `Dataset` and `DataLoader` to load and batch your training (and validation/test) data.\n",
    "4.  **Training Epochs:** Loop for a specified number of epochs (passes over the entire training dataset).\n",
    "    * **Batch Iteration:** Inside each epoch, loop through the `DataLoader` to get batches of data.\n",
    "        1.  **Get Data:** Extract input features and corresponding labels from the current batch.\n",
    "        2.  **Move to Device:** Transfer input and label tensors to the target device (CPU or GPU): `inputs, labels = inputs.to(device), labels.to(device)`.\n",
    "        3.  **Zero Gradients:** Clear previous gradients: `optimizer.zero_grad()`.\n",
    "        4.  **Forward Pass:** Get model predictions: `outputs = model(inputs)`.\n",
    "        5.  **Calculate Loss:** Compute the loss: `loss = criterion(outputs, labels)`.\n",
    "        6.  **Backward Pass:** Compute gradients: `loss.backward()`.\n",
    "        7.  **Update Weights:** Adjust model parameters: `optimizer.step()`.\n",
    "        8.  **(Optional) Logging:** Print or log training statistics (e.g., loss, accuracy) for the current batch or epoch.\n",
    "5.  **Evaluation (Validation/Testing):**\n",
    "    * Typically performed after each epoch or at the end of training.\n",
    "    * Set the model to evaluation mode: `model.eval()` (this disables layers like Dropout and uses running statistics for BatchNorm).\n",
    "    * Use a `with torch.no_grad():` block to disable gradient computation, saving memory and time.\n",
    "    * Iterate through the validation/test DataLoader.\n",
    "    * Perform forward pass, calculate loss, and compute evaluation metrics (e.g., accuracy, precision, recall).\n",
    "    * Set the model back to training mode if continuing training: `model.train()`.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Saving and Loading Models**\n",
    "\n",
    "It's crucial to save your trained models for later use or to resume training.\n",
    "\n",
    "* **Saving/Loading `state_dict` (Recommended for Parameters):**\n",
    "    * The `state_dict` is a Python dictionary object that maps each layer to its learnable parameters (tensors).\n",
    "    * **Saving:** `torch.save(model.state_dict(), 'model_weights.pth')`\n",
    "    * **Loading:**\n",
    "        1.  First, instantiate your model: `model = YourModelClass(...)`\n",
    "        2.  Then load the state dictionary: `model.load_state_dict(torch.load('model_weights.pth'))`\n",
    "    * Make sure to call `model.eval()` before using the loaded model for inference to set dropout and batch normalization layers to evaluation mode.\n",
    "* **Saving/Loading Entire Model:**\n",
    "    * This method saves the entire model object using Python's `pickle` module.\n",
    "    * **Saving:** `torch.save(model, 'entire_model.pth')`\n",
    "    * **Loading:** `model = torch.load('entire_model.pth')`\n",
    "    * **Caution:** This approach can be less robust as the serialized data is bound to the specific classes and directory structure used when the model was saved. It might break if you refactor your code or use the model in a different project.\n",
    "* **Saving Checkpoints:** For resuming training, you might want to save more than just the model's `state_dict`. A checkpoint can include the optimizer's state, the current epoch, the latest training loss, etc.\n",
    "    ```python\n",
    "    # Example saving a checkpoint\n",
    "    # checkpoint = {\n",
    "    #     'epoch': epoch,\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'loss': loss,\n",
    "    #     # ... any other info\n",
    "    # }\n",
    "    # torch.save(checkpoint, 'checkpoint.pth')\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "**8. Device Management (CPU/GPU)**\n",
    "\n",
    "Effectively using available hardware is key.\n",
    "\n",
    "* **Checking for GPU:** `torch.cuda.is_available()` returns `True` if a CUDA-enabled GPU is found.\n",
    "* **Defining the Device:** It's good practice to define a device object early in your script:\n",
    "    `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`\n",
    "* **Moving Tensors to Device:** `tensor = tensor.to(device)`\n",
    "* **Moving Models to Device:** `model = model.to(device)`\n",
    "    * This moves all the model's parameters and buffers to the specified device.\n",
    "* **Consistency:** Ensure all tensors involved in an operation reside on the same device to avoid runtime errors.\n",
    "\n",
    "---\n",
    "\n",
    "**9. The PyTorch Ecosystem (Key Supporting Libraries)**\n",
    "\n",
    "PyTorch's power is amplified by its rich ecosystem:\n",
    "\n",
    "* **`torchvision`:** Provides popular datasets (MNIST, CIFAR10, ImageNet), pre-trained model architectures (ResNet, VGG, AlexNet), and common image transformation utilities for computer vision tasks.\n",
    "* **`torchaudio`:** Offers datasets, pre-trained models, and audio processing utilities for speech and audio applications.\n",
    "* **`torchtext`:** (Its role has evolved) Historically provided tools and datasets for Natural Language Processing. Many NLP practitioners now use libraries like Hugging Face `transformers` and `datasets` which are built on or compatible with PyTorch.\n",
    "* **Higher-Level Frameworks:**\n",
    "    * **PyTorch Lightning:** A lightweight wrapper that organizes PyTorch code, reducing boilerplate and adding features like multi-GPU training, mixed-precision training, and experiment logging with minimal changes to your core PyTorch code.\n",
    "    * **Fastai:** A high-level library built on PyTorch that simplifies training state-of-the-art models using modern best practices.\n",
    "* **Specialized Libraries:**\n",
    "    * **Hugging Face `transformers`:** Provides thousands of pre-trained models for NLP tasks (BERT, GPT, etc.) and tools to easily use and fine-tune them with PyTorch.\n",
    "    * Libraries for reinforcement learning, graph neural networks, probabilistic programming, etc.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b466430-ea95-4711-bf9b-b4cf72c1f7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
