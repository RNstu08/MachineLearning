{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df6e5b-1c99-41bf-8d54-be1fd1e6527a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d1d22-4585-4169-bc77-75f977e10e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d3c0264-3f6c-4e4f-a749-5325583610c8",
   "metadata": {},
   "source": [
    "Clustering is an unsupervised learning task where the goal is to group data points such that points in the same group (cluster) are more similar to each other than to those in other groups. Evaluating clustering performance is challenging because, typically, there are no \"ground truth\" labels to compare against.\n",
    "\n",
    "Clustering metrics are generally divided into two types:\n",
    "\n",
    "1.  **Intrinsic Metrics:** Evaluate the quality of the clustering based solely on the data itself and the generated clusters (e.g., compactness, separation). No external ground truth labels are needed.\n",
    "2.  **Extrinsic Metrics:** Evaluate the clustering by comparing it to a pre-existing \"ground truth\" set of class labels or a known underlying structure. These are useful when such ground truth is available (e.g., for benchmarking or when using clustering on data where labels exist but you want to see if clustering can rediscover them).\n",
    "\n",
    "Here's a brief mention of the key metrics you listed:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fe3ec5-6210-435f-86ab-2eb1b8d73d61",
   "metadata": {},
   "source": [
    "**36. Silhouette Score (Intrinsic Metric)**\n",
    "\n",
    "* **Concept:** Measures how similar a data point is to its own cluster (cohesion) compared to other clusters (separation). A higher Silhouette Score indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "* **Formula:** For a single sample $i$:\n",
    "    $s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$\n",
    "    Where:\n",
    "    * $a(i)$: The average distance from sample $i$ to all other points within the same cluster.\n",
    "    * $b(i)$: The average distance from sample $i$ to all points in the *nearest neighboring cluster* (the cluster to which $i$ is not assigned, that has the smallest average distance to $i$).\n",
    "    The Silhouette Score for a dataset is the mean of $s(i)$ over all samples.\n",
    "* **Interpretation:**\n",
    "    * Score is between -1 and +1.\n",
    "    * **+1:** The sample is far from neighboring clusters and close to its own (ideal).\n",
    "    * **0:** The sample is on or very near the decision boundary between two clusters (overlapping clusters).\n",
    "    * **-1:** The sample is likely misclassified and closer to a neighboring cluster.\n",
    "    The overall mean Silhouette Score indicates the quality of the clustering structure. Higher is generally better.\n",
    "* **Pros:**\n",
    "    * Does not require ground truth labels.\n",
    "    * Score is bounded and interpretable.\n",
    "    * Can be used to compare different clustering algorithms or to help choose the optimal number of clusters (K) for algorithms like K-Means (look for K that maximizes the score).\n",
    "    * Provides per-sample scores for detailed analysis.\n",
    "* **Cons:**\n",
    "    * Computationally intensive for large datasets due to pairwise distance calculations.\n",
    "    * Tends to favor convex, globular clusters and might not score well on clusters with irregular shapes or varying densities (e.g., those found by DBSCAN).\n",
    "* **Example:** Imagine points in 2D. If a point is tightly packed with its cluster-mates and far from any other cluster, its $a(i)$ will be small and $b(i)$ large, leading to $s(i) \\approx 1$. If $a(i) \\approx b(i)$, $s(i) \\approx 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d789a846-109f-4a56-8d14-feba3d9fbf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation (Scikit-learn):**\n",
    "\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs # For example data\n",
    "\n",
    "# Example Data\n",
    "X, _ = make_blobs(n_samples=150, centers=3, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Example Clustering (K-Means)\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init='auto')\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "score = silhouette_score(X, cluster_labels)\n",
    "# print(f\"Silhouette Score: {score:.3f}\")\n",
    "# Example Output: Silhouette Score: 0.692\n",
    "\n",
    "# Per-sample scores\n",
    "# sample_silhouette_values = silhouette_samples(X, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f345875-d163-4908-a9b6-b6cf35be5522",
   "metadata": {},
   "source": [
    "* **Context:** A popular intrinsic metric for evaluating clustering when ground truth is absent. Useful for assessing cluster separation and cohesion, and for parameter tuning (like finding K in K-Means).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889df62-770d-4f56-a317-033192fb7b7a",
   "metadata": {},
   "source": [
    "**37. Adjusted Rand Index (ARI) (Extrinsic Metric)**\n",
    "\n",
    "* **Concept:** Measures the similarity between two data clusterings (e.g., a predicted clustering and a ground truth clustering), correcting for chance. It considers pairs of samples and counts how many pairs are grouped identically or differently in both clusterings.\n",
    "* **Formula:**\n",
    "    $ARI = \\frac{\\text{Rand Index (RI)} - \\text{Expected RI}}{\\text{Max RI} - \\text{Expected RI}}$\n",
    "    The Rand Index (RI) itself is:\n",
    "    $RI = \\frac{TP + TN}{TP + FP + FN + TN}$\n",
    "    Where TP, TN, FP, FN are defined based on pairs of samples:\n",
    "    * TP: Pairs in the same cluster in both true and predicted.\n",
    "    * TN: Pairs in different clusters in both true and predicted.\n",
    "    * FP: Pairs in the same cluster in predicted, but different in true.\n",
    "    * FN: Pairs in different clusters in predicted, but same in true.\n",
    "    ARI adjusts this for chance agreement.\n",
    "* **Interpretation:**\n",
    "    * Ranges from -1 to +1 (though usually non-negative).\n",
    "    * **+1:** Perfect agreement between the true and predicted clusterings.\n",
    "    * **0:** Random agreement (the predicted clustering is no better than assigning clusters randomly, considering the adjustment for chance).\n",
    "    * Negative values are possible but indicate agreement worse than random.\n",
    "    Higher values are better.\n",
    "* **Pros:**\n",
    "    * Corrected for chance, providing a more robust measure than the raw Rand Index.\n",
    "    * Symmetric: `ARI(A,B) = ARI(B,A)`.\n",
    "    * Bounded score.\n",
    "* **Cons:**\n",
    "    * **Requires ground truth labels,** which are typically unavailable in pure unsupervised clustering scenarios.\n",
    "    * Assumes the \"ground truth\" is indeed correct and meaningful.\n",
    "* **Example:**\n",
    "    `labels_true  = [0, 0, 0, 1, 1, 1]`\n",
    "    `labels_pred1 = [0, 0, 0, 1, 1, 1]` (Perfect match) -> ARI $\\approx$ 1\n",
    "    `labels_pred2 = [1, 1, 1, 0, 0, 0]` (Labels swapped, structure identical) -> ARI $\\approx$ 1\n",
    "    `labels_pred3 = [0, 0, 1, 0, 1, 1]` (Some misclassifications) -> ARI will be lower.\n",
    "    `labels_pred4 = [0, 1, 2, 0, 1, 2]` (Random-like) -> ARI $\\approx$ 0\n",
    "* **Implementation (Scikit-learn):**\n",
    "    ```python\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "    labels_true  = [0, 0, 0, 1, 1, 1]\n",
    "    labels_pred1 = [0, 0, 0, 1, 1, 1]\n",
    "    labels_pred2 = [1, 1, 1, 0, 0, 0]\n",
    "    labels_pred3 = [0, 0, 1, 0, 1, 1]\n",
    "\n",
    "    # ari1 = adjusted_rand_score(labels_true, labels_pred1)\n",
    "    # ari2 = adjusted_rand_score(labels_true, labels_pred2)\n",
    "    # ari3 = adjusted_rand_score(labels_true, labels_pred3)\n",
    "    # print(f\"ARI (Perfect): {ari1:.3f}\")      # Expected: 1.000\n",
    "    # print(f\"ARI (Swapped Labels): {ari2:.3f}\") # Expected: 1.000\n",
    "    # print(f\"ARI (Imperfect): {ari3:.3f}\")    # Expected: something like 0.242\n",
    "    ```\n",
    "* **Context:** Used when ground truth class labels or a reference clustering is available. It's excellent for benchmarking clustering algorithms or for situations where clustering is used for a task that has a known correct grouping.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebfab07-b088-4d56-94d0-950143edb39f",
   "metadata": {},
   "source": [
    "**38. Normalized Mutual Information (NMI) (Extrinsic Metric)**\n",
    "\n",
    "* **Concept:** Measures the agreement between two clusterings (true and predicted) using concepts from information theory. It quantifies how much information is shared between the two clusterings, normalized to fall between 0 and 1. It asks: \"How much does knowing one clustering reduce my uncertainty about the other?\"\n",
    "* **Formula:** Based on Mutual Information (MI) and Entropy (H):\n",
    "    $NMI(U, V) = \\frac{MI(U, V)}{\\text{mean}(H(U), H(V))}$ (One common normalization, others exist)\n",
    "    Where $U$ is the true clustering, $V$ is the predicted clustering.\n",
    "    * $MI(U,V) = H(U) - H(U|V) = H(V) - H(V|U)$\n",
    "    * $H(U)$ is the entropy (uncertainty) of clustering U.\n",
    "* **Interpretation:**\n",
    "    * Ranges from 0 to 1.\n",
    "    * **1:** Perfect correlation/agreement between the two clusterings. The predicted clustering perfectly explains the true clustering.\n",
    "    * **0:** The two clusterings are independent; knowing one gives no information about the other.\n",
    "    Higher values are better.\n",
    "* **Pros:**\n",
    "    * Grounded in information theory.\n",
    "    * Normalized, allowing for comparison across datasets/clusterings.\n",
    "    * Generally robust to permutations of cluster labels (like ARI).\n",
    "* **Cons:**\n",
    "    * **Requires ground truth labels.**\n",
    "    * The exact value can depend slightly on the normalization method used (`average_method` in scikit-learn).\n",
    "    * Can be less immediately intuitive than ARI for some.\n",
    "* **Example:**\n",
    "    Using the same labels as for ARI:\n",
    "    `labels_true  = [0, 0, 0, 1, 1, 1]`\n",
    "    `labels_pred1 = [0, 0, 0, 1, 1, 1]` (Perfect match) -> NMI $\\approx$ 1\n",
    "    `labels_pred2 = [1, 1, 1, 0, 0, 0]` (Labels swapped) -> NMI $\\approx$ 1\n",
    "    `labels_pred3 = [0, 0, 1, 0, 1, 1]` (Some misclassifications) -> NMI will be lower.\n",
    "* **Implementation (Scikit-learn):**\n",
    "    ```python\n",
    "    from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "    labels_true  = [0, 0, 0, 1, 1, 1]\n",
    "    labels_pred1 = [0, 0, 0, 1, 1, 1]\n",
    "    labels_pred2 = [1, 1, 1, 0, 0, 0]\n",
    "    labels_pred3 = [0, 0, 1, 0, 1, 1]\n",
    "\n",
    "    # nmi1 = normalized_mutual_info_score(labels_true, labels_pred1)\n",
    "    # nmi2 = normalized_mutual_info_score(labels_true, labels_pred2)\n",
    "    # nmi3 = normalized_mutual_info_score(labels_true, labels_pred3)\n",
    "    # print(f\"NMI (Perfect): {nmi1:.3f}\")      # Expected: 1.000\n",
    "    # print(f\"NMI (Swapped Labels): {nmi2:.3f}\") # Expected: 1.000\n",
    "    # print(f\"NMI (Imperfect): {nmi3:.3f}\")    # Expected: something like 0.221\n",
    "    ```\n",
    "    *Note: The `average_method` parameter in `normalized_mutual_info_score` can affect results slightly; `'arithmetic'` is a common default.*\n",
    "* **Context:** Another strong choice for comparing a predicted clustering to ground truth labels. It's particularly useful when an information-theoretic measure of agreement is desired.\n",
    "\n",
    "---\n",
    "\n",
    "**Concluding Remarks on Clustering Metrics:**\n",
    "\n",
    "* The choice between **intrinsic** (like Silhouette Score) and **extrinsic** (like ARI, NMI) metrics fundamentally depends on whether you have ground truth labels.\n",
    "* In most real-world unsupervised clustering, you'll rely on intrinsic metrics and domain knowledge. Other intrinsic metrics include Davies-Bouldin Index and Calinski-Harabasz Index.\n",
    "* Extrinsic metrics are invaluable for algorithm development, benchmarking, and when clustering is applied to data with known (but perhaps unused during clustering) labels.\n",
    "* **Visual inspection** of the clusters (e.g., using dimensionality reduction if data has >2 features) is also a crucial, albeit qualitative, evaluation step.\n",
    "* No single clustering metric is universally superior; it's often advisable to consider multiple metrics and the characteristics of your data and algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77833c3f-e7ff-46a1-b3fb-3c901192abcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
