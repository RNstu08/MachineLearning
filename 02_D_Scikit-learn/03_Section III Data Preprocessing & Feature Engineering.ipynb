{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e620ac1e-859b-4822-9125-7df3fb1662e9",
   "metadata": {},
   "source": [
    "This is a vital part of the machine learning pipeline. Raw data often isn't in the right format or scale for algorithms to perform optimally. Preprocessing involves transforming data to make it suitable, while feature engineering involves creating new features from existing ones.\n",
    "\n",
    "## Scikit-learn: Data Preprocessing & Feature Engineering\n",
    "\n",
    "This document covers a range of essential preprocessing and feature engineering techniques:\n",
    "\n",
    "* **Scaling:** `StandardScaler`, `MinMaxScaler`, `RobustScaler`, `Normalizer`.\n",
    "* **Encoding Categorical:** `OneHotEncoder`, `OrdinalEncoder`, `LabelEncoder` (for target `y`).\n",
    "* **Imputation:** `SimpleImputer` for handling missing values.\n",
    "* **Feature Engineering:** `PolynomialFeatures` for creating interaction/polynomial terms, `KBinsDiscretizer` for binning continuous data.\n",
    "* **Text Feature Extraction:** `CountVectorizer` and `TfidfVectorizer` for converting text into numerical matrices.\n",
    "* **Feature Selection:** Basic methods like `VarianceThreshold` and `SelectKBest`.\n",
    "\n",
    "---\n",
    "\n",
    "These tools are fundamental for preparing your data effectively before applying machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3b92b-bab3-4ce0-9869-847bf72adb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70328af8-6481-4c50-9643-f21b0c460d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b9a88-cd43-4034-b9f7-c5c2384f00e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1eb2af6-0202-4e0d-8fc0-16655b61311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scaling Numerical Features ---\n",
      "Original Numerical Data:\n",
      "    Age  Salary  Height_cm\n",
      "0   25   50000        175\n",
      "1   45   80000        163\n",
      "2   35   60000        170\n",
      "3   55   95000        180\n",
      "4   22   48000        168\n",
      "\n",
      "Standardized Data (StandardScaler):\n",
      " [[-0.93 -0.91  0.65]\n",
      " [ 0.7   0.74 -1.4 ]\n",
      " [-0.11 -0.36 -0.21]\n",
      " [ 1.51  1.56  1.51]\n",
      " [-1.17 -1.02 -0.55]]\n",
      "\n",
      "Min-Max Scaled Data (MinMaxScaler to [0, 1]):\n",
      " [[0.09 0.04 0.71]\n",
      " [0.7  0.68 0.  ]\n",
      " [0.39 0.26 0.41]\n",
      " [1.   1.   1.  ]\n",
      " [0.   0.   0.29]]\n",
      "\n",
      "Robust Scaled Data (RobustScaler):\n",
      " [[-0.5  -0.33  0.71]\n",
      " [ 0.5   0.67 -1.  ]\n",
      " [ 0.    0.    0.  ]\n",
      " [ 1.    1.17  1.43]\n",
      " [-0.65 -0.4  -0.29]]\n",
      "\n",
      "L2 Normalized Data (Normalizer - row-wise):\n",
      " [[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "------------------------------\n",
      "--- Encoding Categorical Features ---\n",
      "Original Categorical Data:\n",
      "    Color Size Quality\n",
      "0    Red    M    Good\n",
      "1  Green    L   Great\n",
      "2   Blue    S    Good\n",
      "3  Green    M    Fair\n",
      "4    Red    L   Great\n",
      "\n",
      "One-Hot Encoded Data (OneHotEncoder):\n",
      "    Color_Blue  Color_Green  Color_Red  Size_L  Size_M  Size_S\n",
      "0         0.0          0.0        1.0     0.0     1.0     0.0\n",
      "1         0.0          1.0        0.0     1.0     0.0     0.0\n",
      "2         1.0          0.0        0.0     0.0     0.0     1.0\n",
      "3         0.0          1.0        0.0     0.0     1.0     0.0\n",
      "4         0.0          0.0        1.0     1.0     0.0     0.0\n",
      "\n",
      "Ordinal Encoded Data (OrdinalEncoder - Quality):\n",
      "    QualityEncoded\n",
      "0             1.0\n",
      "1             2.0\n",
      "2             1.0\n",
      "3             0.0\n",
      "4             2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\learning\\Machine_learning\\Pandas\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but Normalizer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Target Labels: ['Cat', 'Dog', 'Cat', 'Fish', 'Dog']\n",
      "Label Encoded Target: [0 1 0 2 1]\n",
      "Encoded Classes: ['Cat' 'Dog' 'Fish']\n",
      "------------------------------\n",
      "--- Handling Missing Values ---\n",
      "Original Data with Missing Values:\n",
      "      A     B     C\n",
      "0  1.0   NaN  11.0\n",
      "1  2.0   7.0  12.0\n",
      "2  NaN   8.0  13.0\n",
      "3  4.0   9.0   NaN\n",
      "4  5.0  10.0  15.0\n",
      "\n",
      "Imputed Data (Mean Strategy):\n",
      "      A     B      C\n",
      "0  1.0   8.5  11.00\n",
      "1  2.0   7.0  12.00\n",
      "2  3.0   8.0  13.00\n",
      "3  4.0   9.0  12.75\n",
      "4  5.0  10.0  15.00\n",
      "\n",
      "Imputed Data (Median Strategy):\n",
      "      A     B     C\n",
      "0  1.0   8.5  11.0\n",
      "1  2.0   7.0  12.0\n",
      "2  3.0   8.0  13.0\n",
      "3  4.0   9.0  12.5\n",
      "4  5.0  10.0  15.0\n",
      "------------------------------\n",
      "--- Feature Engineering ---\n",
      "Original Data for Polynomial Features:\n",
      "    X1  X2\n",
      "0   1   4\n",
      "1   2   5\n",
      "2   3   6\n",
      "\n",
      "Polynomial Features (degree=2):\n",
      " [[ 1.  4.  1.  4. 16.]\n",
      " [ 2.  5.  4. 10. 25.]\n",
      " [ 3.  6.  9. 18. 36.]]\n",
      "Feature Names: ['X1' 'X2' 'X1^2' 'X1 X2' 'X2^2']\n",
      "\n",
      "Original Data for Discretization:\n",
      "    Value\n",
      "0      1\n",
      "1      5\n",
      "2     12\n",
      "3     18\n",
      "4     25\n",
      "5     30\n",
      "6     35\n",
      "7     40\n",
      "8     48\n",
      "9     55\n",
      "\n",
      "Discretized Data (uniform bins):\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]]\n",
      "------------------------------\n",
      "--- Text Feature Extraction ---\n",
      "Original Text Corpus:\n",
      " ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
      "\n",
      "Word Counts (CountVectorizer - sparse matrix):\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Feature Names (Vocabulary):\n",
      " ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "\n",
      "TF-IDF Features (TfidfVectorizer - sparse matrix):\n",
      " [[0.   0.47 0.58 0.38 0.   0.   0.38 0.   0.38]\n",
      " [0.   0.69 0.   0.28 0.   0.54 0.28 0.   0.28]\n",
      " [0.51 0.   0.   0.27 0.51 0.   0.27 0.51 0.27]\n",
      " [0.   0.47 0.58 0.38 0.   0.   0.38 0.   0.38]]\n",
      "Feature Names (Vocabulary):\n",
      " ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "------------------------------\n",
      "--- Feature Selection (Introduction) ---\n",
      "Original Iris shape: (150, 4)\n",
      "\n",
      "Shape after VarianceThreshold(0): (150, 4)\n",
      "\n",
      "Shape after SelectKBest(k=2): (150, 2)\n",
      "Selected feature indices: [2 3]\n",
      "\n",
      "Other methods like RFE exist for more advanced selection.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler, RobustScaler, Normalizer,\n",
    "                                   OneHotEncoder, LabelEncoder, OrdinalEncoder,\n",
    "                                   PolynomialFeatures, KBinsDiscretizer)\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
    "from sklearn.datasets import load_iris # For feature selection example\n",
    "\n",
    "# --- 1. Importance of Preprocessing ---\n",
    "# - Many algorithms assume data is normally distributed or features are on a similar scale\n",
    "#   (e.g., algorithms using distance like KNN, SVM, or gradient-based like linear models).\n",
    "# - Categorical features need to be converted to numerical format.\n",
    "# - Missing values often need to be handled (imputed or removed).\n",
    "# - Feature engineering can create more informative inputs for the model.\n",
    "\n",
    "# --- 2. Scaling Numerical Features (sklearn.preprocessing) ---\n",
    "# Applies transformations feature by feature (column-wise).\n",
    "\n",
    "print(\"--- Scaling Numerical Features ---\")\n",
    "data_numeric = pd.DataFrame({\n",
    "    'Age': [25, 45, 35, 55, 22],\n",
    "    'Salary': [50000, 80000, 60000, 95000, 48000],\n",
    "    'Height_cm': [175, 163, 170, 180, 168]\n",
    "})\n",
    "print(\"Original Numerical Data:\\n\", data_numeric)\n",
    "\n",
    "# a) StandardScaler: Z-score normalization (mean=0, std=1)\n",
    "scaler_standard = StandardScaler()\n",
    "# Fit learns mean/std, transform applies the scaling\n",
    "# fit_transform() does both in one step (preferred on training data)\n",
    "data_standardized = scaler_standard.fit_transform(data_numeric)\n",
    "print(\"\\nStandardized Data (StandardScaler):\\n\", data_standardized.round(2))\n",
    "# To apply to new data (test set), use scaler_standard.transform(X_test)\n",
    "\n",
    "# b) MinMaxScaler: Scales data to a specific range [min, max] (default [0, 1])\n",
    "scaler_minmax = MinMaxScaler(feature_range=(0, 1))\n",
    "data_minmax = scaler_minmax.fit_transform(data_numeric)\n",
    "print(\"\\nMin-Max Scaled Data (MinMaxScaler to [0, 1]):\\n\", data_minmax.round(2))\n",
    "\n",
    "# c) RobustScaler: Uses median and IQR, less sensitive to outliers\n",
    "scaler_robust = RobustScaler()\n",
    "data_robust = scaler_robust.fit_transform(data_numeric)\n",
    "print(\"\\nRobust Scaled Data (RobustScaler):\\n\", data_robust.round(2))\n",
    "\n",
    "# d) Normalizer: Scales individual *samples* (rows) to have unit norm (L1 or L2).\n",
    "# Used less often for features, more for text data or specific algorithms.\n",
    "# Note: Fits are not typically needed for Normalizer.\n",
    "normalizer_l2 = Normalizer(norm='l2') # L2 norm (Euclidean distance)\n",
    "data_normalized = normalizer_l2.transform(data_numeric) # Use transform directly\n",
    "print(\"\\nL2 Normalized Data (Normalizer - row-wise):\\n\", data_normalized.round(2))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 3. Encoding Categorical Features (sklearn.preprocessing) ---\n",
    "\n",
    "print(\"--- Encoding Categorical Features ---\")\n",
    "data_categorical = pd.DataFrame({\n",
    "    'Color': ['Red', 'Green', 'Blue', 'Green', 'Red'],\n",
    "    'Size': ['M', 'L', 'S', 'M', 'L'],\n",
    "    'Quality': ['Good', 'Great', 'Good', 'Fair', 'Great'] # Ordinal feature\n",
    "})\n",
    "print(\"Original Categorical Data:\\n\", data_categorical)\n",
    "\n",
    "# a) OneHotEncoder: Creates binary columns for each category. Preferred for nominal features.\n",
    "# handle_unknown='ignore' prevents errors if unseen categories appear in test data.\n",
    "# sparse_output=False returns a dense numpy array (easier to view)\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "data_onehot = ohe.fit_transform(data_categorical[['Color', 'Size']]) # Apply to nominal features\n",
    "# Get feature names for the new columns\n",
    "ohe_feature_names = ohe.get_feature_names_out(['Color', 'Size'])\n",
    "print(\"\\nOne-Hot Encoded Data (OneHotEncoder):\\n\", pd.DataFrame(data_onehot, columns=ohe_feature_names))\n",
    "\n",
    "# b) OrdinalEncoder: Encodes categories into integers (0, 1, 2...). Assumes an order.\n",
    "# Define the desired order for the 'Quality' feature\n",
    "quality_order = ['Fair', 'Good', 'Great']\n",
    "ordinal_enc = OrdinalEncoder(categories=[quality_order], handle_unknown='use_encoded_value', unknown_value=np.nan) # Specify order\n",
    "data_ordinal = ordinal_enc.fit_transform(data_categorical[['Quality']])\n",
    "print(\"\\nOrdinal Encoded Data (OrdinalEncoder - Quality):\\n\", pd.DataFrame(data_ordinal, columns=['QualityEncoded']))\n",
    "\n",
    "# c) LabelEncoder: Encodes target variable (y) into integers [0, n_classes-1].\n",
    "# Generally NOT recommended for features (X) as it implies an arbitrary order.\n",
    "target_labels = ['Cat', 'Dog', 'Cat', 'Fish', 'Dog']\n",
    "le = LabelEncoder()\n",
    "target_encoded = le.fit_transform(target_labels)\n",
    "print(f\"\\nOriginal Target Labels: {target_labels}\")\n",
    "print(f\"Label Encoded Target: {target_encoded}\")\n",
    "print(f\"Encoded Classes: {le.classes_}\") # Shows mapping: Cat=0, Dog=1, Fish=2\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 4. Handling Missing Values (sklearn.impute) ---\n",
    "\n",
    "print(\"--- Handling Missing Values ---\")\n",
    "data_missing = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 7, 8, 9, 10],\n",
    "    'C': [11, 12, 13, np.nan, 15]\n",
    "})\n",
    "print(\"Original Data with Missing Values:\\n\", data_missing)\n",
    "\n",
    "# SimpleImputer: Replaces missing values (NaN) using a strategy.\n",
    "# Strategies: 'mean', 'median', 'most_frequent', 'constant'\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "data_imputed_mean = imputer_mean.fit_transform(data_missing)\n",
    "print(\"\\nImputed Data (Mean Strategy):\\n\", pd.DataFrame(data_imputed_mean, columns=data_missing.columns))\n",
    "\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "data_imputed_median = imputer_median.fit_transform(data_missing)\n",
    "print(\"\\nImputed Data (Median Strategy):\\n\", pd.DataFrame(data_imputed_median, columns=data_missing.columns))\n",
    "\n",
    "# Impute categorical (if needed, use strategy='most_frequent' or 'constant')\n",
    "# data_cat_missing = pd.DataFrame({'Color': ['R', 'G', np.nan, 'B']})\n",
    "# imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "# print(\"\\nImputed Categorical:\\n\", imputer_cat.fit_transform(data_cat_missing))\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 5. Feature Engineering (sklearn.preprocessing) ---\n",
    "\n",
    "print(\"--- Feature Engineering ---\")\n",
    "data_poly = pd.DataFrame({'X1': [1, 2, 3], 'X2': [4, 5, 6]})\n",
    "print(\"Original Data for Polynomial Features:\\n\", data_poly)\n",
    "\n",
    "# PolynomialFeatures: Generates polynomial features (e.g., x1^2, x2^2, x1*x2).\n",
    "# degree: The degree of the polynomial.\n",
    "# include_bias=False: Avoids adding a column of ones (bias term).\n",
    "# interaction_only=False: Includes interaction terms (e.g., X1*X2).\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "data_poly_features = poly.fit_transform(data_poly)\n",
    "print(f\"\\nPolynomial Features (degree=2):\\n\", data_poly_features)\n",
    "print(f\"Feature Names: {poly.get_feature_names_out(['X1', 'X2'])}\") # X1, X2, X1^2, X1*X2, X2^2\n",
    "\n",
    "# KBinsDiscretizer: Bin continuous data into intervals (discretization).\n",
    "# n_bins: Number of bins.\n",
    "# encode: 'ordinal' (integer bins), 'onehot' (sparse), 'onehot-dense'.\n",
    "# strategy: 'uniform' (equal width), 'quantile' (equal frequency), 'kmeans'.\n",
    "data_discretize = pd.DataFrame({'Value': [1, 5, 12, 18, 25, 30, 35, 40, 48, 55]})\n",
    "print(\"\\nOriginal Data for Discretization:\\n\", data_discretize)\n",
    "discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='uniform', subsample=None)\n",
    "data_binned = discretizer.fit_transform(data_discretize[['Value']])\n",
    "print(\"\\nDiscretized Data (uniform bins):\\n\", data_binned)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 6. Feature Extraction (Text - sklearn.feature_extraction.text) ---\n",
    "\n",
    "print(\"--- Text Feature Extraction ---\")\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "print(\"Original Text Corpus:\\n\", corpus)\n",
    "\n",
    "# CountVectorizer: Converts text to a matrix of token (word) counts.\n",
    "vectorizer_count = CountVectorizer()\n",
    "X_counts = vectorizer_count.fit_transform(corpus)\n",
    "print(\"\\nWord Counts (CountVectorizer - sparse matrix):\\n\", X_counts.toarray())\n",
    "print(\"Feature Names (Vocabulary):\\n\", vectorizer_count.get_feature_names_out())\n",
    "\n",
    "# TfidfVectorizer: Converts text to a matrix of TF-IDF features.\n",
    "# TF-IDF = Term Frequency * Inverse Document Frequency. Weights important words higher.\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(corpus)\n",
    "print(\"\\nTF-IDF Features (TfidfVectorizer - sparse matrix):\\n\", X_tfidf.toarray().round(2))\n",
    "print(\"Feature Names (Vocabulary):\\n\", vectorizer_tfidf.get_feature_names_out())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 7. Feature Selection (Introduction - sklearn.feature_selection) ---\n",
    "# Selecting relevant features, potentially improving model performance and reducing complexity.\n",
    "\n",
    "print(\"--- Feature Selection (Introduction) ---\")\n",
    "# Load iris data for example\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "print(f\"Original Iris shape: {X_iris.shape}\")\n",
    "\n",
    "# a) VarianceThreshold: Remove features with variance below a threshold (removes constant features by default).\n",
    "selector_var = VarianceThreshold(threshold=0) # threshold=0 removes zero-variance features\n",
    "X_iris_high_variance = selector_var.fit_transform(X_iris)\n",
    "print(f\"\\nShape after VarianceThreshold(0): {X_iris_high_variance.shape}\") # Usually no change for iris\n",
    "\n",
    "# b) SelectKBest: Select features based on univariate statistical tests.\n",
    "# For regression: f_regression, mutual_info_regression\n",
    "# For classification: chi2, f_classif, mutual_info_classif\n",
    "# Select top 2 features based on F-test for regression (example, though iris is classification)\n",
    "selector_kbest = SelectKBest(score_func=f_regression, k=2)\n",
    "X_iris_kbest = selector_kbest.fit_transform(X_iris, y_iris) # Needs y for supervised selection\n",
    "print(f\"\\nShape after SelectKBest(k=2): {X_iris_kbest.shape}\")\n",
    "print(f\"Selected feature indices: {selector_kbest.get_support(indices=True)}\") # Shows which columns were kept\n",
    "\n",
    "# c) Recursive Feature Elimination (RFE) - More advanced (covered later if needed)\n",
    "# Recursively removes features and builds a model on remaining features.\n",
    "print(\"\\nOther methods like RFE exist for more advanced selection.\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eae307-cb4c-4234-ac6b-cddb582cf421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
