{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eedd8135-19e4-4837-a722-5095bcf9181b",
   "metadata": {},
   "source": [
    "- CIFAR-10 consists of 32x32 color images in 10 different classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). This dataset is more complex than MNIST or FashionMNIST, making it a good candidate for demonstrating the power of CNNs over simple ANNs for image tasks.\n",
    "\n",
    "- This example will cover loading CIFAR-10, defining a basic CNN architecture, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b915fa-ab92-4bc0-a70f-1cdb17a8cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F # For activation functions if not using nn.Module versions\n",
    "\n",
    "# --- 1. Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Hyperparameters ---\n",
    "num_epochs = 10 # Increase epochs for a more complex dataset/model\n",
    "batch_size = 64 # Smaller batch size might be needed depending on GPU memory\n",
    "learning_rate = 0.001\n",
    "num_classes = 10 # CIFAR-10 has 10 classes\n",
    "\n",
    "# --- 3. Load and Prepare CIFAR-10 Dataset ---\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "# CIFAR-10 images are 3x32x32 (3 color channels)\n",
    "# We need to normalize them. The means and stds are commonly used values for CIFAR-10.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # Mean/Std for CIFAR-10\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                             train=True,\n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "# Download and load the test data\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=False,\n",
    "                                            transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2) # num_workers can speed up loading\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Dataset loaded. Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# --- 4. Define the Convolutional Neural Network (CNN) Model ---\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional Layer 1\n",
    "        # Input: 3 channels (RGB), Output: 16 channels, Kernel size: 3x3, Padding: 1 (to maintain size)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Pooling Layer 1\n",
    "        # Kernel size: 2x2, Stride: 2 (downsamples by factor of 2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # After pool1: Image size becomes 16x16 (32/2 = 16)\n",
    "\n",
    "        # Convolutional Layer 2\n",
    "        # Input: 16 channels, Output: 32 channels, Kernel size: 3x3, Padding: 1\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Pooling Layer 2\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # After pool2: Image size becomes 8x8 (16/2 = 8)\n",
    "\n",
    "        # Fully Connected Layer 1\n",
    "        # Input features: 32 channels * 8 width * 8 height = 32 * 8 * 8 = 2048\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # Fully Connected Layer 2 (Output Layer)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, 3, 32, 32)\n",
    "        out = self.conv1(x)       # -> (batch_size, 16, 32, 32)\n",
    "        out = self.relu1(out)\n",
    "        out = self.pool1(out)     # -> (batch_size, 16, 16, 16)\n",
    "\n",
    "        out = self.conv2(out)     # -> (batch_size, 32, 16, 16)\n",
    "        out = self.relu2(out)\n",
    "        out = self.pool2(out)     # -> (batch_size, 32, 8, 8)\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        # Reshape from (batch_size, 32, 8, 8) to (batch_size, 32*8*8)\n",
    "        out = out.view(out.size(0), -1) # -> (batch_size, 2048)\n",
    "\n",
    "        out = self.fc1(out)       # -> (batch_size, 256)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc2(out)       # -> (batch_size, 10) (Logits)\n",
    "        return out\n",
    "\n",
    "# --- 5. Instantiate the Model, Loss, and Optimizer ---\n",
    "model = SimpleCNN(num_classes=num_classes).to(device)\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Loss Function: CrossEntropyLoss for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- 6. Training Loop ---\n",
    "print(\"\\nStarting Training...\")\n",
    "n_total_steps = len(train_loader)\n",
    "train_losses = []\n",
    "train_accuracies = [] # Optional: track training accuracy per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    n_correct_train = 0\n",
    "    n_samples_train = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        n_samples_train += labels.size(0)\n",
    "        n_correct_train += (predicted_train == labels).sum().item()\n",
    "\n",
    "        if (i+1) % 100 == 0: # Print progress every 100 steps\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_loss = running_loss / n_samples_train\n",
    "    epoch_acc = 100.0 * n_correct_train / n_samples_train\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "    print(f'--- Epoch {epoch+1} Summary --- Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}% ---')\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "\n",
    "# --- 7. Evaluation Loop (Testing) ---\n",
    "print(\"\\nStarting Evaluation on Test Set...\")\n",
    "model.eval() # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    n_correct_test = 0\n",
    "    n_samples_test = 0\n",
    "    all_labels_test = []\n",
    "    all_predicted_test = []\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples_test += labels.size(0)\n",
    "        n_correct_test += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Store labels and predictions for confusion matrix/report\n",
    "        all_labels_test.extend(labels.cpu().numpy())\n",
    "        all_predicted_test.extend(predicted.cpu().numpy())\n",
    "\n",
    "\n",
    "    accuracy_test = 100.0 * n_correct_test / n_samples_test\n",
    "    print(f'Accuracy of the network on the {len(test_dataset)} test images: {accuracy_test:.2f} %')\n",
    "\n",
    "    # --- 8. Confusion Matrix and Classification Report ---\n",
    "    print(\"\\nConfusion Matrix (CNN Test Set):\")\n",
    "    cm_cnn_test = confusion_matrix(all_labels_test, all_predicted_test)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_cnn_test, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=cifar10_classes, yticklabels=cifar10_classes)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix - CNN (CIFAR-10 Test Set)\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nClassification Report (CNN Test Set):\")\n",
    "    print(classification_report(all_labels_test, all_predicted_test, target_names=cifar10_classes))\n",
    "\n",
    "\n",
    "# --- 9. Plot Training Loss ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', label='Training Loss (CrossEntropy)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss per Epoch (CNN - CIFAR-10)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f70fa8-3085-495b-a376-8b5918754448",
   "metadata": {},
   "source": [
    "# PyTorch CIFAR-10 CNN Classifier: Code Explanation\n",
    "\n",
    "This document explains a Python script that implements a Convolutional Neural Network (CNN) using PyTorch to classify images from the CIFAR-10 dataset.\n",
    "\n",
    "## 1. Dataset: CIFAR-10\n",
    "- **Source**: `torchvision.datasets.CIFAR10`.\n",
    "- **Nature**: A dataset of 60,000 32x32 color images in 10 classes (e.g., airplane, automobile, bird, cat), with 6,000 images per class.\n",
    "- **Normalization**:\n",
    "    - Pixel values are transformed to PyTorch tensors and then normalized.\n",
    "    - The normalization uses mean and standard deviation values specific to the CIFAR-10 dataset, applied per color channel (Red, Green, Blue). This helps stabilize training and improve model performance.\n",
    "- **`DataLoader`**:\n",
    "    - `num_workers`: This parameter in `DataLoader` can be set to a value greater than 0 to use multiple subprocesses for data loading. This can significantly speed up the data pipeline, especially if preprocessing is non-trivial, by loading data in parallel while the GPU is busy with computations.\n",
    "\n",
    "## 2. Model Definition (`SimpleCNN` class)\n",
    "- **Core Components**: The network primarily uses `nn.Conv2d` for convolutional operations and `nn.MaxPool2d` for pooling.\n",
    "- **`nn.Conv2d` (Convolutional Layers)**:\n",
    "    - **Purpose**: Apply learnable filters to input image data to extract features like edges, textures, and patterns.\n",
    "    - **Key Arguments**:\n",
    "        - `in_channels (int)`: Number of channels in the input image (e.g., 3 for RGB color images like CIFAR-10, 1 for grayscale).\n",
    "        - `out_channels (int)`: Number of filters (also known as kernels or feature detectors) to be applied. This determines the depth (number of channels) of the output feature map.\n",
    "        - `kernel_size (int or tuple)`: Specifies the dimensions of the convolutional filter (e.g., `3` for a 3x3 filter, or `(3, 5)` for a 3x5 filter).\n",
    "        - `stride (int or tuple, optional)`: The step size with which the filter moves across the input image (default is 1).\n",
    "        - `padding (int or tuple, optional)`: Adds a specified number of pixels around the border of the input image. This can be used to control the spatial dimensions of the output feature map. For instance, `padding=1` with a `kernel_size=3` and `stride=1` often preserves the input width and height.\n",
    "- **Activation Function (`nn.ReLU`)**:\n",
    "    - Applied after each convolutional layer to introduce non-linearity, enabling the network to learn more complex features.\n",
    "- **`nn.MaxPool2d` (Max Pooling Layers)**:\n",
    "    - **Purpose**: Downsamples the feature maps, reducing their spatial dimensions (width and height) while retaining the most prominent features. This helps to reduce computational complexity, control overfitting, and create some translation invariance.\n",
    "    - **Key Arguments**:\n",
    "        - `kernel_size (int or tuple)`: The size of the window over which to take the maximum.\n",
    "        - `stride (int or tuple, optional)`: The step size of the window. Often, `stride` is set equal to `kernel_size` (e.g., `kernel_size=2, stride=2`) to achieve non-overlapping pooling, effectively halving the input dimensions.\n",
    "- **Flattening Operation**:\n",
    "    - **Necessity**: Before connecting the output of the convolutional/pooling layers to fully connected (dense) layers, the multi-dimensional feature maps need to be flattened into a 1D vector for each sample in the batch.\n",
    "    - **Implementation**: `out.view(out.size(0), -1)` reshapes the tensor.\n",
    "        - `out.size(0)` gets the batch size.\n",
    "        - `-1` infers the remaining dimensions, effectively collapsing all feature dimensions (channels, height, width) into a single long vector.\n",
    "        - If the output of the last pooling layer is `[batch_size, num_output_channels, final_height, final_width]`, it gets flattened to `[batch_size, num_output_channels * final_height * final_width]`.\n",
    "- **`nn.Linear` (Fully Connected Layers)**:\n",
    "    - Used after the flattening operation, similar to their use in ANNs/MLPs, to perform classification based on the extracted features.\n",
    "\n",
    "## 3. Training and Evaluation Loops\n",
    "- **Structural Similarity**: The overall structure of the training and evaluation loops remains very similar to those used for the ANN/MLP examples. This includes:\n",
    "    - Iterating through epochs and batches.\n",
    "    - Moving data to the configured device.\n",
    "    - Zeroing gradients (`optimizer.zero_grad()`).\n",
    "    - Performing the forward pass (`outputs = model(images)`).\n",
    "    - Calculating the loss (`loss = criterion(outputs, labels)`).\n",
    "    - Performing the backward pass (`loss.backward()`).\n",
    "    - Updating model parameters (`optimizer.step()`).\n",
    "    - Using `model.train()` and `model.eval()` modes appropriately.\n",
    "    - Using `with torch.no_grad():` during evaluation.\n",
    "- **Key Difference from ANN Input Handling**:\n",
    "    - Unlike the ANN examples where input images were explicitly flattened in the `DataLoader` or at the beginning of the `forward` method, CNNs process images in their 2D (or 3D for channels) spatial format directly.\n",
    "    - The `forward` method of the CNN handles the spatial dimensions through its convolutional and pooling layers. The flattening step is an internal part of the CNN's `forward` method, specifically before transitioning to the fully connected layers.\n",
    "\n",
    "## 4. Performance\n",
    "- **Advantage of CNNs**: A simple CNN, like the one described, is expected to achieve significantly better classification accuracy on image datasets like CIFAR-10 compared to a basic MLP/ANN that only uses flattened pixel inputs.\n",
    "- **Reason**: CNNs are designed to effectively capture spatial hierarchies and local patterns in images through their use of convolutions (shared weights, local receptive fields) and pooling.\n",
    "- **State-of-the-Art**: It's important to note that while this `SimpleCNN` demonstrates the core concepts, achieving state-of-the-art results on CIFAR-10 typically requires much deeper and more sophisticated CNN architectures (e.g., ResNets, DenseNets, Vision Transformers).\n",
    "\n",
    "## Summary\n",
    "This example provides a fundamental but complete illustration of how to construct, train, and evaluate a Convolutional Neural Network using PyTorch for an image classification task. It highlights the specific layers and operations (Conv2d, MaxPool2d, flattening) that distinguish CNNs from simpler feedforward networks and showcases their suitability for processing image data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
