{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a236c1-c1a9-4839-a13f-b2b12c55266f",
   "metadata": {},
   "source": [
    "1.  **Handling Imbalanced Datasets:**\n",
    "    * **Problem:** In many classification tasks (like fraud detection or disease diagnosis), one class is much rarer than others. A model might achieve high accuracy by simply always predicting the majority class, which isn't useful.\n",
    "    * **Solutions:**\n",
    "        * **Resampling:** Techniques like oversampling the minority class (e.g., SMOTE from the `imbalanced-learn` library) or undersampling the majority class aim to balance the dataset before training.\n",
    "        * **Class Weighting:** Many Scikit-learn classifiers (`LogisticRegression`, `SVC`, tree-based models) have a `class_weight='balanced'` parameter that automatically adjusts model penalties to give more importance to minority class errors.\n",
    "        * **Evaluation Metrics:** Focus on metrics robust to imbalance, such as Precision, Recall, F1-score (especially for the minority class), ROC AUC, and Precision-Recall AUC, rather than just accuracy.\n",
    "\n",
    "2.  **Feature Importance:**\n",
    "    * **Concept:** Understanding which input features have the most influence on a model's predictions. This is crucial for model interpretation and can guide feature selection.\n",
    "    * **Accessing Importance:**\n",
    "        * **Tree-based Models:** (`RandomForestClassifier`, `DecisionTreeClassifier`, etc.) have a `.feature_importances_` attribute after fitting.\n",
    "        * **Linear Models:** (`LinearRegression`, `LogisticRegression`, `Lasso`, etc.) have a `.coef_` attribute. The magnitude of coefficients (on scaled data) often indicates importance. Lasso (`penalty='l1'`) performs implicit feature selection by setting some coefficients to zero.\n",
    "        * **Permutation Importance:** (`sklearn.inspection.permutation_importance`) is a model-agnostic technique that measures how much the model's score decreases when a feature's values are randomly shuffled.\n",
    "\n",
    "3.  **Partial Fit (Incremental/Online Learning):**\n",
    "    * **Concept:** For datasets too large to fit in memory, some Scikit-learn estimators support learning in batches using the `.partial_fit()` method instead of `.fit()`.\n",
    "    * **Applicability:** Check the documentation for specific estimators like `SGDClassifier`, `SGDRegressor`, `MultinomialNB`, `MiniBatchKMeans`, etc.\n",
    "\n",
    "4.  **Beyond Scikit-learn: Other ML Libraries:**\n",
    "    * **Gradient Boosting:** Libraries like **XGBoost**, **LightGBM**, and **CatBoost** often provide higher performance (speed and accuracy) for tabular data tasks than Scikit-learn's gradient boosting, and offer more advanced features. They often have Scikit-learn compatible wrappers.\n",
    "    * **Deep Learning:** For complex tasks involving unstructured data (images, text, audio), frameworks like **TensorFlow** (with Keras) and **PyTorch** are the standard tools.\n",
    "    * **Statistical Modeling:** For more detailed statistical inference (p-values, confidence intervals, diagnostics), **Statsmodels** is often used alongside or instead of Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f562b0-ec76-4ccf-bd15-4b86c7a8af61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
