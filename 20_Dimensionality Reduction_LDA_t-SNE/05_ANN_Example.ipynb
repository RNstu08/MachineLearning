{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e1fc95-bc98-4aa7-ad01-2c048a033afc",
   "metadata": {},
   "source": [
    "- This example will walk through the entire process: loading data, defining the network, setting up the loss and optimizer, training the network, and evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8de81-16f6-4e9b-8c3e-dc5dbbaf668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Device Configuration ---\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Hyperparameters ---\n",
    "input_size = 784  # MNIST images are 28x28 pixels -> flattened to 784\n",
    "hidden_size = 500 # Number of neurons in the hidden layer\n",
    "num_classes = 10  # Digits 0-9\n",
    "num_epochs = 5    # How many times to iterate over the entire dataset\n",
    "batch_size = 100  # Number of samples per batch\n",
    "learning_rate = 0.001\n",
    "\n",
    "# --- 3. Load and Prepare MNIST Dataset ---\n",
    "print(\"Loading MNIST dataset...\")\n",
    "# Transformations to apply to the data:\n",
    "# - ToTensor(): Converts PIL Image or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "#               to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "# - Normalize(): Normalizes tensor with mean and standard deviation.\n",
    "#                (0.1307,) and (0.3081,) are the calculated mean and std for MNIST.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           transform=transform,\n",
    "                                           download=True)\n",
    "\n",
    "# Download and load the test data\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                          train=False,\n",
    "                                          transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "# DataLoader provides an iterable over the dataset, handling batching, shuffling etc.\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True) # Shuffle training data each epoch\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False) # No need to shuffle test data\n",
    "\n",
    "print(f\"Dataset loaded. Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "\n",
    "# --- 4. Define the Neural Network Model (ANN/MLP) ---\n",
    "# Fully connected neural network with one hidden layer\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleANN, self).__init__() # Initialize the parent class (nn.Module)\n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # Input layer to hidden layer\n",
    "        self.relu = nn.ReLU()                         # Activation function for hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        # Note: No final activation (like Softmax) is applied here because\n",
    "        # nn.CrossEntropyLoss applies LogSoftmax internally.\n",
    "        # The output 'out' contains raw scores (logits) for each class.\n",
    "        return out\n",
    "\n",
    "# --- 5. Instantiate the Model, Loss, and Optimizer ---\n",
    "model = SimpleANN(input_size, hidden_size, num_classes).to(device)\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Loss Function: CrossEntropyLoss is suitable for multi-class classification.\n",
    "# It combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam is a popular choice.\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- 6. Training Loop ---\n",
    "print(\"\\nStarting Training...\")\n",
    "n_total_steps = len(train_loader)\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    running_loss = 0.0\n",
    "    model.train() # Set model to training mode\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Original image shape: [batch_size, 1, 28, 28]\n",
    "        # Reshape images to [batch_size, 784] to feed into the ANN\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device) # Move labels to the configured device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()   # Clear gradients from previous step\n",
    "        loss.backward()         # Compute gradients based on the loss\n",
    "        optimizer.step()        # Update model parameters (weights and biases)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy for the batch\n",
    "        # Get predictions: the class with the highest score (logit)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Record loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100.0 * n_correct / n_samples\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "    print(f'--- Epoch {epoch+1} Summary --- Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}% ---')\n",
    "\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "\n",
    "# --- 7. Evaluation Loop (Testing) ---\n",
    "print(\"\\nStarting Evaluation on Test Set...\")\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "model.eval() # Set model to evaluation mode (disables dropout, batchnorm updates etc.)\n",
    "with torch.no_grad():\n",
    "    n_correct_test = 0\n",
    "    n_samples_test = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Get predictions: max returns (value, index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples_test += labels.size(0)\n",
    "        n_correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy_test = 100.0 * n_correct_test / n_samples_test\n",
    "    print(f'Accuracy of the network on the {len(test_dataset)} test images: {accuracy_test:.2f} %')\n",
    "\n",
    "# --- 8. Plot Training Loss and Accuracy ---\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training Loss', color=color)\n",
    "ax1.plot(range(1, num_epochs + 1), train_losses, color=color, marker='o', label='Training Loss')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Training Accuracy (%)', color=color)\n",
    "ax2.plot(range(1, num_epochs + 1), train_accuracies, color=color, marker='x', linestyle='--', label='Training Accuracy')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout() # otherwise the right y-label is slightly clipped\n",
    "plt.title(\"Training Loss and Accuracy per Epoch\")\n",
    "# Add legends manually if needed, or rely on plot labels if clear\n",
    "# fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14502ca3-e423-4b71-a409-2c18631578b3",
   "metadata": {},
   "source": [
    "# PyTorch MNIST ANN Classifier: Code Explanation\n",
    "\n",
    "This document breaks down a Python script designed to train a simple Artificial Neural Network (ANN) for classifying handwritten digits from the MNIST dataset using PyTorch.\n",
    "\n",
    "## 1. Device Setup\n",
    "- **Purpose**: Optimizes computation by utilizing available hardware.\n",
    "- **Action**: Detects if a CUDA-enabled GPU is present.\n",
    "  - If yes, sets `device` to `'cuda'`.\n",
    "  - If no, sets `device` to `'cpu'`.\n",
    "- **Impact**: All subsequent tensor operations and model parameters are moved to this `device`, enabling GPU acceleration if available.\n",
    "\n",
    "## 2. Hyperparameters\n",
    "- **Purpose**: Configures the model architecture and training process.\n",
    "- **Key Definitions**:\n",
    "    - `input_size = 784`: MNIST images (28x28 pixels) are flattened into a 784-element vector.\n",
    "    - `hidden_size = 500`: Specifies the number of neurons in the network's hidden layer.\n",
    "    - `num_classes = 10`: Represents the ten possible output digits (0-9).\n",
    "    - `num_epochs = 5`: The number of full passes through the entire training dataset.\n",
    "    - `batch_size = 100`: The number of samples processed in one iteration before updating model parameters.\n",
    "    - `learning_rate = 0.001`: Determines the step size for parameter updates during optimization.\n",
    "\n",
    "## 3. MNIST Dataset Handling\n",
    "- **Purpose**: Loads and preprocesses the MNIST digit dataset.\n",
    "- **Actions**:\n",
    "    - **Loading**: `torchvision.datasets.MNIST` is used to download (if needed) and load the training and test sets.\n",
    "    - **Transformations (`transforms.Compose`)**:\n",
    "        - `transforms.ToTensor()`: Converts PIL images (pixel range [0, 255]) to PyTorch FloatTensors (shape: C x H x W, pixel range [0.0, 1.0]).\n",
    "        - `transforms.Normalize((0.1307,), (0.3081,))`: Standardizes pixel values using the MNIST dataset's global mean and standard deviation, aiding in model convergence.\n",
    "    - **`DataLoader`**:\n",
    "        - Wraps the training and test `Dataset` objects.\n",
    "        - Provides an iterable for accessing data in mini-batches.\n",
    "        - `shuffle=True` for `train_loader`: Randomizes the order of training samples each epoch to improve generalization.\n",
    "        - `shuffle=False` for `test_loader`: Order is irrelevant for evaluation.\n",
    "\n",
    "## 4. Model Definition (`SimpleANN` Class)\n",
    "- **Purpose**: Defines the architecture of the neural network.\n",
    "- **Structure**: Inherits from `torch.nn.Module`.\n",
    "    - **`__init__(self, input_size, hidden_size, num_classes)` (Constructor)**:\n",
    "        - Initializes `super(SimpleANN, self).__init__()`.\n",
    "        - `self.fc1 = nn.Linear(input_size, hidden_size)`: First fully connected layer (input to hidden).\n",
    "        - `self.relu = nn.ReLU()`: ReLU activation function for non-linearity.\n",
    "        - `self.fc2 = nn.Linear(hidden_size, num_classes)`: Second fully connected layer (hidden to output).\n",
    "    - **`forward(self, x)` (Forward Pass)**:\n",
    "        - Defines the data flow: Input `x` -> `fc1` -> `relu` -> `fc2` -> Output (Logits).\n",
    "        - The output consists of raw scores (logits) for each class; `nn.CrossEntropyLoss` handles the necessary Softmax internally.\n",
    "\n",
    "## 5. Instantiation of Model, Loss, and Optimizer\n",
    "- **Model**: `model = SimpleANN(input_size, hidden_size, num_classes).to(device)` creates an instance of the network and moves it to the configured `device`.\n",
    "- **Loss Function**: `criterion = nn.CrossEntropyLoss()` is selected, ideal for multi-class classification tasks.\n",
    "- **Optimizer**: `optimizer = optim.Adam(model.parameters(), lr=learning_rate)` is chosen to update model parameters based on computed gradients.\n",
    "\n",
    "## 6. Training Loop\n",
    "- **Purpose**: Iteratively trains the model using the training dataset.\n",
    "- **Process per Epoch**:\n",
    "    - `model.train()`: Sets the model to training mode (important for layers like Dropout or BatchNorm, though not heavily used in this simple model).\n",
    "    - **Batch Iteration (`for images, labels in train_loader`)**:\n",
    "        - **Data Preparation**:\n",
    "            - `images = images.reshape(-1, 28*28).to(device)`: Flattens images and moves them to the `device`.\n",
    "            - `labels = labels.to(device)`: Moves labels to the `device`.\n",
    "        - **Forward Pass**: `outputs = model(images)` generates predictions.\n",
    "        - **Loss Calculation**: `loss = criterion(outputs, labels)` computes the error.\n",
    "        - **Backward Pass & Optimization**:\n",
    "            - `optimizer.zero_grad()`: Clears accumulated gradients from previous steps.\n",
    "            - `loss.backward()`: Computes gradients of the loss with respect to model parameters.\n",
    "            - `optimizer.step()`: Updates model parameters using the Adam optimization algorithm.\n",
    "        - **Metrics**: Tracks and prints running loss and accuracy for monitoring.\n",
    "\n",
    "## 7. Evaluation Loop (Testing)\n",
    "- **Purpose**: Assesses the trained model's performance on unseen data.\n",
    "- **Process**:\n",
    "    - `model.eval()`: Sets the model to evaluation mode.\n",
    "    - `with torch.no_grad():`: Disables gradient computation, saving memory and speeding up inference.\n",
    "    - **Batch Iteration (`for images, labels in test_loader`)**:\n",
    "        - Data is prepared and moved to the `device`.\n",
    "        - Model predictions are generated.\n",
    "        - Accuracy is calculated by comparing predictions to true labels.\n",
    "    - The overall test accuracy is printed.\n",
    "\n",
    "## 8. Plotting Results\n",
    "- **Purpose**: Visualizes the training progress.\n",
    "- **Action**: Uses `matplotlib.pyplot` to plot the training loss and accuracy values recorded at the end of each epoch. This helps in understanding the learning dynamics and diagnosing potential issues.\n",
    "\n",
    "## Summary\n",
    "This script provides a comprehensive, yet fundamental, example of a PyTorch workflow for a standard image classification task. It showcases data loading, model definition, training with backpropagation, and performance evaluation. By running this code, one can expect to see the model's loss decrease and its accuracy on the MNIST dataset improve over the specified number of epochs, typically achieving high classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2c581-3060-4b0f-aff5-d60b8ded4d81",
   "metadata": {},
   "source": [
    "---\n",
    "- let's build a simple Artificial Neural Network (ANN) for a regression task using PyTorch. We'll use the California Housing dataset for this example, which involves predicting median house values based on several features.\n",
    "\n",
    "- This example will cover loading and preparing the data, defining a suitable ANN architecture for regression, training the model, and evaluating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5a0e0-e59e-4ca6-bf84-3953d64ea5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Hyperparameters ---\n",
    "# input_size will be determined by the dataset\n",
    "hidden_size1 = 128  # Number of neurons in the first hidden layer\n",
    "hidden_size2 = 64   # Number of neurons in the second hidden layer\n",
    "output_size = 1     # Predicting a single continuous value (median house value)\n",
    "num_epochs = 50     # Increased epochs for regression\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# --- 3. Load and Prepare California Housing Dataset ---\n",
    "print(\"Loading California Housing dataset...\")\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Convert to Pandas DataFrame for easier inspection (optional)\n",
    "X_df = pd.DataFrame(X, columns=housing.feature_names)\n",
    "y_series = pd.Series(y, name='MedHouseVal')\n",
    "\n",
    "input_size = X.shape[1] # Number of features\n",
    "print(f\"Dataset loaded. Number of features: {input_size}\")\n",
    "print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature Scaling (Very important for ANNs)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "# Convert data to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_raw, dtype=torch.float32).unsqueeze(1) # Reshape y to [n_samples, 1]\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_raw, dtype=torch.float32).unsqueeze(1) # Reshape y\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Data prepared. Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# --- 4. Define the Neural Network Model for Regression ---\n",
    "class SimpleANNRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleANNRegression, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        # No activation function for the output layer in regression\n",
    "        # as we want to predict continuous values directly.\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# --- 5. Instantiate the Model, Loss, and Optimizer ---\n",
    "model = SimpleANNRegression(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Loss Function: Mean Squared Error is suitable for regression.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer: Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- 6. Training Loop ---\n",
    "print(\"\\nStarting Training...\")\n",
    "n_total_steps = len(train_loader)\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 200 == 0: # Print more frequently for larger datasets\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f'--- Epoch {epoch+1} Summary --- Training Loss (MSE): {epoch_loss:.4f} ---')\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "\n",
    "# --- 7. Evaluation Loop (Testing) ---\n",
    "print(\"\\nStarting Evaluation on Test Set...\")\n",
    "model.eval() # Set model to evaluation mode\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    for features, targets in test_loader:\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        all_predictions.append(outputs.cpu().numpy()) # Move to CPU before converting to NumPy\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    mse_test = mean_squared_error(all_targets, all_predictions) # Should be close to avg_test_loss\n",
    "    r2_test = r2_score(all_targets, all_predictions)\n",
    "\n",
    "    print(f'Average Test Loss (MSE): {avg_test_loss:.4f}')\n",
    "    print(f'Calculated Test MSE: {mse_test:.4f}')\n",
    "    print(f'Test R-squared (R2): {r2_test:.4f}')\n",
    "\n",
    "\n",
    "# --- 8. Plot Training Loss ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', label='Training Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 9. Plot Actual vs. Predicted Values (Test Set) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(all_targets, all_predictions, alpha=0.3, edgecolors='k', label='Predictions')\n",
    "# Plot a line for perfect predictions\n",
    "min_val = min(all_targets.min(), all_predictions.min())\n",
    "max_val = max(all_targets.max(), all_predictions.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Median House Value\")\n",
    "plt.ylabel(\"Predicted Median House Value\")\n",
    "plt.title(\"Actual vs. Predicted Values (Test Set)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e297c-0541-4acd-a078-5b426eea1b54",
   "metadata": {},
   "source": [
    "# PyTorch Regression ANN: Key Changes and Explanations\n",
    "\n",
    "This document outlines the modifications made to a simple Artificial Neural Network (ANN) structure to adapt it for a regression task, specifically using the California Housing dataset. The core PyTorch training workflow remains similar to classification, but key differences arise in data preparation, model output, loss function, and evaluation metrics.\n",
    "\n",
    "## 1. Dataset\n",
    "- **Source**: `Workspace_california_housing` from `sklearn.datasets`.\n",
    "- **Nature**: A regression dataset where the goal is to predict housing prices based on various features.\n",
    "\n",
    "## 2. Feature Scaling\n",
    "- **Method**: `StandardScaler` from `sklearn.preprocessing`.\n",
    "- **Action**: Applied to the input features (`X`).\n",
    "- **Importance**: Crucial for neural networks, especially those using gradient-based optimization. Scaling features to have zero mean and unit variance helps the network train more effectively and converge faster by preventing features with larger magnitudes from dominating the learning process.\n",
    "\n",
    "## 3. Target Reshaping\n",
    "- **Action**: The target variable `y` (housing prices) is reshaped using `.unsqueeze(1)`.\n",
    "- **Reason**: This converts the 1D target tensor (shape `[n_samples]`) into a 2D tensor of shape `[n_samples, 1]`.\n",
    "- **Compatibility**: PyTorch's `nn.MSELoss` (and many other loss functions) typically expects the model's output and the target tensor to have compatible shapes for element-wise comparison. For a single regression output per sample, both output and target should ideally be `[batch_size, 1]`.\n",
    "\n",
    "## 4. Model Architecture (`SimpleANNRegression`)\n",
    "- **Output Layer (`self.fc3`)**:\n",
    "    - `output_size = 1`: The final linear layer is configured to have a single output neuron.\n",
    "    - **Reason**: In regression, we are predicting a single continuous value (e.g., price) for each input sample.\n",
    "- **Output Activation**:\n",
    "    - **None**: No activation function (like ReLU, Sigmoid, or Softmax) is applied to the output of `self.fc3`.\n",
    "    - **Reason**: For regression tasks, the model should be able to output any real number. Applying an activation like ReLU would restrict outputs to be non-negative, and Sigmoid/Tanh would restrict them to a specific range, which is usually not desired for general regression problems. The raw linear output of the last layer is used directly as the prediction.\n",
    "\n",
    "## 5. Loss Function\n",
    "- **Type**: `nn.MSELoss()` (Mean Squared Error).\n",
    "- **Formula**: $L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$, where $y_i$ is the true value and $\\hat{y}_i$ is the predicted value.\n",
    "- **Suitability**: Standard and widely used loss function for regression tasks as it penalizes larger errors more heavily.\n",
    "\n",
    "## 6. Training Loop\n",
    "- **Core Logic**: Remains fundamentally similar to the classification training loop (forward pass, loss calculation, backward pass, optimizer step).\n",
    "- **Loss Calculation**: The `nn.MSELoss` is directly applied between the model's continuous outputs and the continuous target values.\n",
    "- **Metrics**:\n",
    "    - **Accuracy is not used**: Accuracy is a classification metric and is not meaningful for regression.\n",
    "    - **Training Loss (MSE)**: The primary metric tracked during training is the Mean Squared Error. The goal is to minimize this value.\n",
    "\n",
    "## 7. Evaluation Loop\n",
    "- **Primary Metric**:\n",
    "    - **Test MSE**: The average Mean Squared Error is calculated on the test set to assess performance on unseen data.\n",
    "- **Additional Metric**:\n",
    "    - **R-squared ($R^2$) Score**:\n",
    "        - Calculated using `sklearn.metrics.r2_score`.\n",
    "        - **Purpose**: Measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It indicates how well the regression model fits the observed data.\n",
    "        - **Interpretation**:\n",
    "            - $R^2 = 1$: Perfect fit (model explains all the variability).\n",
    "            - $R^2 = 0$: Model performs no better than predicting the mean of the target variable.\n",
    "            - $R^2 < 0$: Model performs worse than predicting the mean (indicates a very poor fit).\n",
    "\n",
    "## 8. Plotting\n",
    "- **Training Progress**: A plot of the training loss (MSE) over epochs is generated to visualize the learning process and convergence.\n",
    "- **Prediction Quality**: A scatter plot of \"Actual vs. Predicted\" values on the test set is created.\n",
    "    - **Ideal Scenario**: For a well-performing regression model, the points on this scatter plot should cluster tightly around the diagonal line (where actual value equals predicted value). This visually indicates the model's predictive accuracy.\n",
    "\n",
    "## Summary\n",
    "This example demonstrates the necessary adaptations to transform a basic ANN structure from a classification setup to a regression setup within the PyTorch framework. The main changes revolve around the model's output layer configuration (single neuron, no activation), the choice of loss function (`MSELoss`), and the relevant evaluation metrics (MSE, R-squared instead of accuracy). The fundamental training loop structure, however, remains consistent, highlighting the flexibility of PyTorch for various machine learning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
