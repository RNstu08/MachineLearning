{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1990d320-fbc9-4b64-a8f4-435b5f9e7cfb",
   "metadata": {},
   "source": [
    "- GRU for Time Series Regression (Sine Wave Prediction)\n",
    "\n",
    "- In this example, we'll train a GRU to predict the next value in a sine wave sequence, given a sequence of past values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20e7ba-1dc0-48dd-a274-0c8cf2de92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Hyperparameters ---\n",
    "sequence_length = 20  # Length of each input sequence\n",
    "feature_size = 1      # Each element in the sequence is a single number (0 or 1 for simplicity, or random)\n",
    "hidden_size_lstm = 64 # Number of features in the hidden state of LSTM\n",
    "num_layers_lstm = 1   # Number of recurrent layers (stacked LSTMs)\n",
    "num_classes_seq = 2   # Binary classification (pattern present or not)\n",
    "num_epochs_seq = 30\n",
    "batch_size_seq = 32\n",
    "learning_rate_seq = 0.001\n",
    "\n",
    "# --- 3. Generate Synthetic Sequential Data ---\n",
    "print(\"Generating synthetic sequential data...\")\n",
    "def generate_sequence(seq_len, pattern=[1, 1, 0]):\n",
    "    contains_pattern = False\n",
    "    # Decide if this sequence will contain the pattern\n",
    "    if np.random.rand() > 0.5: # 50% chance to contain pattern\n",
    "        contains_pattern = True\n",
    "        pattern_len = len(pattern)\n",
    "        if seq_len < pattern_len:\n",
    "            # Fallback if sequence is too short for pattern (should not happen with current settings)\n",
    "            sequence = np.random.randint(0, 2, seq_len)\n",
    "            contains_pattern = False # Re-evaluate based on actual sequence\n",
    "            for i in range(len(sequence) - pattern_len + 1):\n",
    "                if list(sequence[i:i+pattern_len]) == pattern:\n",
    "                    contains_pattern = True\n",
    "                    break\n",
    "            return sequence, 1 if contains_pattern else 0\n",
    "\n",
    "\n",
    "        start_idx = np.random.randint(0, seq_len - pattern_len + 1)\n",
    "        sequence = np.random.randint(0, 2, seq_len) # Fill with random 0s and 1s\n",
    "        sequence[start_idx : start_idx + pattern_len] = pattern\n",
    "    else:\n",
    "        sequence = np.random.randint(0, 2, seq_len)\n",
    "        # Double check if random sequence accidentally contains pattern\n",
    "        pattern_len = len(pattern)\n",
    "        for i in range(len(sequence) - pattern_len + 1):\n",
    "            if list(sequence[i:i+pattern_len]) == pattern:\n",
    "                contains_pattern = True # It does, so label it as 1\n",
    "                break\n",
    "    return sequence, 1 if contains_pattern else 0\n",
    "\n",
    "num_samples_seq = 2000\n",
    "sequences = []\n",
    "labels = []\n",
    "for _ in range(num_samples_seq):\n",
    "    seq, label = generate_sequence(sequence_length)\n",
    "    sequences.append(seq)\n",
    "    labels.append(label)\n",
    "\n",
    "sequences = np.array(sequences, dtype=np.float32).reshape(-1, sequence_length, feature_size)\n",
    "labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "print(f\"Generated {num_samples_seq} sequences of length {sequence_length}.\")\n",
    "print(f\"Example sequence (shape {sequences[0].shape}):\\n{sequences[0].ravel()}\")\n",
    "print(f\"Label for example: {labels[0]}\")\n",
    "print(f\"Class distribution: Class 0: {np.sum(labels==0)}, Class 1: {np.sum(labels==1)}\")\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
    "    sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train_tensor_seq = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "y_train_tensor_seq = torch.tensor(y_train_seq, dtype=torch.long) # CrossEntropyLoss expects long for labels\n",
    "X_test_tensor_seq = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "y_test_tensor_seq = torch.tensor(y_test_seq, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset_seq = TensorDataset(X_train_tensor_seq, y_train_tensor_seq)\n",
    "test_dataset_seq = TensorDataset(X_test_tensor_seq, y_test_tensor_seq)\n",
    "\n",
    "train_loader_seq = DataLoader(dataset=train_dataset_seq, batch_size=batch_size_seq, shuffle=True)\n",
    "test_loader_seq = DataLoader(dataset=test_dataset_seq, batch_size=batch_size_seq, shuffle=False)\n",
    "\n",
    "# --- 4. Define the LSTM Model for Sequence Classification ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # LSTM layer:\n",
    "        #   input_size: The number of expected features in the input x\n",
    "        #   hidden_size: The number of features in the hidden state h\n",
    "        #   num_layers: Number of recurrent layers.\n",
    "        #   batch_first=True: Means input/output tensors are (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state with zeros\n",
    "        # h0 shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        # c0 shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size) containing output features from the last layer of LSTM\n",
    "        # (hn, cn): tuple of hidden state and cell state for t = seq_length\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # We only need the output from the last time step for classification\n",
    "        # out shape: (batch_size, seq_len, hidden_size) -> out[:, -1, :] shape: (batch_size, hidden_size)\n",
    "        out = self.fc(out[:, -1, :]) # Decode the hidden state of the last time step\n",
    "        return out\n",
    "\n",
    "# --- 5. Instantiate the Model, Loss, and Optimizer ---\n",
    "model_lstm = LSTMClassifier(feature_size, hidden_size_lstm, num_layers_lstm, num_classes_seq).to(device)\n",
    "print(\"\\nLSTM Model Architecture:\")\n",
    "print(model_lstm)\n",
    "\n",
    "criterion_seq = nn.CrossEntropyLoss()\n",
    "optimizer_seq = optim.Adam(model_lstm.parameters(), lr=learning_rate_seq)\n",
    "\n",
    "# --- 6. Training Loop ---\n",
    "print(\"\\nStarting LSTM Training...\")\n",
    "train_losses_seq = []\n",
    "for epoch in range(num_epochs_seq):\n",
    "    model_lstm.train()\n",
    "    running_loss_seq = 0.0\n",
    "    for i, (seq_batch, labels_batch) in enumerate(train_loader_seq):\n",
    "        seq_batch = seq_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        outputs = model_lstm(seq_batch)\n",
    "        loss = criterion_seq(outputs, labels_batch)\n",
    "\n",
    "        optimizer_seq.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_seq.step()\n",
    "        running_loss_seq += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss_seq / len(train_loader_seq)\n",
    "    train_losses_seq.append(epoch_loss)\n",
    "    if (epoch+1) % 5 == 0 or epoch == num_epochs_seq -1 :\n",
    "      print(f'Epoch [{epoch+1}/{num_epochs_seq}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "print(\"Finished LSTM Training.\")\n",
    "\n",
    "# --- 7. Evaluation ---\n",
    "print(\"\\nStarting LSTM Evaluation...\")\n",
    "model_lstm.eval()\n",
    "all_labels_lstm, all_predicted_lstm = [], []\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for seq_batch, labels_batch in test_loader_seq:\n",
    "        seq_batch = seq_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        outputs = model_lstm(seq_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels_batch.size(0)\n",
    "        n_correct += (predicted == labels_batch).sum().item()\n",
    "        all_labels_lstm.extend(labels_batch.cpu().numpy())\n",
    "        all_predicted_lstm.extend(predicted.cpu().numpy())\n",
    "\n",
    "accuracy_lstm = 100.0 * n_correct / n_samples\n",
    "print(f'Accuracy of the LSTM on test sequences: {accuracy_lstm:.2f} %')\n",
    "\n",
    "print(\"\\nConfusion Matrix (LSTM):\")\n",
    "cm_lstm = confusion_matrix(all_labels_lstm, all_predicted_lstm)\n",
    "sns.heatmap(cm_lstm, annot=True, fmt=\"d\", cmap=\"cividis\", xticklabels=[\"No Pattern\", \"Pattern\"], yticklabels=[\"No Pattern\", \"Pattern\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - LSTM Sequence Classification\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report (LSTM):\")\n",
    "print(classification_report(all_labels_lstm, all_predicted_lstm, target_names=[\"No Pattern\", \"Pattern\"]))\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses_seq, label='Training Loss')\n",
    "plt.title('LSTM Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CrossEntropy Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67435f2-9134-4cbc-81ec-c83426e28dbb",
   "metadata": {},
   "source": [
    "# Discussion: GRU for Time Series Regression Example\n",
    "\n",
    "This document discusses the key components and rationale behind using a Gated Recurrent Unit (GRU) network for a time series regression task, specifically predicting future values of a noisy sine wave.\n",
    "\n",
    "## 1. Data Generation & Preparation\n",
    "- **Synthetic Time Series**:\n",
    "    - A sine wave is generated as the underlying true signal.\n",
    "    - Noise is added to this sine wave to create a more realistic and challenging time series to model.\n",
    "- **Feature Scaling (`MinMaxScaler`)**:\n",
    "    - The noisy sine wave data is scaled, typically to a range like `[0, 1]` or `[-1, 1]`.\n",
    "    - **Rationale**: Scaling is common and often beneficial for training neural networks, including GRUs. It helps stabilize the learning process and can prevent issues caused by large input values or differing scales if multiple input features were used. `MinMaxScaler` is chosen here to preserve the shape of the time series within a bounded range.\n",
    "- **Sequence Creation (Sliding Window Approach)**:\n",
    "    - The core of preparing time series data for RNNs involves creating input sequences (`X_ts`) and corresponding target values (`y_ts`).\n",
    "    - This is typically done using a \"sliding window\" technique:\n",
    "        - `X_ts`: A sequence of `sequence_length` consecutive data points from the time series.\n",
    "        - `y_ts`: The single data point in the time series that immediately follows the end of its corresponding `X_ts` sequence.\n",
    "    - **Example**: If `sequence_length=10`, the first `X_ts` might be `data[0:10]`, and its corresponding `y_ts` would be `data[10]`. The next `X_ts` would be `data[1:11]`, and `y_ts` would be `data[11]`, and so on.\n",
    "    - This setup trains the GRU to predict the next value in the series given a history of previous values.\n",
    "\n",
    "## 2. The `nn.GRU` Layer\n",
    "- **Core Component**: `nn.GRU(input_size, hidden_size, num_layers, batch_first=True)` is the primary recurrent layer used for processing the time series sequences.\n",
    "- **GRU vs. LSTM**: GRUs are similar to LSTMs but have a simpler architecture (fewer gates: an update gate and a reset gate, lacking a separate cell state). They are often computationally slightly less expensive and can perform comparably on many tasks, though LSTMs might be preferred for tasks requiring learning very long-range dependencies.\n",
    "- **Key Parameters (similar to `nn.LSTM`)**:\n",
    "    - `input_size`: The number of expected features in the input `x` at each time step (dimensionality of each element in the sequence). For a univariate time series like the sine wave, this is 1.\n",
    "    - `hidden_size`: The number of features in the hidden state `h`. This dictates the memory capacity of the GRU.\n",
    "    - `num_layers (int, optional)`: Number of recurrent layers. Stacking GRUs can allow for learning more complex temporal patterns.\n",
    "    - `batch_first (bool, optional)`: If `True`, input and output tensors are `(batch, seq, feature)`. `True` is generally more intuitive.\n",
    "\n",
    "## 3. Output Layer\n",
    "- **Structure**: A single `nn.Linear` layer is used after the GRU layer.\n",
    "- **Configuration**:\n",
    "    - `in_features`: This is typically the `hidden_size` of the GRU, as the output of the GRU (often the last hidden state) is fed into this linear layer.\n",
    "    - `out_features = 1`: The linear layer has one output neuron.\n",
    "- **Activation**: No activation function is applied to the output of this linear layer.\n",
    "- **Rationale**: For regression tasks where the goal is to predict a continuous value (the next point in the sine wave), a raw linear output is desired. Activation functions like ReLU or Sigmoid would restrict the output range, which is generally unsuitable for unbounded regression targets.\n",
    "\n",
    "## 4. Loss Function\n",
    "- **Type**: `nn.MSELoss()` (Mean Squared Error).\n",
    "- **Suitability**: This is the standard and appropriate loss function for regression tasks. It measures the average squared difference between the predicted values and the actual target values, penalizing larger errors more significantly.\n",
    "\n",
    "## 5. Evaluation Metrics\n",
    "- **Primary Metric**:\n",
    "    - **MSE (Mean Squared Error)**: Calculated on the test set to assess the model's predictive accuracy on unseen data.\n",
    "- **Additional Metric**:\n",
    "    - **R-squared ($R^2$) Score**: Provides insight into how well the model's predictions approximate the true values, relative to simply predicting the mean.\n",
    "- **Inverse Transformation**:\n",
    "    - **Importance**: Before calculating interpretable metrics like MSE or $R^2$ (and for plotting), it's crucial to inverse transform the scaled predictions (model outputs) and the scaled actual target values back to their original data scale using the `scaler.inverse_transform()` method.\n",
    "    - **Reason**: Metrics calculated on the scaled data (e.g., range `[0,1]`) are not directly interpretable in the context of the original problem's units and magnitude. Inverse transformation ensures that the evaluation reflects the model's performance on the actual scale of the time series.\n",
    "\n",
    "## 6. Plotting\n",
    "- **Purpose**: To visually assess the GRU's performance in predicting the time series.\n",
    "- **Content**: The plot typically shows:\n",
    "    - The actual values of the time series from the test set.\n",
    "    - The GRU model's predictions for the corresponding time steps in the test set.\n",
    "- **Interpretation**: A good regression model will have its predictions closely tracking the actual sine wave, indicating that it has learned the underlying pattern and can forecast future values with reasonable accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd587b-2df2-40d6-a24f-f148f91df342",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**B. More Complex GRU for Multivariate Time Series Forecasting**\n",
    "\n",
    "- Task: We'll generate a synthetic multivariate time series where future values of one series depend on its own past and the past of other series. We will predict multiple steps ahead for one of the series.\n",
    "\n",
    "- Complexity Additions:\n",
    "\n",
    "- Multivariate input sequences (multiple features per time step).\n",
    "- Predicting a sequence of future values (sequence-to-sequence, but simplified to many-to-N).\n",
    "- Stacked GRU.\n",
    "- Validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23790eab-580a-4e15-bc68-b461b91218c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- 1. Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Hyperparameters ---\n",
    "input_features_ts = 3 # Number of features in the input time series\n",
    "output_features_ts = 1 # Predicting one of the features\n",
    "hidden_size_gru_c = 100\n",
    "num_layers_gru_c = 2\n",
    "input_seq_len = 24   # Use 24 past steps\n",
    "output_seq_len = 5   # Predict 5 future steps\n",
    "num_epochs_ts_c = 60\n",
    "batch_size_ts_c = 64\n",
    "learning_rate_ts_c = 0.001\n",
    "\n",
    "# --- 3. Generate Synthetic Multivariate Time Series Data ---\n",
    "print(\"Generating synthetic multivariate time series data...\")\n",
    "def generate_multivariate_timeseries(n_samples, seq_len, n_features):\n",
    "    t = np.linspace(0, 4 * np.pi * (n_samples / 100), n_samples + seq_len + output_seq_len) # Time vector\n",
    "    series = []\n",
    "    # Feature 1: Sine wave\n",
    "    s1 = np.sin(t) + np.random.normal(0, 0.1, len(t))\n",
    "    series.append(s1)\n",
    "    # Feature 2: Cosine wave (phase shifted)\n",
    "    s2 = np.cos(t - np.pi/4) + np.random.normal(0, 0.1, len(t))\n",
    "    series.append(s2)\n",
    "    # Feature 3: Combination with some trend and noise (this will be our target to predict parts of)\n",
    "    s3 = 0.5 * s1 + 0.3 * s2 + t * 0.01 + np.random.normal(0, 0.15, len(t))\n",
    "    series.append(s3)\n",
    "    \n",
    "    data = np.stack(series, axis=1).astype(np.float32) # Shape: (total_len, n_features)\n",
    "    return data\n",
    "\n",
    "total_data_points = 1500\n",
    "raw_data_mv = generate_multivariate_timeseries(total_data_points, input_seq_len, input_features_ts)\n",
    "print(f\"Generated raw multivariate data shape: {raw_data_mv.shape}\")\n",
    "\n",
    "# Normalize data\n",
    "scalers = [MinMaxScaler(feature_range=(-1, 1)) for _ in range(input_features_ts)]\n",
    "scaled_data_mv = np.zeros_like(raw_data_mv)\n",
    "for i in range(input_features_ts):\n",
    "    scaled_data_mv[:, i] = scalers[i].fit_transform(raw_data_mv[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create sequences for input and multi-step output for the target feature (e.g., feature 2)\n",
    "def create_multivariate_sequences(data, input_len, output_len, target_feature_idx=2):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - input_len - output_len + 1):\n",
    "        X.append(data[i : i + input_len, :]) # All features for input sequence\n",
    "        Y.append(data[i + input_len : i + input_len + output_len, target_feature_idx]) # Target feature for output sequence\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "target_feature_index = 2 # Predict the 3rd feature (index 2)\n",
    "X_mv, y_mv = create_multivariate_sequences(scaled_data_mv, input_seq_len, output_seq_len, target_feature_index)\n",
    "print(f\"Created sequences. X_mv shape: {X_mv.shape}, y_mv shape: {y_mv.shape}\") # y_mv will be (samples, output_seq_len)\n",
    "\n",
    "# Split data\n",
    "X_train_val_mv, X_test_mv, y_train_val_mv, y_test_mv = train_test_split(X_mv, y_mv, test_size=0.2, random_state=42)\n",
    "X_train_mv, X_val_mv, y_train_mv, y_val_mv = train_test_split(X_train_val_mv, y_train_val_mv, test_size=0.15, random_state=42)\n",
    "\n",
    "# Convert to Tensors\n",
    "X_train_tensor_mv = torch.tensor(X_train_mv, dtype=torch.float32)\n",
    "y_train_tensor_mv = torch.tensor(y_train_mv, dtype=torch.float32)\n",
    "X_val_tensor_mv = torch.tensor(X_val_mv, dtype=torch.float32)\n",
    "y_val_tensor_mv = torch.tensor(y_val_mv, dtype=torch.float32)\n",
    "X_test_tensor_mv = torch.tensor(X_test_mv, dtype=torch.float32)\n",
    "y_test_tensor_mv = torch.tensor(y_test_mv, dtype=torch.float32)\n",
    "\n",
    "\n",
    "train_dataset_mv = TensorDataset(X_train_tensor_mv, y_train_tensor_mv)\n",
    "val_dataset_mv = TensorDataset(X_val_tensor_mv, y_val_tensor_mv)\n",
    "test_dataset_mv = TensorDataset(X_test_tensor_mv, y_test_tensor_mv)\n",
    "\n",
    "train_loader_mv = DataLoader(train_dataset_mv, batch_size=batch_size_ts_c, shuffle=True)\n",
    "val_loader_mv = DataLoader(val_dataset_mv, batch_size=batch_size_ts_c, shuffle=False)\n",
    "test_loader_mv = DataLoader(test_dataset_mv, batch_size=batch_size_ts_c, shuffle=False)\n",
    "\n",
    "# --- 4. Define the Complex GRU Model for Multi-Step Forecasting ---\n",
    "class ComplexGRURegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_seq_len, dropout_prob=0.2):\n",
    "        super(ComplexGRURegressor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob if num_layers > 1 else 0)\n",
    "        # We want to predict 'output_seq_len' steps for one feature.\n",
    "        # One common approach is to have the GRU output a hidden state, then use a Linear layer\n",
    "        # to map this hidden state to the desired output_seq_len.\n",
    "        # If GRU outputs sequence, take last hidden state.\n",
    "        self.fc = nn.Linear(hidden_size, output_seq_len) # Predicts all future steps at once from last hidden state\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.gru(x, h0) # out: (batch, seq_len, hidden_size)\n",
    "        out = self.fc(out[:, -1, :]) # Use only the last hidden state to predict the future sequence\n",
    "                                     # out: (batch, output_seq_len)\n",
    "        return out\n",
    "\n",
    "# --- 5. Instantiate Model, Loss, Optimizer ---\n",
    "model_gru_complex = ComplexGRURegressor(input_features_ts, hidden_size_gru_c, num_layers_gru_c, output_seq_len).to(device)\n",
    "print(\"\\nComplex GRU Model Architecture:\")\n",
    "print(model_gru_complex)\n",
    "\n",
    "criterion_ts_c = nn.MSELoss()\n",
    "optimizer_ts_c = optim.Adam(model_gru_complex.parameters(), lr=learning_rate_ts_c)\n",
    "\n",
    "# --- 6. Training Loop with Validation ---\n",
    "print(\"\\nStarting Complex GRU Training...\")\n",
    "train_losses_c_gru, val_losses_c_gru = [], []\n",
    "\n",
    "for epoch in range(num_epochs_ts_c):\n",
    "    model_gru_complex.train()\n",
    "    running_train_loss = 0.0\n",
    "    for seq_batch, targets_batch in train_loader_mv:\n",
    "        seq_batch, targets_batch = seq_batch.to(device), targets_batch.to(device)\n",
    "        outputs = model_gru_complex(seq_batch)\n",
    "        loss = criterion_ts_c(outputs, targets_batch)\n",
    "        optimizer_ts_c.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_ts_c.step()\n",
    "        running_train_loss += loss.item() * seq_batch.size(0)\n",
    "    epoch_train_loss = running_train_loss / len(train_dataset_mv)\n",
    "    train_losses_c_gru.append(epoch_train_loss)\n",
    "\n",
    "    model_gru_complex.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for seq_batch_val, targets_batch_val in val_loader_mv:\n",
    "            seq_batch_val, targets_batch_val = seq_batch_val.to(device), targets_batch_val.to(device)\n",
    "            outputs_val = model_gru_complex(seq_batch_val)\n",
    "            loss_val = criterion_ts_c(outputs_val, targets_batch_val)\n",
    "            running_val_loss += loss_val.item() * seq_batch_val.size(0)\n",
    "    epoch_val_loss = running_val_loss / len(val_dataset_mv)\n",
    "    val_losses_c_gru.append(epoch_val_loss)\n",
    "    \n",
    "    if (epoch+1) % 5 == 0 or epoch == num_epochs_ts_c-1:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs_ts_c}], Train Loss (MSE): {epoch_train_loss:.6f}, Val Loss (MSE): {epoch_val_loss:.6f}')\n",
    "\n",
    "print(\"Finished Complex GRU Training.\")\n",
    "\n",
    "# --- 7. Evaluation on Test Set ---\n",
    "model_gru_complex.eval()\n",
    "all_preds_gru_c, all_targets_gru_c = [], []\n",
    "with torch.no_grad():\n",
    "    for sb, tb in test_loader_mv:\n",
    "        sb, tb = sb.to(device), tb.to(device)\n",
    "        outs = model_gru_complex(sb)\n",
    "        all_preds_gru_c.append(outs.cpu().numpy())\n",
    "        all_targets_gru_c.append(tb.cpu().numpy())\n",
    "\n",
    "all_preds_gru_c = np.concatenate(all_preds_gru_c, axis=0)\n",
    "all_targets_gru_c = np.concatenate(all_targets_gru_c, axis=0)\n",
    "\n",
    "# Inverse transform predictions and targets for the target feature (index 2)\n",
    "preds_orig_scale = scalers[target_feature_index].inverse_transform(all_preds_gru_c)\n",
    "targets_orig_scale = scalers[target_feature_index].inverse_transform(all_targets_gru_c)\n",
    "\n",
    "mse_gru_c = mean_squared_error(targets_orig_scale, preds_orig_scale)\n",
    "r2_gru_c = r2_score(targets_orig_scale, preds_orig_scale)\n",
    "print(f\"\\nComplex GRU Test MSE (original scale): {mse_gru_c:.4f}\")\n",
    "print(f\"Complex GRU Test R2 Score (original scale): {r2_gru_c:.4f}\")\n",
    "\n",
    "# --- 8. Plot Training and Validation Loss ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs_ts_c + 1), train_losses_c_gru, 'bo-', label='Training Loss (MSE)')\n",
    "plt.plot(range(1, num_epochs_ts_c + 1), val_losses_c_gru, 'ro-', label='Validation Loss (MSE)')\n",
    "plt.title('Complex GRU: Training & Validation Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('MSE Loss'); plt.legend(); plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 9. Plot some Test Predictions vs Actual ---\n",
    "# Plot for the first few test samples, showing the multi-step prediction\n",
    "num_plots = 3\n",
    "plt.figure(figsize=(15, num_plots * 4))\n",
    "for i in range(num_plots):\n",
    "    plt.subplot(num_plots, 1, i+1)\n",
    "    plt.plot(targets_orig_scale[i], \"bo-\", label=\"Actual Future Steps\")\n",
    "    plt.plot(preds_orig_scale[i], \"ro--\", label=\"Predicted Future Steps\")\n",
    "    plt.title(f\"Test Sample {i+1}: Multi-Step Prediction\")\n",
    "    plt.ylabel(\"Value (Original Scale)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "plt.xlabel(\"Future Time Step\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ce149-fe23-4974-abbf-51b23eb2d8e1",
   "metadata": {},
   "source": [
    "# Discussion: Complex GRU for Multivariate Time Series Regression Example\n",
    "\n",
    "This document discusses the key components and rationale behind using a more complex Gated Recurrent Unit (GRU) network for a multivariate time series regression task, specifically focusing on multi-step forecasting.\n",
    "\n",
    "## 1. Data Characteristics\n",
    "- **Multivariate Time Series**:\n",
    "    - The input data consists of sequences where each time step comprises multiple interacting features (e.g., three features in the example).\n",
    "    - This means the `input_size` (or `input_features_ts`) for the GRU will be greater than 1, reflecting the dimensionality of the input at each time step.\n",
    "- **Interacting Features**: The generation process or the nature of the real-world data implies that the different features influence each other over time, requiring the model to learn these cross-feature dependencies.\n",
    "\n",
    "## 2. Task Definition (Many-to-N Forecasting)\n",
    "- **Input**: The GRU model takes an input sequence of a defined length (`input_seq_len`), where each step in this sequence contains all available input features (`input_features_ts`).\n",
    "- **Output**: The model's task is to predict a sequence of future values (`output_seq_len` steps) for **one specific target feature** from the multivariate input.\n",
    "- **Nature**: This is a \"many-to-N\" sequence-to-sequence regression problem, where multiple future time steps of a single target variable are predicted simultaneously based on a history of multiple input features.\n",
    "\n",
    "## 3. Model Architecture (`ComplexGRURegressor`)\n",
    "- **Stacked GRU (`nn.GRU(..., num_layers=2, ...)`):**\n",
    "    - **Mechanism**: The model utilizes a stacked GRU, meaning multiple GRU layers are placed one on top of the other. The output sequence of the first GRU layer serves as the input sequence to the second GRU layer.\n",
    "    - **Benefit**: Stacking recurrent layers allows the network to learn more complex temporal hierarchies and representations from the data. Lower layers might learn short-term patterns, while higher layers can learn longer-term dependencies based on the outputs of the lower layers.\n",
    "- **Dropout Between GRU Layers**:\n",
    "    - **Functionality**: If `num_layers > 1` and the `dropout` parameter in the `nn.GRU` constructor is set to a value greater than 0, dropout layers are automatically introduced between the stacked GRU layers (on the outputs of each GRU layer except the final one).\n",
    "    - **Purpose**: This acts as a regularizer, helping to prevent overfitting by reducing co-adaptation between the stacked recurrent layers.\n",
    "- **Output Layer (`self.fc`)**:\n",
    "    - **Input**: The output of the GRU from the *last time step* of the input sequence (e.g., `out[:, -1, :]` which represents the final hidden state of the last GRU layer) is typically used as the consolidated feature representation of the input sequence.\n",
    "    - **Structure**: This feature vector is then fed into a single `nn.Linear` layer.\n",
    "    - **Output Neurons**: The linear layer is configured to have `output_seq_len` output neurons.\n",
    "    - **No Activation**: No activation function is applied after this linear layer, as the goal is to predict raw continuous values for the future time steps.\n",
    "    - **Prediction Strategy**: This setup enables the model to predict all `output_seq_len` future steps of the target variable simultaneously (a \"direct multi-step forecasting\" strategy).\n",
    "\n",
    "## 4. Data Preparation\n",
    "- **`create_multivariate_sequences` Function**:\n",
    "    - **Crucial Role**: This function is essential for transforming the raw multivariate time series data into a format suitable for training the GRU.\n",
    "    - **Process**: It implements a sliding window approach to generate:\n",
    "        - Input sequences (`X`): Each input sequence consists of `input_seq_len` consecutive time steps, with each time step containing all `input_features_ts`.\n",
    "        - Target sequences (`y`): Each target sequence consists of `output_seq_len` future values of the *specific target feature* that immediately follow the corresponding input sequence.\n",
    "\n",
    "## 5. Feature Scaling\n",
    "- **Method**: `MinMaxScaler` from `sklearn.preprocessing` is typically used.\n",
    "- **Application**: It's applied independently to each feature of the multivariate time series before creating the input/output sequences.\n",
    "- **Inverse Transformation for Evaluation**:\n",
    "    - **Necessity**: When evaluating the model's performance (e.g., calculating MSE, R-squared) and for plotting, the scaled predictions and the scaled actual target values for the *specific feature being predicted* must be inverse-transformed back to their original data scale.\n",
    "    - **Method**: This is done using the `inverse_transform` method of the `MinMaxScaler` instance that was fitted for that particular target feature.\n",
    "    - **Importance**: This ensures that the performance metrics are interpretable in the original units and magnitude of the target variable.\n",
    "\n",
    "## 6. Plotting and Visualization\n",
    "- **Purpose**: To visually assess the quality of the GRU's multi-step predictions on unseen test data.\n",
    "- **Content**: The final plot typically displays several examples from the test set, showing:\n",
    "    - The actual future sequence of the target variable (ground truth).\n",
    "    - The GRU model's `output_seq_len`-step prediction for that same period.\n",
    "- **Interpretation**: By comparing the predicted sequence to the actual sequence, one can visually gauge how well the model captures the trends, seasonality (if any), and overall dynamics of the target time series over the prediction horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2438235-3f10-405b-aa8e-17a370f5f655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
