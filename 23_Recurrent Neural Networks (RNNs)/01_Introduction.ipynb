{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c5ac0a-f86c-4683-95be-294b98fa504a",
   "metadata": {},
   "source": [
    "**Recurrent Neural Networks (RNNs)**.\n",
    "\n",
    "This is **Topic 23: Recurrent Neural Networks (RNNs)** from your list.\n",
    "\n",
    "**1. Introduction: Why RNNs? Handling Sequential Data**\n",
    "\n",
    "* **Limitations of Feedforward Networks (ANNs/MLPs and CNNs):**\n",
    "    * Standard ANNs and CNNs assume that all inputs (and outputs) are **independent of each other**. For example, when classifying an image, the classification of one image doesn't directly depend on the classification of the previous one.\n",
    "    * They process fixed-size inputs and produce fixed-size outputs.\n",
    "    * They don't have any \"memory\" of past inputs when processing current inputs.\n",
    "\n",
    "* **Sequential Data:** Many real-world problems involve sequences of data where the order matters, and past information is crucial for understanding the present or predicting the future. Examples include:\n",
    "    * **Natural Language Processing (NLP):** Words in a sentence, sentences in a document. The meaning of a word often depends on the words that came before it.\n",
    "    * **Time Series Data:** Stock prices over time, weather measurements, sensor readings. Future values often depend on past trends.\n",
    "    * **Speech Recognition:** A sequence of audio signals.\n",
    "    * **Video Analysis:** A sequence of image frames.\n",
    "    * **Music Generation:** A sequence of musical notes.\n",
    "\n",
    "* **RNNs are designed to work with sequential data.** They have an internal \"memory\" (called a hidden state) that allows them to persist information from previous steps in the sequence and use it to process the current step.\n",
    "\n",
    "---\n",
    "\n",
    "**2. The Core Idea: Loops and Hidden States**\n",
    "\n",
    "The defining feature of an RNN is its **recurrent connection** or **loop**. Unlike feedforward networks, where information flows strictly in one direction, RNNs have connections that loop back on themselves.\n",
    "\n",
    "* **Unrolling the Loop (Conceptual Diagram):**\n",
    "    It's often easier to understand an RNN by \"unrolling\" it through time. Imagine the network processing a sequence of inputs $x_0, x_1, x_2, \\dots, x_t$.\n",
    "\n",
    "    ```\n",
    "    Time step:       t-1                     t                       t+1\n",
    "                   ------>                 ------>                 ------>\n",
    "    Input:           x(t-1)                  x(t)                    x(t+1)\n",
    "                      |                       |                       |\n",
    "                      V                       V                       V\n",
    "                +-----------+           +-----------+           +-----------+\n",
    "    Previous --->|  RNN Cell |---------->|  RNN Cell |---------->|  RNN Cell |---> Next\n",
    "    Hidden State |   h(t-1)  |  (Output  |   h(t)    |  (Output  |  h(t+1)   | Hidden State\n",
    "    (Memory)     +-----------+  from     +-----------+  from     +-----------+ (Memory)\n",
    "                      |        previous    |         previous    |\n",
    "                      V        step)       V         step)       V\n",
    "    Output:          ŷ(t-1)                  ŷ(t)                    ŷ(t+1)\n",
    "    (Optional at\n",
    "     each step)\n",
    "    ```\n",
    "\n",
    "    * **RNN Cell:** This is the basic repeating unit of the RNN. At each time step $t$:\n",
    "        * It takes two inputs:\n",
    "            1.  The current input from the sequence, $x(t)$.\n",
    "            2.  The **hidden state** (or activation) from the previous time step, $h(t-1)$.\n",
    "        * It performs some computation (typically involving weighted sums and an activation function like `tanh` or `ReLU`).\n",
    "        * It produces two outputs:\n",
    "            1.  The **output for the current time step**, $\\hat{y}(t)$ (this output is optional and depends on the task; sometimes we only care about the final output after processing the whole sequence).\n",
    "            2.  The **new hidden state**, $h(t)$, which is then passed to the RNN cell at the next time step $t+1$.\n",
    "\n",
    "* **Hidden State ($h(t)$) - The \"Memory\":**\n",
    "    * The hidden state $h(t)$ is the crucial component that acts as the network's memory.\n",
    "    * It captures information from all previous steps in the sequence.\n",
    "    * The calculation for the hidden state at time $t$ typically looks like this:\n",
    "        $$h(t) = f(W_{hh} h(t-1) + W_{xh} x(t) + b_h)$$\n",
    "        Where:\n",
    "        * $f$ is an activation function (commonly `tanh` or `ReLU`).\n",
    "        * $W_{hh}$ are the weights for the recurrent connection (from previous hidden state to current hidden state).\n",
    "        * $W_{xh}$ are the weights for the input connection (from current input to current hidden state).\n",
    "        * $b_h$ is the bias for the hidden state.\n",
    "    * **Parameter Sharing:** Importantly, the same set of weights ($W_{hh}, W_{xh}$) and bias ($b_h$) are used across *all time steps*. This is similar to how a filter's weights are shared across different spatial locations in a CNN. This makes RNNs efficient and allows them to generalize to sequences of varying lengths.\n",
    "\n",
    "* **Output ($\\hat{y}(t)$):**\n",
    "    * The output at time step $t$ (if needed) is typically calculated from the hidden state at that time step:\n",
    "        $$\\hat{y}(t) = g(W_{hy} h(t) + b_y)$$\n",
    "        Where:\n",
    "        * $g$ is an activation function appropriate for the output (e.g., sigmoid for binary classification, softmax for multi-class, linear for regression).\n",
    "        * $W_{hy}$ are the weights connecting the hidden state to the output.\n",
    "        * $b_y$ is the bias for the output.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Different RNN Architectures (Types of Sequence Processing)**\n",
    "\n",
    "RNNs can be configured in various ways depending on the nature of the input and output sequences:\n",
    "\n",
    "* **a) One-to-One (Not really an RNN, but a standard feedforward network):**\n",
    "    * Fixed-size input to fixed-size output (e.g., image classification).\n",
    "    * `Input -> ANN/CNN -> Output`\n",
    "\n",
    "* **b) One-to-Many (Sequence Output):**\n",
    "    * Takes a single input and produces a sequence of outputs.\n",
    "    * Example: Image captioning (input: image; output: sequence of words).\n",
    "    * **Conceptual Diagram:**\n",
    "        ```\n",
    "        Input --> [RNN Cell] --h0--> [RNN Cell] --h1--> [RNN Cell] --h2--> ...\n",
    "                     |                  |                  |\n",
    "                     V                  V                  V\n",
    "                   Output1            Output2            Output3\n",
    "        ```\n",
    "        (The input might be fed only to the first cell, or its representation fed at each step).\n",
    "\n",
    "* **c) Many-to-One (Sequence Input):**\n",
    "    * Takes a sequence of inputs and produces a single output (usually after processing the entire sequence).\n",
    "    * Example: Sentiment analysis (input: sequence of words in a sentence; output: positive/negative sentiment). Text classification.\n",
    "    * **Conceptual Diagram:**\n",
    "        ```\n",
    "        Input1 --> [RNN Cell] --h0--> Input2 --> [RNN Cell] --h1--> Input3 --> [RNN Cell] --h2--> Output\n",
    "                                                                                        |\n",
    "                                                                                        V\n",
    "                                                                                      (Final Output)\n",
    "        ```\n",
    "\n",
    "* **d) Many-to-Many (Synchronized Sequence Input and Output):**\n",
    "    * Takes a sequence of inputs and produces a sequence of outputs, where each output corresponds to an input at that time step.\n",
    "    * Example: Part-of-speech tagging (input: sequence of words; output: sequence of POS tags for each word). Video classification on a frame-by-frame basis.\n",
    "    * **Conceptual Diagram:** (Same as the first general unrolled diagram)\n",
    "\n",
    "* **e) Many-to-Many (Delayed Sequence Input and Output - Encoder-Decoder):**\n",
    "    * Takes a sequence of inputs, processes it (encodes it into a context vector), and then generates a sequence of outputs. Input and output sequences can have different lengths.\n",
    "    * Example: Machine translation (input: sentence in one language; output: sentence in another language). Question answering.\n",
    "    * **Conceptual Diagram:**\n",
    "        ```\n",
    "        Input1 --> [Encoder RNN Cell] --> Input2 --> [Encoder RNN Cell] --> ... --> Context Vector (Final Hidden State of Encoder)\n",
    "                                                                                          |\n",
    "                                                                                          V\n",
    "                                          Start Token --> [Decoder RNN Cell] --> Output1 --> [Decoder RNN Cell] --> Output2 ...\n",
    "        ```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Training RNNs: Backpropagation Through Time (BPTT)**\n",
    "\n",
    "* Training RNNs involves a modified version of backpropagation called **Backpropagation Through Time (BPTT)**.\n",
    "* **How it works:**\n",
    "    1.  The RNN is \"unrolled\" for a certain number of time steps (the length of the input sequence, or a fixed number of steps for very long sequences).\n",
    "    2.  A forward pass is performed through the unrolled network to compute the outputs and the loss (e.g., sum of losses at each time step or loss at the final time step).\n",
    "    3.  The error is then propagated backward through the unrolled network, from the last time step to the first.\n",
    "    4.  Since the same weights ($W_{hh}, W_{xh}, W_{hy}$) are used at every time step in the unrolled network, the gradients for these shared weights are accumulated (summed up) across all time steps.\n",
    "    5.  Finally, the shared weights are updated using an optimizer based on these accumulated gradients.\n",
    "\n",
    "This covers the fundamental architecture of RNNs and how they process sequential data. The concept of the hidden state carrying information through time is key.\n",
    "\n",
    "---\n",
    "\n",
    "we've understood the groundwork for Recurrent Neural Networks (RNNs), understanding their structure with recurrent connections and hidden states, and how they are trained using Backpropagation Through Time (BPTT).\n",
    "\n",
    "However, training traditional \"vanilla\" RNNs (the simple RNN cells we discussed) on long sequences comes with significant challenges. The most prominent of these are the **Vanishing Gradient Problem** and the **Exploding Gradient Problem**.\n",
    "\n",
    "---\n",
    "\n",
    "**5. The Vanishing/Exploding Gradients Problem**\n",
    "\n",
    "These problems arise during the backpropagation process in deep networks, and they are particularly acute in RNNs due to the recurrent nature where the same weight matrices are applied repeatedly across many time steps.\n",
    "\n",
    "**a) The Vanishing Gradient Problem**\n",
    "\n",
    "* **What it is:** During backpropagation, gradients (error signals) are propagated backward from the output layer to the earlier layers (or earlier time steps in an unrolled RNN). If these gradients become extremely small as they are propagated back, the weights in the earlier layers (or for earlier time steps) will update very slowly, or not at all. The network essentially fails to learn long-range dependencies – it cannot effectively connect information from distant past time steps to the current prediction.\n",
    "\n",
    "* **Why it Happens in RNNs (Conceptual):**\n",
    "    * Recall that the hidden state $h(t)$ is computed using $h(t-1)$ and the input $x(t)$, often involving an activation function like `tanh`.\n",
    "    * During BPTT, to calculate the gradient of the loss with respect to $h(t-k)$ (a hidden state $k$ steps in the past), we effectively multiply a chain of Jacobian matrices (matrices of partial derivatives) related to the recurrent weight matrix $W_{hh}$ and the derivative of the activation function at each intermediate time step.\n",
    "    * If the activation function used (like `tanh` or `sigmoid`) has derivatives that are less than 1 in its operating range, and/or if the recurrent weights $W_{hh}$ are small (e.g., their largest singular value is less than 1), then repeatedly multiplying these small values over many time steps ($k$ times) will cause the gradient to shrink exponentially.\n",
    "        * Imagine multiplying many numbers less than 1: $0.5 \\times 0.5 \\times 0.5 \\times \\dots \\times 0.5 \\rightarrow 0$.\n",
    "    * **Conceptual Diagram (Gradient Flow in Unrolled RNN):**\n",
    "        ```\n",
    "        Loss(t) --> ... --> ∂L/∂h(t) --> (x W_hh') --> ∂L/∂h(t-1) --> (x W_hh') --> ∂L/∂h(t-2) --> ... --> ∂L/∂h(0)\n",
    "                                  (Small Factor)                 (Small Factor)                 (Vanishingly Small)\n",
    "        ```\n",
    "        (Where `W_hh'` represents the derivative term involving $W_{hh}$ and the activation function's derivative. If this factor is consistently < 1, the gradient diminishes).\n",
    "\n",
    "* **Consequences:**\n",
    "    * **Difficulty Learning Long-Range Dependencies:** The network struggles to learn relationships between events that are far apart in a sequence. For example, in a long sentence, the meaning of a word at the end might depend on a word at the beginning, but the vanishing gradient prevents this information from effectively influencing the earlier parts of the network during training.\n",
    "    * **Slow Training or Stagnation:** Earlier layers (or connections related to earlier time steps) learn very slowly or get stuck.\n",
    "    * The model effectively has a very short-term \"memory.\"\n",
    "\n",
    "**b) The Exploding Gradient Problem**\n",
    "\n",
    "* **What it is:** This is the opposite scenario. During backpropagation, if the gradients become extremely large as they are propagated backward, the weight updates will also be very large. This can lead to unstable training where the weights oscillate wildly and can even result in numerical overflow (weights becoming `NaN` - Not a Number - or infinity).\n",
    "* **Why it Happens in RNNs (Conceptual):**\n",
    "    * Similar to vanishing gradients, this occurs due to the repeated multiplication of Jacobian matrices during BPTT.\n",
    "    * If the recurrent weights $W_{hh}$ are large (e.g., their largest singular value is greater than 1), and/or the derivatives of the activation function are sometimes large, then repeatedly multiplying these large values can cause the gradient to grow exponentially.\n",
    "        * Imagine multiplying many numbers greater than 1: $1.5 \\times 1.5 \\times 1.5 \\times \\dots \\times 1.5 \\rightarrow \\infty$.\n",
    "    * **Conceptual Diagram (Gradient Flow in Unrolled RNN):**\n",
    "        ```\n",
    "        Loss(t) --> ... --> ∂L/∂h(t) --> (x W_hh') --> ∂L/∂h(t-1) --> (x W_hh') --> ∂L/∂h(t-2) --> ... --> ∂L/∂h(0)\n",
    "                                  (Large Factor)                 (Large Factor)                 (Explodingly Large)\n",
    "        ```\n",
    "\n",
    "* **Consequences:**\n",
    "    * **Unstable Training:** The loss function might fluctuate wildly or diverge.\n",
    "    * **Weights become NaN or Infinity:** Large gradient updates can push weights to extreme values, leading to numerical overflow.\n",
    "    * Training essentially fails.\n",
    "\n",
    "**Mitigation Strategies for Exploding Gradients:**\n",
    "\n",
    "* **Gradient Clipping:** This is a common and effective technique. If the norm (magnitude) of the gradient vector exceeds a predefined threshold during backpropagation, the gradient vector is scaled down to meet that threshold before the weight update. This prevents the gradients from becoming too large.\n",
    "    * **Conceptual Diagram:** Imagine a gradient vector pointing steeply uphill. If it's too long (norm > threshold), gradient clipping shortens it while keeping its direction, preventing an excessively large step.\n",
    "\n",
    "**Addressing Vanishing Gradients (The Need for More Advanced Architectures):**\n",
    "\n",
    "While gradient clipping helps with exploding gradients, vanishing gradients are a more fundamental problem for simple RNNs when learning long sequences. This issue was a major motivation for the development of more sophisticated recurrent architectures like:\n",
    "\n",
    "* **Long Short-Term Memory (LSTM) units**\n",
    "* **Gated Recurrent Units (GRUs)**\n",
    "\n",
    "These architectures have internal mechanisms (gates) that control the flow of information and gradients more effectively, allowing them to learn and remember information over much longer time scales.\n",
    "\n",
    "Understanding the vanishing and exploding gradient problems is crucial for appreciating why LSTMs and GRUs became so important and successful in sequence modeling.\n",
    "\n",
    "---\n",
    "\n",
    "we've discussed the challenges traditional RNNs face with vanishing and exploding gradients, which makes it difficult for them to learn long-range dependencies. This leads us directly to more advanced recurrent architectures designed to overcome these issues: **Long Short-Term Memory (LSTM)** units and **Gated Recurrent Units (GRUs)**.\n",
    "\n",
    "**6. Long Short-Term Memory (LSTM)**\n",
    "\n",
    "LSTMs were introduced by Hochreiter & Schmidhuber (1997) and are a type of RNN architecture specifically designed to remember information for long periods, making them excellent for learning long-term dependencies.\n",
    "\n",
    "* **Core Idea: Gated Cell Structure**\n",
    "    The key to LSTMs is their **cell state** and a series of **gates** that regulate the flow of information into and out of this cell state. These gates are like little neural networks themselves, with sigmoid activation functions (outputting values between 0 and 1) that control how much information is allowed to pass through.\n",
    "\n",
    "* **Key Components of an LSTM Cell:**\n",
    "\n",
    "    1.  **Cell State ($C_t$): The \"Memory Conveyor Belt\"**\n",
    "        * This is the core component that runs straight down the entire chain of LSTM units, with only minor linear interactions. It's like a conveyor belt that carries information through time.\n",
    "        * Information can be easily added to or removed from the cell state, regulated by the gates.\n",
    "        * This design helps information flow unchanged over long sequences, mitigating the vanishing gradient problem.\n",
    "\n",
    "    2.  **Forget Gate ($f_t$): Deciding What to Throw Away**\n",
    "        * **Purpose:** Decides what information from the *previous cell state* ($C_{t-1}$) should be discarded or kept.\n",
    "        * **Mechanism:** It looks at the previous hidden state $h_{t-1}$ and the current input $x_t$. It passes them through a sigmoid function. The output is a vector of numbers between 0 and 1 for each number in the cell state $C_{t-1}$.\n",
    "            * A '1' means \"completely keep this.\"\n",
    "            * A '0' means \"completely get rid of this.\"\n",
    "        * **Formula (conceptual):** $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "        * **Conceptual Diagram (Forget Gate):**\n",
    "            ```\n",
    "            h(t-1) ---↘\n",
    "                       [Sigmoid] ---(ft)--> [Element-wise multiply with C(t-1)] --> Information to potentially keep\n",
    "            x(t) ----↗      (Controls forgetting from old cell state)\n",
    "            ```\n",
    "\n",
    "    3.  **Input Gate ($i_t$): Deciding What New Information to Store**\n",
    "        * **Purpose:** Decides which new information from the current input $x_t$ and previous hidden state $h_{t-1}$ should be stored in the current cell state $C_t$.\n",
    "        * **Mechanism:** This has two parts:\n",
    "            a.  **Sigmoid Layer (Input Gate):** Decides which values to update.\n",
    "                $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "            b.  **Tanh Layer:** Creates a vector of new candidate values, $\\tilde{C}_t$, that *could* be added to the cell state.\n",
    "                $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
    "        * **Updating the Cell State:** The old cell state $C_{t-1}$ is updated to the new cell state $C_t$ by:\n",
    "            1.  Forgetting some old information (multiplying $C_{t-1}$ by $f_t$).\n",
    "            2.  Adding some new candidate information (multiplying $\\tilde{C}_t$ by $i_t$).\n",
    "            $$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "            (where $\\odot$ denotes element-wise multiplication)\n",
    "        * **Conceptual Diagram (Input Gate & Cell State Update):**\n",
    "            ```\n",
    "            h(t-1) ---↘            h(t-1) ---↘\n",
    "                       [Sigmoid] ---(it)-->      [Tanh] --- (Candidate Values ~Ct)\n",
    "            x(t) ----↗ (Input Gate)        x(t) ----↗ (New Info Creator)\n",
    "                               |                     |\n",
    "                               V                     V\n",
    "            (Old C(t-1) * ft) ---[+]---> New Cell State C(t) <--- (it * ~Ct)\n",
    "            ```\n",
    "\n",
    "    4.  **Output Gate ($o_t$): Deciding What to Output as the Hidden State**\n",
    "        * **Purpose:** Decides what part of the current cell state $C_t$ should be outputted as the hidden state $h_t$ for the current time step (which will also be fed to the next LSTM cell and potentially used for prediction).\n",
    "        * **Mechanism:**\n",
    "            a.  **Sigmoid Layer (Output Gate):** Decides which parts of the cell state to output.\n",
    "                $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "            b.  **Cell State through Tanh:** The current cell state $C_t$ is passed through a `tanh` function (to squash values between -1 and 1).\n",
    "            c.  **Final Hidden State:** The output of the sigmoid layer ($o_t$) is multiplied element-wise with the $\\tanh(C_t)$.\n",
    "                $$h_t = o_t \\odot \\tanh(C_t)$$\n",
    "        * **Conceptual Diagram (Output Gate):**\n",
    "            ```\n",
    "            h(t-1) ---↘\n",
    "                       [Sigmoid] ---(ot)-->\n",
    "            x(t) ----↗ (Output Gate)       |\n",
    "                                           V\n",
    "                         C(t) ---[Tanh]---[Element-wise multiply] ---> New Hidden State h(t) (and Output ŷ(t))\n",
    "            ```\n",
    "\n",
    "* **Overall LSTM Cell Structure (Simplified Diagram):**\n",
    "    ```\n",
    "    +-----------------------------------------------------------------------+\n",
    "    |                                                                       |\n",
    "    |  C(t-1) -------------------[Forget Gate (ft)]----(+)----[Input Gate (it)]---> C(t) | Cell State \"Conveyor Belt\"\n",
    "    |                                                  |      |                     |\n",
    "    |                                                  |      ^                     |\n",
    "    |                                                  |---(Candidate ~Ct)          |\n",
    "    |                                                                               |\n",
    "    |  h(t-1) --+---------------------[Forget Gate Sigmoid]                         |\n",
    "    |           |                                                                 |\n",
    "    |           +---------------------[Input Gate Sigmoid]                          |\n",
    "    |           |                                                                 |\n",
    "    |           +---------------------[Candidate Tanh]                            |\n",
    "    |           |                                                                 V\n",
    "    |           +---------------------[Output Gate Sigmoid]----(ot)----[*]------> h(t) --> To next cell & Output ŷ(t)\n",
    "    |                                                                 ^\n",
    "    |  x(t) ----+----------------------------------------------------| (C(t) through Tanh)\n",
    "    |           |\n",
    "    +-----------------------------------------------------------------------+\n",
    "    LSTM Cell at time t\n",
    "    ```\n",
    "    This gated mechanism allows LSTMs to selectively remember or forget information over long sequences, effectively combating the vanishing gradient problem. The cell state acts as a protected memory, and the gates control the flow of information into and out of this memory.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Gated Recurrent Units (GRUs)**\n",
    "\n",
    "GRUs, introduced by Cho et al. (2014), are a newer generation of gated RNNs that provide similar performance to LSTMs but with a simpler architecture (fewer gates and parameters).\n",
    "\n",
    "* **Core Idea:** GRUs also use gates to control information flow, but they combine the forget and input gates into a single **update gate** and merge the cell state and hidden state.\n",
    "* **Key Components of a GRU Cell:**\n",
    "\n",
    "    1.  **Update Gate ($z_t$):**\n",
    "        * **Purpose:** Decides how much of the past information (previous hidden state $h_{t-1}$) to keep and how much new information (candidate hidden state $\\tilde{h}_t$) to add. It acts like a combination of LSTM's forget and input gates.\n",
    "        * **Mechanism:** Looks at $h_{t-1}$ and $x_t$.\n",
    "            $$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$$\n",
    "            * $z_t$ values are between 0 and 1. A value close to 1 means \"mostly keep the old state,\" and a value close to 0 means \"mostly use the new candidate state.\"\n",
    "\n",
    "    2.  **Reset Gate ($r_t$):**\n",
    "        * **Purpose:** Decides how much of the past information ($h_{t-1}$) to *forget* when computing the current candidate hidden state $\\tilde{h}_t$.\n",
    "        * **Mechanism:** Looks at $h_{t-1}$ and $x_t$.\n",
    "            $$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$$\n",
    "            * If $r_t$ is close to 0, it means \"ignore the previous hidden state almost completely\" when creating the new candidate.\n",
    "\n",
    "    3.  **Candidate Hidden State ($\\tilde{h}_t$):**\n",
    "        * **Purpose:** Proposes a new hidden state based on the current input $x_t$ and a *reset* version of the previous hidden state.\n",
    "        * **Mechanism:**\n",
    "            $$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)$$\n",
    "            (The reset gate $r_t$ controls how much of $h_{t-1}$ influences $\\tilde{h}_t$).\n",
    "\n",
    "    4.  **Final Hidden State ($h_t$):**\n",
    "        * **Purpose:** Combines the previous hidden state $h_{t-1}$ and the candidate hidden state $\\tilde{h}_t$ using the update gate $z_t$.\n",
    "        * **Mechanism:**\n",
    "            $$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "            * If $z_t \\approx 1$, then $h_t \\approx \\tilde{h}_t$ (mostly new candidate).\n",
    "            * If $z_t \\approx 0$, then $h_t \\approx h_{t-1}$ (mostly old state).\n",
    "        * This $h_t$ is also the output for the current time step.\n",
    "\n",
    "* **Overall GRU Cell Structure (Simplified Diagram):**\n",
    "    ```\n",
    "    +-----------------------------------------------------------------------+\n",
    "    |                                                                       |\n",
    "    |  h(t-1) --+---------------------[Update Gate (zt) Sigmoid]----(+)----> h(t) --> To next cell & Output ŷ(t)\n",
    "    |           |                                                    ^      |\n",
    "    |           |                                                    |----(1-zt)\n",
    "    |           |                                                    |\n",
    "    |           +---------------------[Reset Gate (rt) Sigmoid]------(rt)\n",
    "    |           |                                                    |\n",
    "    |           |                                                    V\n",
    "    |           +---------------------[Candidate Hidden State ~ht Tanh]----(zt)\n",
    "    |                                      ^      ^\n",
    "    |                                      |      | (rt * h(t-1))\n",
    "    |  x(t) ----+--------------------------+------+\n",
    "    |           |\n",
    "    +-----------------------------------------------------------------------+\n",
    "    GRU Cell at time t\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "**8. LSTM vs. GRU**\n",
    "\n",
    "* **Complexity:** GRUs are simpler than LSTMs (fewer gates, fewer parameters). This can make them computationally slightly faster and require less data to train effectively.\n",
    "* **Performance:** Their performance is often comparable on many tasks. There's no definitive winner; the best choice can be task-dependent.\n",
    "    * LSTMs, with their separate cell state, might be slightly better at tasks requiring remembering very long sequences or very fine-grained control over memory.\n",
    "    * GRUs are often a good choice when computational resources are more limited or for datasets where their simpler structure is sufficient.\n",
    "* **Common Practice:** It's common to try both and see which performs better for a specific problem. LSTMs are often a good default starting point due to their historical success and robustness.\n",
    "\n",
    "Both LSTMs and GRUs are powerful solutions to the vanishing gradient problem, enabling RNNs to effectively model long-range dependencies in sequential data. They form the backbone of many state-of-the-art sequence modeling applications.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc42b8f8-7a83-4f2a-9247-d15d59c66e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
