{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb831fc0-df28-4382-86ee-b9e25dab7496",
   "metadata": {},
   "source": [
    "let's explore **Topic 13: Decision Trees**. These are intuitive and powerful models used for both classification and regression tasks, known for their interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3c7bc-0b91-41f7-b1b8-6ef18faeeccd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**1. Introduction: What are Decision Trees?**\n",
    "\n",
    "* **Versatile Models:** Decision Trees are supervised learning algorithms that can predict a target variable by learning simple decision rules inferred from the data features. They can be used for:\n",
    "    * **Classification tasks:** Predicting a discrete class label (e.g., \"spam\" or \"not spam\").\n",
    "    * **Regression tasks:** Predicting a continuous numerical value (e.g., price of a house).\n",
    "* **Structure (Conceptual Diagram):**\n",
    "    Imagine an **upside-down tree** or a **flowchart**.\n",
    "    * It starts with a **root node** at the top, representing the entire dataset.\n",
    "    * This root node branches out into several **internal nodes** (or decision nodes). Each internal node represents a \"test\" or a \"question\" about a specific feature (e.g., \"Is Petal Width < 0.8 cm?\").\n",
    "    * Each **branch** extending from an internal node represents an outcome or answer to that test (e.g., \"Yes\" or \"No\"; or for numerical features, a range like \"< 0.8 cm\" vs. \">= 0.8 cm\").\n",
    "    * The branches lead either to other internal nodes (further questions) or to **leaf nodes** (also called terminal nodes).\n",
    "    * **Leaf nodes** represent the final outcome or decision.\n",
    "        * In a **classification tree**, each leaf node corresponds to a class label (e.g., \"Iris-setosa\").\n",
    "        * In a **regression tree**, each leaf node corresponds to a predicted continuous value (e.g., an average value like \"$250,000\").\n",
    "* **Interpretability (\"White Box\" Model):** One of the biggest advantages of decision trees is their interpretability. The decision rules are explicit and easy to understand, unlike \"black box\" models like complex neural networks or SVMs with non-linear kernels.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Tree Structure Terminology**\n",
    "\n",
    "Let's formalize the parts of a decision tree:\n",
    "\n",
    "* **Root Node:** The topmost node where the decision-making process begins. It contains all the training samples.\n",
    "* **Internal Node (Decision Node):** A node that performs a test on a feature and splits the data into two or more subsets (child nodes) based on the outcome of the test.\n",
    "* **Branch (Edge):** A link between nodes, representing the outcome of a test.\n",
    "* **Leaf Node (Terminal Node):** A node that does not split further. It represents a final decision:\n",
    "    * For classification: Contains a class label. The prediction for an instance reaching this leaf is that class label.\n",
    "    * For regression: Contains a continuous value (typically the average of the target values of the training instances that reached this leaf).\n",
    "* **Parent Node:** A node that is split into child nodes.\n",
    "* **Child Node:** Nodes that are created as a result of a split from a parent node.\n",
    "* **Depth of a Node:** The number of edges on the path from the root node to that node.\n",
    "* **Depth of a Tree:** The depth of its deepest leaf node (length of the longest path from the root to a leaf).\n",
    "\n",
    "**Text-based Diagram Example (Simple Classification Tree):**\n",
    "Imagine we are classifying fruits based on color and size:\n",
    "\n",
    "```\n",
    "Is Color == Red?\n",
    "|--- Yes: Is Size < 10cm?\n",
    "|      |--- Yes: Leaf (Class: Cherry)\n",
    "|      |--- No:  Leaf (Class: Apple)\n",
    "|--- No: Is Color == Yellow?\n",
    "       |--- Yes: Leaf (Class: Banana)\n",
    "       |--- No:  Leaf (Class: Grape)  (assuming other colors lead here)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. How Decision Trees Work (Building the Tree - Recursive Partitioning)**\n",
    "\n",
    "Decision trees are typically built using a **greedy, top-down, recursive partitioning** approach.\n",
    "\n",
    "1.  **Start at the Root:** Begin with all training samples at the root node.\n",
    "2.  **Find the Best Split:** For the current node, examine all possible features and all possible split points (thresholds for numerical features, or categories for categorical features). The goal is to find the feature and split point that results in the \"best\" split.\n",
    "    * \"Best\" split means it divides the data into subsets (child nodes) that are as **\"pure\"** as possible with respect to the target variable. A pure node ideally contains samples from only one class (for classification) or samples with very similar target values (for regression).\n",
    "3.  **Create Child Nodes:** Based on the best split, create child nodes and move the corresponding subsets of data into these child nodes.\n",
    "4.  **Recurse:** Repeat steps 2 and 3 for each newly created child node.\n",
    "5.  **Stopping Condition:** The recursion stops for a branch when one of the following conditions is met:\n",
    "    * The node is perfectly pure (all samples belong to the same class or have identical target values).\n",
    "    * A predefined stopping criterion is met (e.g., maximum tree depth reached, minimum number of samples in a node for splitting, etc.). These are hyperparameters used to control tree growth and prevent overfitting.\n",
    "    * No split further improves the purity of the nodes significantly.\n",
    "\n",
    "This algorithm is **greedy** because it makes the best local choice at each step (the split that looks best at the current node) without looking ahead to see if this choice will lead to a globally optimal tree.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a187c-e7c6-4b62-9198-1a66d9e9bb74",
   "metadata": {},
   "source": [
    "\n",
    "**4. Splitting Criteria (Measuring Purity or Impurity)**\n",
    "\n",
    "How does the algorithm decide which split is \"best\"? It uses a criterion to measure the purity (or impurity) of a node.\n",
    "\n",
    "**a) For Classification Trees:**\n",
    "\n",
    "The goal is to create child nodes where samples predominantly belong to a single class.\n",
    "\n",
    "* **Gini Impurity:**\n",
    "    * Measures the probability of misclassifying a randomly chosen element from the node if it were randomly labeled according to the distribution of labels in that node.\n",
    "    * For a node $t$ with $K$ classes, and $p(C_k|t)$ being the proportion of samples of class $C_k$ in node $t$:\n",
    "        $$Gini(t) = 1 - \\sum_{k=1}^{K} [p(C_k|t)]^2$$\n",
    "    * **Interpretation:**\n",
    "        * $Gini(t) = 0$: Perfectly pure node (all samples belong to a single class).\n",
    "        * $Gini(t) = 0.5$ (for binary classification): Maximally impure node (50/50 split of classes).\n",
    "    * The algorithm selects the split that results in the **lowest weighted average Gini impurity** of the child nodes (i.e., maximizes the Gini gain or reduction in impurity).\n",
    "    * **Conceptual Diagram for Purity:** Imagine a basket of fruits.\n",
    "        * Basket A: 10 Apples, 0 Oranges (Pure, Gini = 0).\n",
    "        * Basket B: 5 Apples, 5 Oranges (Impure, Gini = 0.5).\n",
    "        A good split would separate an impure basket into purer ones.\n",
    "\n",
    "* **Entropy / Information Gain:**\n",
    "    * **Entropy** is a measure of uncertainty or disorder in a set of samples.\n",
    "        $$Entropy(t) = -\\sum_{k=1}^{K} p(C_k|t) \\log_2(p(C_k|t))$$\n",
    "        (Conventionally, $0 \\log_2 0 = 0$).\n",
    "    * **Interpretation:**\n",
    "        * $Entropy(t) = 0$: Perfectly pure node.\n",
    "        * $Entropy(t) = 1$ (for binary classification, $\\log_2$): Maximally impure node.\n",
    "    * **Information Gain (IG):** The reduction in entropy achieved by splitting the data on a particular feature. The algorithm chooses the split that **maximizes information gain**.\n",
    "        $$IG(Parent, Split) = Entropy(Parent) - \\sum_{j \\in \\text{children}} \\frac{N_j}{N_{Parent}} Entropy(Child_j)$$\n",
    "        where $N_j$ is the number of samples in child node $j$, and $N_{Parent}$ is the number of samples in the parent node.\n",
    "\n",
    "* **Gini vs. Entropy:** Both are common and usually lead to similar trees. Gini impurity is often slightly faster to compute as it doesn't involve logarithmic calculations. Scikit-learn uses Gini by default for classification.\n",
    "\n",
    "**b) For Regression Trees:**\n",
    "\n",
    "The goal is to create child nodes where the target values of the samples are as similar as possible (i.e., have low variance).\n",
    "\n",
    "* **Mean Squared Error (MSE):**\n",
    "    * The algorithm tries to find splits that **minimize the MSE** within each resulting child node.\n",
    "    * For a node $t$ with $N_t$ samples, and $\\bar{y}_t$ being the average target value of samples in node $t$:\n",
    "        $$MSE(t) = \\frac{1}{N_t} \\sum_{i \\in t} (y_i - \\bar{y}_t)^2$$\n",
    "    * The prediction made at a leaf node is typically the average of the target values of the training instances in that leaf.\n",
    "* **Mean Absolute Error (MAE):** Less common but can also be used. It's less sensitive to outliers than MSE.\n",
    "\n",
    "Scikit-learn uses MSE by default for regression (referred to as \"squared_error\").\n",
    "\n",
    "---\n",
    "This covers the basic structure, how trees are built, and the splitting criteria. Next, we'll look at controlling tree growth (hyperparameters and pruning), visualization, pros/cons, and then the Scikit-learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5d5c7-bc3e-4d4b-9773-7543b45eac20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
