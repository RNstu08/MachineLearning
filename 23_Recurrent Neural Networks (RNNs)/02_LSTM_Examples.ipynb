{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29ca37c-fe7e-4975-8ef6-311bc074733d",
   "metadata": {},
   "source": [
    "- We'll do two examples:\n",
    "\n",
    "- An LSTM for a sequence classification task using synthetic data.\n",
    "- A GRU for a time series regression task (predicting future values of a sine wave)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d15401a-3e0a-4e68-9feb-f2da660138f3",
   "metadata": {},
   "source": [
    "---\n",
    "**A. LSTM for Sequence Classification (Synthetic Data)**\n",
    "\n",
    "In this example, we'll generate synthetic sequences where the class label depends on whether a specific sub-sequence pattern appears. This will demonstrate how an LSTM can learn to recognize patterns over time steps.\n",
    "\n",
    "Task: Classify sequences as Class 0 or Class 1. A sequence is Class 1 if it contains the sub-sequence [1, 1, 0] at any point, otherwise it's Class 0. All other elements will be random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9ca70-b627-471b-963e-e9c0a273dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Hyperparameters ---\n",
    "sequence_length = 20  # Length of each input sequence\n",
    "feature_size = 1      # Each element in the sequence is a single number (0 or 1 for simplicity, or random)\n",
    "hidden_size_lstm = 64 # Number of features in the hidden state of LSTM\n",
    "num_layers_lstm = 1   # Number of recurrent layers (stacked LSTMs)\n",
    "num_classes_seq = 2   # Binary classification (pattern present or not)\n",
    "num_epochs_seq = 30\n",
    "batch_size_seq = 32\n",
    "learning_rate_seq = 0.001\n",
    "\n",
    "# --- 3. Generate Synthetic Sequential Data ---\n",
    "print(\"Generating synthetic sequential data...\")\n",
    "def generate_sequence(seq_len, pattern=[1, 1, 0]):\n",
    "    contains_pattern = False\n",
    "    # Decide if this sequence will contain the pattern\n",
    "    if np.random.rand() > 0.5: # 50% chance to contain pattern\n",
    "        contains_pattern = True\n",
    "        pattern_len = len(pattern)\n",
    "        if seq_len < pattern_len:\n",
    "            # Fallback if sequence is too short for pattern (should not happen with current settings)\n",
    "            sequence = np.random.randint(0, 2, seq_len)\n",
    "            contains_pattern = False # Re-evaluate based on actual sequence\n",
    "            for i in range(len(sequence) - pattern_len + 1):\n",
    "                if list(sequence[i:i+pattern_len]) == pattern:\n",
    "                    contains_pattern = True\n",
    "                    break\n",
    "            return sequence, 1 if contains_pattern else 0\n",
    "\n",
    "\n",
    "        start_idx = np.random.randint(0, seq_len - pattern_len + 1)\n",
    "        sequence = np.random.randint(0, 2, seq_len) # Fill with random 0s and 1s\n",
    "        sequence[start_idx : start_idx + pattern_len] = pattern\n",
    "    else:\n",
    "        sequence = np.random.randint(0, 2, seq_len)\n",
    "        # Double check if random sequence accidentally contains pattern\n",
    "        pattern_len = len(pattern)\n",
    "        for i in range(len(sequence) - pattern_len + 1):\n",
    "            if list(sequence[i:i+pattern_len]) == pattern:\n",
    "                contains_pattern = True # It does, so label it as 1\n",
    "                break\n",
    "    return sequence, 1 if contains_pattern else 0\n",
    "\n",
    "num_samples_seq = 2000\n",
    "sequences = []\n",
    "labels = []\n",
    "for _ in range(num_samples_seq):\n",
    "    seq, label = generate_sequence(sequence_length)\n",
    "    sequences.append(seq)\n",
    "    labels.append(label)\n",
    "\n",
    "sequences = np.array(sequences, dtype=np.float32).reshape(-1, sequence_length, feature_size)\n",
    "labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "print(f\"Generated {num_samples_seq} sequences of length {sequence_length}.\")\n",
    "print(f\"Example sequence (shape {sequences[0].shape}):\\n{sequences[0].ravel()}\")\n",
    "print(f\"Label for example: {labels[0]}\")\n",
    "print(f\"Class distribution: Class 0: {np.sum(labels==0)}, Class 1: {np.sum(labels==1)}\")\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
    "    sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train_tensor_seq = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "y_train_tensor_seq = torch.tensor(y_train_seq, dtype=torch.long) # CrossEntropyLoss expects long for labels\n",
    "X_test_tensor_seq = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "y_test_tensor_seq = torch.tensor(y_test_seq, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset_seq = TensorDataset(X_train_tensor_seq, y_train_tensor_seq)\n",
    "test_dataset_seq = TensorDataset(X_test_tensor_seq, y_test_tensor_seq)\n",
    "\n",
    "train_loader_seq = DataLoader(dataset=train_dataset_seq, batch_size=batch_size_seq, shuffle=True)\n",
    "test_loader_seq = DataLoader(dataset=test_dataset_seq, batch_size=batch_size_seq, shuffle=False)\n",
    "\n",
    "# --- 4. Define the LSTM Model for Sequence Classification ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # LSTM layer:\n",
    "        #   input_size: The number of expected features in the input x\n",
    "        #   hidden_size: The number of features in the hidden state h\n",
    "        #   num_layers: Number of recurrent layers.\n",
    "        #   batch_first=True: Means input/output tensors are (batch, seq, feature)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state with zeros\n",
    "        # h0 shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        # c0 shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size) containing output features from the last layer of LSTM\n",
    "        # (hn, cn): tuple of hidden state and cell state for t = seq_length\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # We only need the output from the last time step for classification\n",
    "        # out shape: (batch_size, seq_len, hidden_size) -> out[:, -1, :] shape: (batch_size, hidden_size)\n",
    "        out = self.fc(out[:, -1, :]) # Decode the hidden state of the last time step\n",
    "        return out\n",
    "\n",
    "# --- 5. Instantiate the Model, Loss, and Optimizer ---\n",
    "model_lstm = LSTMClassifier(feature_size, hidden_size_lstm, num_layers_lstm, num_classes_seq).to(device)\n",
    "print(\"\\nLSTM Model Architecture:\")\n",
    "print(model_lstm)\n",
    "\n",
    "criterion_seq = nn.CrossEntropyLoss()\n",
    "optimizer_seq = optim.Adam(model_lstm.parameters(), lr=learning_rate_seq)\n",
    "\n",
    "# --- 6. Training Loop ---\n",
    "print(\"\\nStarting LSTM Training...\")\n",
    "train_losses_seq = []\n",
    "for epoch in range(num_epochs_seq):\n",
    "    model_lstm.train()\n",
    "    running_loss_seq = 0.0\n",
    "    for i, (seq_batch, labels_batch) in enumerate(train_loader_seq):\n",
    "        seq_batch = seq_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        outputs = model_lstm(seq_batch)\n",
    "        loss = criterion_seq(outputs, labels_batch)\n",
    "\n",
    "        optimizer_seq.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_seq.step()\n",
    "        running_loss_seq += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss_seq / len(train_loader_seq)\n",
    "    train_losses_seq.append(epoch_loss)\n",
    "    if (epoch+1) % 5 == 0 or epoch == num_epochs_seq -1 :\n",
    "      print(f'Epoch [{epoch+1}/{num_epochs_seq}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "print(\"Finished LSTM Training.\")\n",
    "\n",
    "# --- 7. Evaluation ---\n",
    "print(\"\\nStarting LSTM Evaluation...\")\n",
    "model_lstm.eval()\n",
    "all_labels_lstm, all_predicted_lstm = [], []\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for seq_batch, labels_batch in test_loader_seq:\n",
    "        seq_batch = seq_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        outputs = model_lstm(seq_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels_batch.size(0)\n",
    "        n_correct += (predicted == labels_batch).sum().item()\n",
    "        all_labels_lstm.extend(labels_batch.cpu().numpy())\n",
    "        all_predicted_lstm.extend(predicted.cpu().numpy())\n",
    "\n",
    "accuracy_lstm = 100.0 * n_correct / n_samples\n",
    "print(f'Accuracy of the LSTM on test sequences: {accuracy_lstm:.2f} %')\n",
    "\n",
    "print(\"\\nConfusion Matrix (LSTM):\")\n",
    "cm_lstm = confusion_matrix(all_labels_lstm, all_predicted_lstm)\n",
    "sns.heatmap(cm_lstm, annot=True, fmt=\"d\", cmap=\"cividis\", xticklabels=[\"No Pattern\", \"Pattern\"], yticklabels=[\"No Pattern\", \"Pattern\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - LSTM Sequence Classification\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report (LSTM):\")\n",
    "print(classification_report(all_labels_lstm, all_predicted_lstm, target_names=[\"No Pattern\", \"Pattern\"]))\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses_seq, label='Training Loss')\n",
    "plt.title('LSTM Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CrossEntropy Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db21fe-faa2-4714-b8a1-c349969af683",
   "metadata": {},
   "source": [
    "# Discussion: LSTM for Sequence Classification Example\n",
    "\n",
    "This document discusses the key components and rationale behind using a Long Short-Term Memory (LSTM) network for a sequence classification task, as illustrated in the provided example.\n",
    "\n",
    "## 1. Data Generation Strategy\n",
    "- **Task**: The synthetic dataset is designed to create a sequence classification problem where the label is contingent on identifying a specific temporal pattern or sub-sequence (e.g., `[1,1,0]`) within a longer sequence.\n",
    "- **Purpose**: This type of data forces the LSTM to learn and remember information over time steps, which is the core strength of Recurrent Neural Networks (RNNs) and LSTMs. It's not just about individual elements but their order and relationship.\n",
    "\n",
    "## 2. Input Shape for `nn.LSTM`\n",
    "- **Requirement**: The `nn.LSTM` layer in PyTorch, when `batch_first=True`, expects input tensors to have a specific shape: `(batch_size, sequence_length, feature_size)`.\n",
    "    - `batch_size`: The number of sequences processed in parallel.\n",
    "    - `sequence_length`: The number of time steps in each sequence.\n",
    "    - `feature_size`: The dimensionality of the input features at each time step.\n",
    "- **Example Adaptation**: In the described scenario, if each element in the sequence is a single number, `feature_size` would be 1. The raw sequences are reshaped to meet this `(batch_size, seq_len, 1)` format before being fed into the LSTM.\n",
    "\n",
    "## 3. The `nn.LSTM` Layer\n",
    "- **Core Component**: `nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)` is the heart of the sequence processing model.\n",
    "- **Key Parameters**:\n",
    "    - `input_size`: The number of expected features in the input `x` at each time step (dimensionality of each element in the sequence). In the example, this is 1.\n",
    "    - `hidden_size`: The number of features in the hidden state `h` (and also the cell state `c`). This determines the capacity of the LSTM to store information.\n",
    "    - `num_layers (int, optional)`: The number of recurrent layers. E.g., setting `num_layers=2` would mean stacking two LSTMs together to form a \"stacked LSTM,\" with the second LSTM taking in outputs of the first LSTM and computing the final results. This can allow for learning more complex temporal hierarchies (default is 1).\n",
    "    - `batch_first (bool, optional)`: If `True`, then the input and output tensors are provided as `(batch, seq, feature)` instead of `(seq, batch, feature)`. `True` is often more intuitive (default is `False`).\n",
    "    - `dropout (float, optional)`: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to `dropout`.\n",
    "    - `bidirectional (bool, optional)`: If `True`, becomes a bidirectional LSTM (processes sequence from start-to-end and end-to-start).\n",
    "\n",
    "## 4. Initialization of Hidden and Cell States (`h0`, `c0`)\n",
    "- **Requirement**: LSTMs (and other RNNs) require an initial hidden state (`h_0`) and, for LSTMs specifically, an initial cell state (`c_0`) at the beginning of processing each sequence.\n",
    "- **Common Practice**: For each batch, these initial states are typically initialized to tensors of zeros.\n",
    "    - Shape of `h_0`: `(num_layers * num_directions, batch_size, hidden_size)`\n",
    "    - Shape of `c_0`: `(num_layers * num_directions, batch_size, hidden_size)`\n",
    "- **Action**: Before processing a new batch of sequences, new zero-tensors for `h_0` and `c_0` are created and passed to the LSTM layer along with the input batch. These must be moved to the correct device (CPU/GPU).\n",
    "\n",
    "## 5. Using the LSTM Output for Classification\n",
    "- **LSTM Output Structure**: The `nn.LSTM` layer returns:\n",
    "    1.  `output`: A tensor containing the output features (`h_t`) from the last layer of the LSTM, for each time step. If `batch_first=True`, its shape is `(batch_size, sequence_length, num_directions * hidden_size)`.\n",
    "    2.  `(h_n, c_n)`: A tuple containing the final hidden state and final cell state for each element in the batch.\n",
    "        - `h_n` shape: `(num_layers * num_directions, batch_size, hidden_size)`\n",
    "        - `c_n` shape: `(num_layers * num_directions, batch_size, hidden_size)`\n",
    "- **Strategy for Sequence Classification**: A common approach is to use the hidden state of the LSTM from the *last time step* as a summary or encoding of the entire sequence.\n",
    "    - This can be obtained either from `output[:, -1, :]` (which takes the last time step's output from all layers if `num_layers=1`, or the last layer's output at the last time step) or from `h_n` (by selecting the appropriate layer's final hidden state, often the last layer if `num_layers > 1`).\n",
    "    - For a uni-directional, single-layer LSTM, `output[:, -1, :]` is equivalent to `h_n.squeeze(0)` (if `num_layers * num_directions = 1`).\n",
    "- **Final Classification**: This last hidden state vector (e.g., `output[:, -1, :]`) is then fed into one or more fully connected (`nn.Linear`) layers, followed by an appropriate activation function (e.g., Sigmoid for binary classification, Softmax via `nn.CrossEntropyLoss` for multi-class) to produce the final class predictions.\n",
    "\n",
    "## 6. Evaluation Metrics\n",
    "- **Standard Approach**: For sequence classification tasks, standard classification metrics are used, similar to those in image or tabular data classification.\n",
    "- **Examples**:\n",
    "    - **Accuracy**: The proportion of correctly classified sequences.\n",
    "    - **Precision, Recall, F1-score**: Especially useful for imbalanced datasets.\n",
    "    - **Confusion Matrix**: To visualize performance across different classes.\n",
    "    - **Loss Value**: The value from the chosen loss function (e.g., `nn.BCELossWithLogits` for binary, `nn.CrossEntropyLoss` for multi-class) is also a key indicator.\n",
    "\n",
    "## Summary\n",
    "This example using an LSTM for sequence classification demonstrates how to leverage the memory capabilities of LSTMs to understand temporal dependencies in data. Key aspects include correctly shaping the input, initializing hidden/cell states, and strategically using the LSTM's output (often the final hidden state) as a feature representation for a subsequent classification layer. The synthetic data generation method ensures the task genuinely requires learning sequential patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64846d0-5951-4c62-9824-c37e8a319748",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A. More Complex LSTM for Sequence Classification**\n",
    "\n",
    "- Task: We'll generate sequences where each element is a small vector. The classification task will be to determine if a sequence contains a specific \"trigger\" pattern followed by an \"action\" pattern within a certain window, making it a more complex temporal dependency problem than just detecting a single sub-sequence.\n",
    "\n",
    "- Complexity Additions:\n",
    "\n",
    "- Input features per time step are vectors.\n",
    "- More complex sequential pattern for classification.\n",
    "- Stacked Bidirectional LSTM.\n",
    "- Dropout for regularization.\n",
    "- Validation loop during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59210a2c-5e56-4a06-a187-bad98d317536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Hyperparameters ---\n",
    "sequence_length = 30\n",
    "feature_size = 5     # Each element in the sequence is a 5-dim vector\n",
    "hidden_size_lstm = 128\n",
    "num_layers_lstm = 2  # Stacked LSTM\n",
    "num_classes_seq = 2  # Binary: specific complex pattern present or not\n",
    "num_epochs_seq = 40\n",
    "batch_size_seq = 64\n",
    "learning_rate_seq = 0.001\n",
    "dropout_lstm = 0.4\n",
    "\n",
    "# --- 3. Generate Complex Synthetic Sequential Data ---\n",
    "print(\"Generating complex synthetic sequential data...\")\n",
    "# Define patterns: trigger_pattern followed by action_pattern within a window\n",
    "trigger_pattern = np.array([[1,0,0,0,0], [0,1,0,0,0]], dtype=np.float32) # Example 2-step trigger\n",
    "action_pattern = np.array([[0,0,1,0,0], [0,0,0,1,0]], dtype=np.float32)  # Example 2-step action\n",
    "max_gap_between_patterns = 5 # Max steps between end of trigger and start of action\n",
    "\n",
    "def generate_complex_sequence(seq_len, feat_size, trigger, action, max_gap):\n",
    "    sequence = np.random.rand(seq_len, feat_size).astype(np.float32) * 0.5 # Background noise\n",
    "    label = 0 # Default: pattern not present\n",
    "\n",
    "    if np.random.rand() > 0.5: # 50% chance to attempt inserting the complex pattern\n",
    "        trigger_len = trigger.shape[0]\n",
    "        action_len = action.shape[0]\n",
    "        \n",
    "        if seq_len >= trigger_len + action_len + 1: # Ensure enough space\n",
    "            # Try to place trigger\n",
    "            trigger_start = np.random.randint(0, seq_len - trigger_len - action_len - max_gap +1)\n",
    "            sequence[trigger_start : trigger_start + trigger_len] = trigger\n",
    "            \n",
    "            # Try to place action after trigger within the gap\n",
    "            action_start_min = trigger_start + trigger_len\n",
    "            action_start_max = min(seq_len - action_len, trigger_start + trigger_len + max_gap)\n",
    "            \n",
    "            if action_start_min <= action_start_max :\n",
    "                action_start = np.random.randint(action_start_min, action_start_max + 1)\n",
    "                if action_start + action_len <= seq_len:\n",
    "                    sequence[action_start : action_start + action_len] = action\n",
    "                    label = 1 # Pattern successfully inserted\n",
    "    return sequence, label\n",
    "\n",
    "num_samples_seq = 3000\n",
    "sequences, labels = [], []\n",
    "for _ in range(num_samples_seq):\n",
    "    seq, label = generate_complex_sequence(sequence_length, feature_size, trigger_pattern, action_pattern, max_gap_between_patterns)\n",
    "    sequences.append(seq)\n",
    "    labels.append(label)\n",
    "\n",
    "sequences = np.array(sequences, dtype=np.float32)\n",
    "labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "print(f\"Generated {num_samples_seq} sequences. Feature size per step: {feature_size}\")\n",
    "print(f\"Class distribution: Class 0: {np.sum(labels==0)}, Class 1: {np.sum(labels==1)}\")\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train_val, X_test_seq, y_train_val, y_test_seq = train_test_split(\n",
    "    sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15, random_state=42, stratify=y_train_val # 0.15 of 0.8 = 0.12\n",
    ")\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train_tensor_seq = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "y_train_tensor_seq = torch.tensor(y_train_seq, dtype=torch.long)\n",
    "X_val_tensor_seq = torch.tensor(X_val_seq, dtype=torch.float32)\n",
    "y_val_tensor_seq = torch.tensor(y_val_seq, dtype=torch.long)\n",
    "X_test_tensor_seq = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "y_test_tensor_seq = torch.tensor(y_test_seq, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset_seq = TensorDataset(X_train_tensor_seq, y_train_tensor_seq)\n",
    "val_dataset_seq = TensorDataset(X_val_tensor_seq, y_val_tensor_seq)\n",
    "test_dataset_seq = TensorDataset(X_test_tensor_seq, y_test_tensor_seq)\n",
    "\n",
    "train_loader_seq = DataLoader(dataset=train_dataset_seq, batch_size=batch_size_seq, shuffle=True)\n",
    "val_loader_seq = DataLoader(dataset=val_dataset_seq, batch_size=batch_size_seq, shuffle=False)\n",
    "test_loader_seq = DataLoader(dataset=test_dataset_seq, batch_size=batch_size_seq, shuffle=False)\n",
    "\n",
    "# --- 4. Define the Complex LSTM Model ---\n",
    "class ComplexLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(ComplexLSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout_prob if num_layers > 1 else 0,\n",
    "                            bidirectional=True) # Bidirectional LSTM\n",
    "        \n",
    "        # Adjust linear layer input size for bidirectional LSTM\n",
    "        # Output of bidirectional LSTM is (batch, seq, 2 * hidden_size)\n",
    "        # We typically take the concatenation of the last hidden states from forward and backward passes\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes) # Multiply hidden_size by 2\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h0 and c0 shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        # num_directions is 2 for bidirectional\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # For bidirectional, out is (batch, seq_len, hidden_size * 2)\n",
    "        # We can take the output of the last time step from both directions.\n",
    "        # One way is to concatenate the last hidden state of the forward pass\n",
    "        # and the first hidden state of the backward pass (which is the last time step's backward output).\n",
    "        # Or, more simply, just use the full output of the last time step which already combines them.\n",
    "        # out[:, -1, :] will have dimension hidden_size * 2\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# --- 5. Instantiate the Model, Loss, and Optimizer ---\n",
    "model_lstm_complex = ComplexLSTMClassifier(feature_size, hidden_size_lstm, num_layers_lstm, num_classes_seq, dropout_lstm).to(device)\n",
    "print(\"\\nComplex LSTM Model Architecture:\")\n",
    "print(model_lstm_complex)\n",
    "\n",
    "criterion_seq_complex = nn.CrossEntropyLoss()\n",
    "optimizer_seq_complex = optim.Adam(model_lstm_complex.parameters(), lr=learning_rate_seq)\n",
    "\n",
    "# --- 6. Training Loop with Validation ---\n",
    "print(\"\\nStarting Complex LSTM Training...\")\n",
    "train_losses_c_lstm, val_losses_c_lstm = [], []\n",
    "train_accs_c_lstm, val_accs_c_lstm = [], []\n",
    "\n",
    "for epoch in range(num_epochs_seq):\n",
    "    model_lstm_complex.train()\n",
    "    running_train_loss, n_correct_train, n_samples_train = 0.0, 0, 0\n",
    "    for seq_batch, labels_batch in train_loader_seq:\n",
    "        seq_batch, labels_batch = seq_batch.to(device), labels_batch.to(device)\n",
    "        outputs = model_lstm_complex(seq_batch)\n",
    "        loss = criterion_seq_complex(outputs, labels_batch)\n",
    "        optimizer_seq_complex.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_seq_complex.step()\n",
    "        running_train_loss += loss.item() * seq_batch.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples_train += labels_batch.size(0)\n",
    "        n_correct_train += (predicted == labels_batch).sum().item()\n",
    "    \n",
    "    epoch_train_loss = running_train_loss / n_samples_train\n",
    "    epoch_train_acc = 100.0 * n_correct_train / n_samples_train\n",
    "    train_losses_c_lstm.append(epoch_train_loss)\n",
    "    train_accs_c_lstm.append(epoch_train_acc)\n",
    "\n",
    "    model_lstm_complex.eval()\n",
    "    running_val_loss, n_correct_val, n_samples_val = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for seq_batch_val, labels_batch_val in val_loader_seq:\n",
    "            seq_batch_val, labels_batch_val = seq_batch_val.to(device), labels_batch_val.to(device)\n",
    "            outputs_val = model_lstm_complex(seq_batch_val)\n",
    "            loss_val = criterion_seq_complex(outputs_val, labels_batch_val)\n",
    "            running_val_loss += loss_val.item() * seq_batch_val.size(0)\n",
    "            _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "            n_samples_val += labels_batch_val.size(0)\n",
    "            n_correct_val += (predicted_val == labels_batch_val).sum().item()\n",
    "            \n",
    "    epoch_val_loss = running_val_loss / n_samples_val\n",
    "    epoch_val_acc = 100.0 * n_correct_val / n_samples_val\n",
    "    val_losses_c_lstm.append(epoch_val_loss)\n",
    "    val_accs_c_lstm.append(epoch_val_acc)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs_seq}], Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, '\n",
    "          f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n",
    "\n",
    "print(\"Finished Complex LSTM Training.\")\n",
    "\n",
    "# --- 7. Evaluation on Test Set ---\n",
    "# (Similar to previous examples, calculate accuracy, confusion matrix, classification report)\n",
    "model_lstm_complex.eval()\n",
    "all_labels_lstm_c, all_predicted_lstm_c = [], []\n",
    "with torch.no_grad():\n",
    "    n_correct_test, n_samples_test = 0,0\n",
    "    for sb, lb in test_loader_seq:\n",
    "        sb, lb = sb.to(device), lb.to(device)\n",
    "        outs = model_lstm_complex(sb)\n",
    "        _, p = torch.max(outs.data,1)\n",
    "        n_samples_test += lb.size(0)\n",
    "        n_correct_test += (p == lb).sum().item()\n",
    "        all_labels_lstm_c.extend(lb.cpu().numpy())\n",
    "        all_predicted_lstm_c.extend(p.cpu().numpy())\n",
    "accuracy_lstm_c = 100.0 * n_correct_test / n_samples_test\n",
    "print(f'\\nAccuracy of Complex LSTM on test sequences: {accuracy_lstm_c:.2f} %')\n",
    "# Plotting and detailed reports can be added here as in previous examples\n",
    "\n",
    "# --- 8. Plot Training and Validation Loss and Accuracy ---\n",
    "epochs_range = range(1, num_epochs_seq + 1)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_losses_c_lstm, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses_c_lstm, 'ro-', label='Validation Loss')\n",
    "plt.title('Complex LSTM: Training & Validation Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_accs_c_lstm, 'bs-', label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accs_c_lstm, 'rs-', label='Validation Accuracy')\n",
    "plt.title('Complex LSTM: Training & Validation Accuracy')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy (%)'); plt.legend(); plt.grid(True)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15567b-17bc-4aa4-a718-37ff6a50ded1",
   "metadata": {},
   "source": [
    "# Discussion: Complex LSTM for Sequence Classification Example\n",
    "\n",
    "This document discusses the key components and enhancements in a more complex Long Short-Term Memory (LSTM) network designed for sequence classification, particularly when dealing with multi-featured sequences and intricate temporal dependencies.\n",
    "\n",
    "## 1. Data Characteristics\n",
    "- **Multi-Feature Time Steps**: Unlike simpler examples where each time step might have a single feature, the input sequences here consist of multiple features at each time step. This means `input_size` for the LSTM will be greater than 1.\n",
    "- **Complex Temporal Relationships**: The classification task is designed to depend on more sophisticated temporal patterns. For instance, the label might be determined by the occurrence of a specific sequence of patterns (e.g., \"pattern A\" must be followed by \"pattern B\" with some variable delay or intervening elements). This requires the LSTM to learn longer-range and more nuanced dependencies.\n",
    "\n",
    "## 2. Model Architecture (`ComplexLSTMClassifier`)\n",
    "- **Bidirectional LSTM (`nn.LSTM(..., bidirectional=True)`)**:\n",
    "    - **Mechanism**: A key enhancement is the use of a bidirectional LSTM. This involves two separate LSTMs:\n",
    "        1.  One processes the input sequence in the forward direction (from the first time step to the last).\n",
    "        2.  The other processes the input sequence in the backward direction (from the last time step to the first).\n",
    "    - **Output Concatenation**: The hidden states (outputs) from the forward LSTM at each time step are typically concatenated with the hidden states from the backward LSTM at the corresponding time step.\n",
    "    - **Benefit**: This allows the LSTM at any given time step to have information from both past and future contexts within the sequence. This can be very powerful for tasks where understanding the full context around a particular point is crucial for classification.\n",
    "    - **Impact on Subsequent Layers**: If the `hidden_size` of the LSTM is `H`, the concatenated output for each time step will have a dimensionality of `hidden_size * 2`. This means the `in_features` for the subsequent `nn.Linear` layer (if using the output of the bidirectional LSTM) needs to be adjusted accordingly.\n",
    "\n",
    "- **Dropout within `nn.LSTM` (`dropout` parameter)**:\n",
    "    - **Functionality**: If `num_layers > 1` (i.e., a stacked LSTM) and the `dropout` parameter in the `nn.LSTM` constructor is set to a value greater than 0, dropout layers are automatically added on the outputs of each LSTM layer *except the last one*.\n",
    "    - **Purpose**: This helps to regularize the connections between stacked LSTM layers, preventing them from becoming too co-dependent and improving generalization.\n",
    "\n",
    "- **Additional `nn.Dropout` Layer**:\n",
    "    - **Placement**: A standard `nn.Dropout` layer is typically added *after* the LSTM layers (or after concatenating outputs from a bidirectional LSTM) and *before* the final fully connected (`nn.Linear`) classification layer.\n",
    "    - **Purpose**: This provides further regularization to the features that are fed into the classifier, reducing the risk of overfitting on the training data.\n",
    "\n",
    "## 3. Training Process Enhancements\n",
    "- **Validation Loop**:\n",
    "    - **Integration**: The training process incorporates a validation loop that is executed after each training epoch.\n",
    "    - **Mechanism**:\n",
    "        1.  The model is switched to evaluation mode (`model.eval()`). This is critical as it disables dropout layers and ensures layers like Batch Normalization (if used) use their learned running statistics.\n",
    "        2.  Performance metrics (e.g., loss, accuracy) are computed on a separate validation dataset (unseen during the training updates of that epoch).\n",
    "        3.  All computations within the validation loop are performed under `with torch.no_grad():` to disable gradient tracking, saving memory and computation.\n",
    "    - **Benefits**:\n",
    "        - **Overfitting Detection**: By comparing training performance with validation performance, one can identify if the model is starting to overfit (e.g., training loss decreases while validation loss increases, or training accuracy improves while validation accuracy stagnates/drops).\n",
    "        - **Hyperparameter Tuning**: Validation performance guides the tuning of hyperparameters.\n",
    "        - **Early Stopping Potential**: Although not explicitly implemented in the base example, the validation metrics provide the signal needed for early stopping criteria (i.e., stopping training when validation performance no longer improves or starts to degrade, to prevent overfitting and save computational resources).\n",
    "\n",
    "## Summary\n",
    "This \"Complex LSTM\" example demonstrates how to build more powerful sequence classifiers by incorporating bidirectionality for richer contextual understanding and various dropout techniques for robust regularization. The inclusion of a validation loop is a critical best practice for developing reliable models, allowing for ongoing performance monitoring on unseen data and strategies to combat overfitting. These enhancements make the model better suited for challenging sequence classification tasks with multi-featured inputs and complex temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c8c3d-8a2f-47b0-9af8-4cef1d916a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
