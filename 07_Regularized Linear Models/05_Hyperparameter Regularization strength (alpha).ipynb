{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c580a89-5ec6-4530-ac12-50f9907efdb6",
   "metadata": {},
   "source": [
    "let's discuss the practical aspects of **choosing the right regularization strength (`alpha`)** and then summarize **when to choose between Ridge, Lasso, and Elastic Net**.\n",
    "\n",
    "---\n",
    "\n",
    "**Part 1: Choosing the Right Regularization Strength (`alpha` and `l1_ratio`)**\n",
    "\n",
    "The regularization strength `alpha` (and `l1_ratio` for Elastic Net) is a hyperparameter, meaning it's not learned from the data directly by the model during the `fit` process. Instead, we need to choose a value for it that results in the best model performance on unseen data.\n",
    "\n",
    "1.  **The Challenge:**\n",
    "    * If `alpha` is too small: The regularization effect is weak, and the model might still overfit (high variance).\n",
    "    * If `alpha` is too large: The penalty on coefficients is too strong, leading the model to underfit (high bias), as coefficients (even for important features) are shrunk too much.\n",
    "    Our goal is to find an optimal `alpha` that balances this bias-variance trade-off to minimize the error on unseen data.\n",
    "\n",
    "2.  **Why Not Use the Test Set for Tuning?**\n",
    "    It might seem tempting to try different `alpha` values, train models, and see which one performs best on our held-out test set. **This is a mistake.** If you use the test set to tune hyperparameters, you are effectively leaking information from the test set into your model selection process. Your chosen `alpha` will be optimized for that specific test set, and your final performance estimate will be overly optimistic and not a true reflection of how the model will perform on genuinely new, unseen data. The test set must be reserved for a single, final evaluation of the *chosen* model.\n",
    "\n",
    "3.  **Cross-Validation (CV): The Standard Approach**\n",
    "    The most common and robust method for hyperparameter tuning is cross-validation, performed on the *training data*.\n",
    "    * **Concept:**\n",
    "        1.  Split your training data into $K$ \"folds\" (e.g., $K=5$ or $K=10$).\n",
    "        2.  For a given hyperparameter value (e.g., a specific `alpha`):\n",
    "            * Iterate $K$ times:\n",
    "                * In each iteration `i`, train your model on $K-1$ folds.\n",
    "                * Validate (evaluate) the trained model on the remaining fold `i` (the validation fold).\n",
    "            * Calculate the average performance metric (e.g., average MSE or R-squared) across all $K$ validation folds.\n",
    "        3.  Repeat step 2 for all candidate hyperparameter values you want to test.\n",
    "        4.  Select the hyperparameter value that resulted in the best average cross-validated performance.\n",
    "\n",
    "4.  **Scikit-learn's CV Estimators:**\n",
    "    Scikit-learn provides convenient estimators that have built-in cross-validation capabilities for finding the best `alpha` (and `l1_ratio` for Elastic Net):\n",
    "    * **`RidgeCV`**: We saw this takes a list of `alphas`. It trains a Ridge model for each `alpha` using cross-validation and then selects the `alpha` that performs best. The chosen `alpha` is stored in `ridge_cv_model.alpha_`.\n",
    "    * **`LassoCV`**: Similar to `RidgeCV`, but for Lasso. It also stores the best `alpha` in `lasso_cv_model.alpha_`.\n",
    "    * **`ElasticNetCV`**: This one is particularly useful as it can search for both the best `alpha` and the best `l1_ratio`. You can provide a list of `alphas` and a list of `l1_ratios`. It will find the combination that performs best under cross-validation, storing them in `en_cv_model.alpha_` and `en_cv_model.l1_ratio_`.\n",
    "\n",
    "    These `*CV` models make hyperparameter tuning very straightforward for regularized linear models.\n",
    "\n",
    "5.  **Grid Search with Cross-Validation (More General):**\n",
    "    If a model doesn't have a dedicated `*CV` version, or if you want to tune multiple hyperparameters not covered by the dedicated estimator (or for other types of models), Scikit-learn offers `GridSearchCV` and `RandomizedSearchCV` from `sklearn.model_selection`.\n",
    "    * **`GridSearchCV`**: You define a \"grid\" of hyperparameter values you want to test. `GridSearchCV` will then evaluate every possible combination of these hyperparameters using K-fold cross-validation.\n",
    "        ```python\n",
    "        # Example for ElasticNet if not using ElasticNetCV's l1_ratio search directly\n",
    "        # from sklearn.model_selection import GridSearchCV\n",
    "        # from sklearn.linear_model import ElasticNet\n",
    "        #\n",
    "        # en_model = ElasticNet(max_iter=10000)\n",
    "        # param_grid = {\n",
    "        #     'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "        #     'l1_ratio': [0.1, 0.5, 0.7, 0.9, 0.99, 1.0]\n",
    "        # }\n",
    "        # grid_search = GridSearchCV(en_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        # grid_search.fit(X_train, y_train)\n",
    "        # print(f\"Best parameters from GridSearchCV: {grid_search.best_params_}\")\n",
    "        # best_en_model = grid_search.best_estimator_\n",
    "        ```\n",
    "    * **`RandomizedSearchCV`**: Useful when the hyperparameter search space is very large. Instead of trying all combinations, it samples a fixed number of combinations randomly.\n",
    "\n",
    "6.  **Visualizing the Effect of Alpha:**\n",
    "    It can be insightful to plot how the model's error (on both training and validation sets) changes with different values of `alpha`.\n",
    "    * Typically, training error will decrease or stay low as `alpha` decreases (less regularization, model fits training data better).\n",
    "    * Validation error often shows a U-shaped curve:\n",
    "        * High error for very small `alpha` (model overfits).\n",
    "        * High error for very large `alpha` (model underfits).\n",
    "        * A minimum point in between, representing the optimal `alpha`.\n",
    "    The `*CV` estimators in Scikit-learn (like `RidgeCV` if `store_cv_values=True`) can sometimes provide access to these cross-validated scores for plotting.\n",
    "\n",
    "---\n",
    "\n",
    "**Part 2: Summarizing the Choice Between Ridge, Lasso, and Elastic Net**\n",
    "\n",
    "Once you know how to tune `alpha` (and `l1_ratio`), how do you decide which regularization technique to use?\n",
    "\n",
    "| Feature/Consideration    | Ridge (L2)                                     | Lasso (L1)                                                            | Elastic Net (L1+L2)                                                  |\n",
    "| :----------------------- | :--------------------------------------------- | :-------------------------------------------------------------------- | :------------------------------------------------------------------- |\n",
    "| **Coefficient Behavior** | Shrinks all coefficients towards zero.         | Shrinks some coefficients to *exactly* zero.                          | Shrinks coefficients; some can become exactly zero.                  |\n",
    "| **Feature Selection** | No (keeps all features).                       | Yes (performs automatic feature selection).                           | Yes (performs feature selection).                                    |\n",
    "| **Multicollinearity** | Handles it well; distributes effect among correlated features. | Can be unstable; may arbitrarily pick one among correlated features. | Good compromise; often groups correlated features (selects/discards together). |\n",
    "| **Sparsity of Solution** | Non-sparse.                                    | Sparse.                                                               | Can be sparse.                                                       |\n",
    "| **Primary Use Case** | General overfitting prevention, when most features are likely relevant, multicollinearity. | High-dimensional data, when many features are suspected to be irrelevant, desire for a simpler/interpretable model. | When benefits of both Lasso and Ridge are desired; many correlated features; robust feature selection. |\n",
    "| **Hyperparameters** | `alpha`                                        | `alpha`                                                               | `alpha` and `l1_ratio`                                               |\n",
    "| **Computational Notes** | Closed-form solution exists (but usually solved iteratively with large `p`). | Requires specialized iterative solvers (e.g., coordinate descent).    | Requires specialized iterative solvers.                              |\n",
    "\n",
    "**Practical Guidelines:**\n",
    "\n",
    "1.  **Starting Point:** `Ridge` is often a good first choice. It's robust and can improve upon OLS if there's some overfitting or multicollinearity.\n",
    "2.  **If you need feature selection or suspect many features are irrelevant:**\n",
    "    * `Lasso` is a strong candidate. It can simplify your model significantly.\n",
    "    * `ElasticNet` can also perform feature selection and might be more stable than Lasso if you have groups of highly correlated features.\n",
    "3.  **If you have high multicollinearity:**\n",
    "    * `Ridge` handles this well by shrinking coefficients of correlated variables together.\n",
    "    * `ElasticNet` is also very good here and generally preferred over Lasso if you want to retain groups of correlated features rather than having Lasso pick one somewhat arbitrarily.\n",
    "4.  **If you have more features than samples ($p > n$):**\n",
    "    * OLS is ill-defined. `Lasso` and `ElasticNet` are particularly useful in this scenario as they can select a subset of features. `Ridge` can also be used.\n",
    "5.  **When in doubt, try multiple approaches:** It's common practice to try `Ridge`, `Lasso`, and `ElasticNet`, tune their hyperparameters using cross-validation, and then select the model that yields the best cross-validated performance.\n",
    "6.  **Consider the `l1_ratio` in Elastic Net:**\n",
    "    * If `ElasticNetCV` consistently picks an `l1_ratio` close to 1, it suggests a Lasso-like model is best.\n",
    "    * If it picks an `l1_ratio` close to 0 (e.g., 0.01, 0.1), it suggests a Ridge-like model is better.\n",
    "    * Intermediate values suggest a true mix is optimal.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Model Training and Evaluation:**\n",
    "1.  **Tune Hyperparameters:** Use cross-validation (e.g., `RidgeCV`, `LassoCV`, `ElasticNetCV`, or `GridSearchCV`) on your **training dataset** to find the best `alpha` (and `l1_ratio`).\n",
    "2.  **Train Final Model:** Once the best hyperparameters are identified, train your chosen model (Ridge, Lasso, or Elastic Net with these best hyperparameters) on the **entire training dataset**.\n",
    "3.  **Evaluate on Test Set:** Finally, evaluate the performance of this trained model on the **held-out test set**. This provides an unbiased estimate of how well your model is likely to perform on new, unseen data.\n",
    "\n",
    "This concludes our discussion on regularized linear models! We've covered why we need them, the mechanisms of Ridge, Lasso, and Elastic Net, how they prevent overfitting, and how to choose the right model and tune its strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235806a1-2967-4b53-8b5b-99dc3aec7e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
