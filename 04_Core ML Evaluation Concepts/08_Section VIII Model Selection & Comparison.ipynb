{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "967b0029-6c88-4ec2-8003-1813088b5ba3",
   "metadata": {},
   "source": [
    "### VIII. Model Selection & Comparison\n",
    "\n",
    "This section outlines the process of using evaluation results to select the best model configuration and perform a final performance assessment.\n",
    "\n",
    "#### 1. The Goal: Choosing the Best Performing Model\n",
    "\n",
    "After exploring different algorithms (e.g., `Logistic Regression`, `SVC`, `Random Forest`) and/or tuning the `hyperparameters` of one or more algorithms (using techniques like `GridSearchCV` or `RandomizedSearchCV` with `cross-validation`), you will have performance estimates (e.g., mean `CV` `accuracy`, mean `CV` `F1-score`, mean `CV` `MSE`) for various model configurations.\n",
    "\n",
    "The goal of model selection is to use these performance estimates, obtained on the validation folds during `cross-validation` (or on a dedicated validation set), to choose the single model configuration (algorithm + specific `hyperparameters`) that is expected to generalize best to new, unseen data.\n",
    "\n",
    "#### 2. Using Cross-Validation Results for Selection\n",
    "\n",
    "* **Primary Metric:** Decide on the primary evaluation metric that best reflects the goals of your project (e.g., `accuracy`, `F1-score` for imbalanced classification, `AUC`, `MAE`, `RMSE`, `RÂ²`).\n",
    "* **Compare Mean CV Scores:** Compare the average `cross-validation` scores for your chosen metric across the different models/`hyperparameter` settings you tested. The configuration with the best average score is typically the leading candidate.\n",
    "* **Consider Score Variability (Standard Deviation):** Look at the standard deviation of the scores across the `CV` folds. A model with a slightly lower average score but much lower standard deviation might be more reliable or stable than one with a slightly higher average but very high variability.\n",
    "* **Other Factors:** Consider computational cost (training/prediction time), model interpretability, and specific business constraints when making the final choice, especially if performance differences are small.\n",
    "\n",
    "**Example Scenario:**\n",
    "Suppose you used `GridSearchCV` with 5-fold `CV` to tune an `SVC` and a `RandomForestClassifier`, optimizing for `accuracy`:\n",
    "\n",
    "* Best `SVC` configuration: Mean `CV` Accuracy = 0.95 +/- 0.02\n",
    "* Best `RandomForest` config: Mean `CV` Accuracy = 0.96 +/- 0.04\n",
    "\n",
    "Based purely on mean `accuracy`, `RandomForest` seems slightly better. However, its performance is slightly more variable across folds (higher std dev). You might choose `RandomForest` if the absolute best performance is critical, or `SVC` if stability is more important, or investigate further if the difference isn't statistically significant.\n",
    "\n",
    "#### 3. Final Evaluation on the Test Set\n",
    "\n",
    "* **Purpose:** To get a final, unbiased estimate of the chosen model's generalization performance.\n",
    "* **CRITICAL:** The `test set` (e.g., `X_final_test`, `y_final_test` created during the initial data split) must only be used at this final stage. It should never have been used for training, hyperparameter tuning, or model selection decisions. Using it earlier invalidates it as an unbiased measure.\n",
    "* **Steps:**\n",
    "    1.  **Identify the best model configuration:** Based on the `cross-validation` results on the `training`/`validation` data (e.g., the `best_estimator_` attribute from `GridSearchCV`).\n",
    "    2.  **Retrain the best model:** Train this chosen model configuration on the entire `training` + `validation` dataset (e.g., `X_train_val`, `y_train_val` from Section II, or `X_train`, `y_train` if no separate validation set was used but `CV` was performed on the `training set`). This allows the model to learn from as much data as possible before final testing.\n",
    "    3.  **Evaluate on the Test Set:** Make predictions on the held-out `test set` (`X_final_test`) and calculate the chosen evaluation metric(s) by comparing predictions to the true test labels (`y_final_test`).\n",
    "    4.  **Reporting:** The performance score obtained on the `test set` is the reported estimate of how well your model is expected to perform on new, unseen data.\n",
    "\n",
    "```python\n",
    "# --- Conceptual Code Outline for Final Evaluation ---\n",
    "\n",
    "# Assume 'best_model' is the chosen estimator after CV/tuning\n",
    "# Assume X_train_val, y_train_val is the full training+validation set\n",
    "# Assume X_final_test, y_final_test is the held-out test set\n",
    "# Assume preprocessing steps (scaler, encoder) are part of best_model if it's a Pipeline,\n",
    "# or need to be applied consistently if not using a pipeline.\n",
    "\n",
    "# 1. Retrain the best model on the full training+validation data\n",
    "# best_model.fit(X_train_val, y_train_val) # Or fit the pipeline\n",
    "\n",
    "# 2. Make predictions on the final test set\n",
    "# y_final_pred = best_model.predict(X_final_test)\n",
    "\n",
    "# 3. Calculate final performance metric(s)\n",
    "# from sklearn.metrics import accuracy_score # or other relevant metric\n",
    "# final_score = accuracy_score(y_final_test, y_final_pred)\n",
    "\n",
    "# print(f\"Final performance estimate on the held-out test set: {final_score:.4f}\")\n",
    "# --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cd4af-9f38-4711-93e8-1884cbbe9fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
