{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5504077d-e2bf-4dc3-b2c1-61a5dc145d39",
   "metadata": {},
   "source": [
    "**Topic 17: Clustering - Hierarchical Clustering**.\n",
    "\n",
    "Unlike K-Means, where we have to specify the number of clusters ($K$) beforehand, Hierarchical Clustering builds a hierarchy of clusters, which can be visualized as a tree-like structure called a dendrogram. This dendrogram can then be cut at different levels to obtain different numbers of clusters.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Introduction: What is Hierarchical Clustering?**\n",
    "\n",
    "* **Goal:** To build a hierarchy of clusters, where clusters are nested within each other. It doesn't require pre-specifying the number of clusters.\n",
    "* **Output:** The primary output is a **dendrogram**, which is a tree diagram that shows the arrangement of the clusters produced by the corresponding analyses.\n",
    "* **Two Main Approaches:**\n",
    "    * **Agglomerative (Bottom-up):** Starts with each data point as its own cluster and iteratively merges the closest pair of clusters until only one cluster (containing all data points) remains. This is the more common approach.\n",
    "    * **Divisive (Top-down):** Starts with all data points in a single cluster and recursively splits the cluster into smaller sub-clusters until each data point is its own cluster or some stopping criterion is met. This is less common due to its computational complexity.\n",
    "\n",
    "**Conceptual Diagram of a Dendrogram:**\n",
    "Imagine a tree structure where:\n",
    "* The **leaves** at the bottom represent individual data points.\n",
    "* As you move up the tree, branches merge, representing the merging of clusters.\n",
    "* The **height** of the merge indicates the distance (or dissimilarity) at which those clusters were combined. Longer vertical lines mean the clusters were further apart when merged.\n",
    "* The **root** at the top represents a single cluster containing all data points.\n",
    "\n",
    "```\n",
    "      |-----------------------------------------| (All points: Cluster A+B+C+D+E)\n",
    "      |                                         |\n",
    "      |                            |------------| (Cluster D+E)\n",
    "      |                            |            |\n",
    "      |                |-----------| (C+D+E)    DE\n",
    "      |                |           |            |\n",
    "      |------| (A+B)   C           D            E\n",
    "      |      |\n",
    "      A      B\n",
    "(Individual data points at the bottom)\n",
    "```\n",
    "*By \"cutting\" the dendrogram horizontally at a certain height, you can determine the number of clusters.* For example, cutting the above dendrogram at a height that intersects three vertical lines would result in three clusters: (A+B), C, and (D+E).\n",
    "\n",
    "---\n",
    "\n",
    "**2. Agglomerative Hierarchical Clustering (Bottom-Up)**\n",
    "\n",
    "This is the most widely used type of hierarchical clustering.\n",
    "\n",
    "* **Algorithm Steps:**\n",
    "    1.  **Initialization:** Start by treating each of the $N$ data points as an individual cluster. So, you begin with $N$ clusters.\n",
    "    2.  **Iteration (Merging):**\n",
    "        a.  **Calculate Proximity Matrix:** Compute the distance (or dissimilarity) between every pair of clusters. Initially, this is just the distance between individual data points.\n",
    "        b.  **Find Closest Pair:** Identify the two closest (most similar) clusters based on the proximity matrix.\n",
    "        c.  **Merge:** Merge these two clusters into a new, single cluster.\n",
    "        d.  **Update Proximity Matrix:** Update the proximity matrix to reflect the distances between this new cluster and the remaining clusters (and between the remaining clusters themselves).\n",
    "    3.  **Repeat:** Repeat Step 2 until only one cluster (containing all data points) remains.\n",
    "\n",
    "* **Key Element: Linkage Criteria**\n",
    "    A crucial part of agglomerative clustering is how the distance between two *clusters* (which might contain multiple points) is defined. This is determined by the **linkage criterion**.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Linkage Criteria (Defining Inter-Cluster Distance)**\n",
    "\n",
    "Once we have clusters with more than one point, we need a way to measure the distance between them. Different linkage criteria can lead to different cluster structures. Let $d(p, q)$ be the distance between two individual points $p$ and $q$.\n",
    "\n",
    "* **a) Single Linkage (MIN):**\n",
    "    * The distance between two clusters is defined as the **minimum distance** between any single data point in the first cluster and any single data point in the second cluster.\n",
    "    * $D(\\text{Cluster}_A, \\text{Cluster}_B) = \\min_{p \\in \\text{Cluster}_A, q \\in \\text{Cluster}_B} d(p, q)$\n",
    "    * **Effect:** Tends to produce long, \"chain-like\" clusters. It can sometimes connect distant clusters if there's just one close pair of points between them (sensitive to outliers or \"bridges\" between groups).\n",
    "    * **Conceptual Diagram:** Imagine two elongated clusters. Single linkage would measure the distance between their closest tips.\n",
    "\n",
    "* **b) Complete Linkage (MAX):**\n",
    "    * The distance between two clusters is defined as the **maximum distance** between any single data point in the first cluster and any single data point in the second cluster.\n",
    "    * $D(\\text{Cluster}_A, \\text{Cluster}_B) = \\max_{p \\in \\text{Cluster}_A, q \\in \\text{Cluster}_B} d(p, q)$\n",
    "    * **Effect:** Tends to produce more compact, spherical clusters. It tries to keep all points within a cluster relatively close to each other. Less sensitive to outliers than single linkage.\n",
    "    * **Conceptual Diagram:** For the same two elongated clusters, complete linkage would measure the distance between their furthest tips.\n",
    "\n",
    "* **c) Average Linkage (UPGMA - Unweighted Pair Group Method with Arithmetic Mean):**\n",
    "    * The distance between two clusters is defined as the **average distance** between all pairs of data points, where one point is from the first cluster and the other is from the second cluster.\n",
    "    * $D(\\text{Cluster}_A, \\text{Cluster}_B) = \\frac{1}{|C_A| \\cdot |C_B|} \\sum_{p \\in C_A} \\sum_{q \\in C_B} d(p, q)$\n",
    "    * **Effect:** Often provides a balance between single and complete linkage. Tends to produce relatively compact clusters but is less susceptible to outliers than single linkage.\n",
    "\n",
    "* **d) Ward's Linkage (Minimum Variance Method):**\n",
    "    * This method aims to **minimize the increase in the total within-cluster variance** when two clusters are merged.\n",
    "    * At each step, it merges the pair of clusters that leads to the minimum increase in the sum of squared errors (SSE or WCSS, similar to K-Means' inertia).\n",
    "    * **Effect:** Tends to produce compact, spherical clusters of roughly equal sizes. It's a popular and often effective method. It's sensitive to outliers because it uses squared errors.\n",
    "    * **Note:** Ward's linkage is typically used with Euclidean distance.\n",
    "\n",
    "* **Other Linkage Methods:** Centroid linkage (distance between cluster centroids), Median linkage, etc.\n",
    "\n",
    "The choice of linkage criterion can significantly impact the final clustering structure (the shape of the dendrogram).\n",
    "\n",
    "---\n",
    "\n",
    "- let's continue with Hierarchical Clustering. We've covered the more common agglomerative (bottom-up) approach and its linkage criteria.\n",
    "\n",
    "**4. Divisive Hierarchical Clustering (Top-Down)**\n",
    "\n",
    "* **Approach:** Divisive clustering takes the opposite approach to agglomerative clustering.\n",
    "    1.  **Initialization:** Start with all data points belonging to a single, large cluster (the root of the hierarchy).\n",
    "    2.  **Iteration (Splitting):**\n",
    "        a.  At each step, the most heterogeneous cluster (the one with the most internal variation or spread) is selected.\n",
    "        b.  This selected cluster is then split into two (or more) smaller, more homogeneous sub-clusters. The challenge here is how to decide the best way to split a cluster, which can be computationally intensive as there are many possibilities.\n",
    "    3.  **Repeat:** This process is recursively applied to the new sub-clusters until each data point forms its own cluster, or some stopping criterion (like a desired number of clusters or a minimum cluster size) is met.\n",
    "* **Complexity:** Divisive methods are generally more computationally complex than agglomerative methods. This is because, at each step, there's a vast number of ways to split a cluster. For $N$ data points, there are $2^{N-1} - 1$ possible ways to divide them into two non-empty subsets.\n",
    "* **Use Cases:** While less common than agglomerative methods, divisive clustering can sometimes find more accurate hierarchies, especially if the global structure is more important than local details.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Dendrograms: Visualizing and Choosing the Number of Clusters**\n",
    "\n",
    "The primary output of a hierarchical clustering algorithm is a **dendrogram**, which is a tree-like diagram that graphically represents the hierarchy of clusters.\n",
    "\n",
    "* **Structure of a Dendrogram (Conceptual Diagram - Revisited):**\n",
    "\n",
    "    ```\n",
    "          |-----------------------------------------|  <-- Level 0 (All points in one cluster)\n",
    "          |                                         |\n",
    "          |                            |------------|  <-- Level 1 (e.g., D & E merge)\n",
    "          |                            |            |\n",
    "          |                |-----------| (C merges with D+E) <-- Level 2\n",
    "          |                |           |            |\n",
    "          |------| (A & B merge)       C           DE\n",
    "          |      |                                  |\n",
    "          A      B                                 D E\n",
    "    (Individual data points at the bottom - Leaves)\n",
    "    ```\n",
    "    * **Leaves:** Represent individual data points.\n",
    "    * **Branches:** Show how clusters are merged (in agglomerative) or split (in divisive).\n",
    "    * **Height of Merge/Split:** The vertical axis of the dendrogram represents the distance (or dissimilarity) at which clusters were merged or the criterion value at which a split occurred.\n",
    "        * Longer vertical lines indicate that the merged clusters were further apart (more dissimilar).\n",
    "        * Shorter vertical lines indicate that the merged clusters were closer (more similar).\n",
    "\n",
    "* **Choosing the Number of Clusters from a Dendrogram:**\n",
    "    One of the main advantages of hierarchical clustering is that you don't need to specify the number of clusters beforehand. You can decide on the number of clusters *after* the hierarchy is built by examining the dendrogram.\n",
    "    1.  **Horizontal Cut:** Imagine drawing a horizontal line across the dendrogram.\n",
    "    2.  **Counting Intersections:** The number of vertical lines that this horizontal line intersects determines the number of clusters.\n",
    "    3.  **Where to Cut?**\n",
    "        * **Longest Vertical Lines:** A common heuristic is to look for the longest vertical lines that are not intersected by your chosen horizontal cut. Cutting just above these long lines often suggests a natural separation into clusters, as these lines represent merges between relatively dissimilar groups.\n",
    "        * **Desired Number of Clusters:** If you have a specific number of clusters in mind (e.g., based on domain knowledge), you can find the horizontal cut that yields that number.\n",
    "        * **Distance Threshold:** You can decide on a maximum inter-cluster distance you're willing to tolerate. Cut the dendrogram at that distance threshold on the vertical axis.\n",
    "\n",
    "    **Conceptual Diagram (Cutting the Dendrogram):**\n",
    "    ```\n",
    "          |-----------------------------------------|\n",
    "          |                                         |\n",
    "          |                            |------------|\n",
    "          |                            |            |\n",
    "    ------|----------------|-----------|------------------ Horizontal Cut (Yields 3 Clusters)\n",
    "          |                |           |            |\n",
    "          |------|         C           DE\n",
    "          |      |\n",
    "          A      B\n",
    "    ```\n",
    "    In this diagram, the horizontal cut intersects three main vertical segments leading down to clusters, resulting in three clusters: (A,B), (C), and (DE).\n",
    "\n",
    "* **Interpretation:** Dendrograms provide a rich visualization of the data's structure, showing not just the final clusters but also how they are related and formed at different levels of similarity.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Feature Scaling**\n",
    "\n",
    "Just like K-Means, hierarchical clustering algorithms that use distance metrics (which is most of them, especially for the proximity matrix calculation) are **sensitive to feature scaling**.\n",
    "* If features have vastly different scales or units, features with larger values will dominate the distance calculations.\n",
    "* Therefore, it's crucial to **scale or normalize your features** (e.g., using `StandardScaler` or `MinMaxScaler`) before applying hierarchical clustering to ensure that all features contribute appropriately to the distance measurements.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Advantages and Disadvantages of Hierarchical Clustering**\n",
    "\n",
    "Hierarchical clustering offers a different approach to grouping data compared to partitional methods like K-Means, and this comes with its own set of strengths and weaknesses.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1.  **No Need to Pre-specify the Number of Clusters (K):**\n",
    "    * This is a major advantage. The algorithm produces a full hierarchy of clusters (the dendrogram). You can then choose the number of clusters that seems most appropriate by examining the dendrogram and cutting it at a desired level or distance threshold. This allows for more exploration of potential groupings.\n",
    "2.  **Provides a Rich Visualization (Dendrogram):**\n",
    "    * The dendrogram offers an intuitive and informative visualization of how clusters are formed and related to each other at different levels of similarity. It can reveal the underlying structure and hierarchy in the data.\n",
    "3.  **Can Discover Clusters of Arbitrary Shapes (Depending on Linkage):**\n",
    "    * Unlike K-Means, which assumes spherical clusters, some linkage methods in hierarchical clustering (especially single linkage) can identify non-globular, elongated, or even more complex cluster shapes.\n",
    "4.  **Deterministic (for a given linkage and distance metric):**\n",
    "    * Given the same distance metric and linkage criterion, agglomerative hierarchical clustering will always produce the same hierarchy of clusters. K-Means, on the other hand, can produce different results on different runs if the initial centroid placement (for `init='random'`) varies (though K-Means++ and `n_init` in Scikit-learn mitigate this).\n",
    "5.  **Useful for Understanding Data Structure:**\n",
    "    * The hierarchical nature can be very insightful for understanding relationships between data points and groups, especially in fields like biology (e.g., phylogenetic trees) or social network analysis.\n",
    "\n",
    "---\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1.  **Computationally Intensive (Especially for Large Datasets):**\n",
    "    * Agglomerative hierarchical clustering typically has a time complexity of at least $O(N^2 \\log N)$ or $O(N^2)$ for simpler linkage methods, and can go up to $O(N^3)$ for others, where $N$ is the number of data points. This is because it needs to compute and update a proximity matrix (or search for closest pairs). This makes it slow and memory-intensive for large datasets.\n",
    "    * Divisive clustering is generally even more complex.\n",
    "2.  **Decisions are Irreversible (Greedy Nature):**\n",
    "    * Once a merge (in agglomerative) or a split (in divisive) is made, it cannot be undone in subsequent steps. This greedy nature means that an early suboptimal decision can lead to a less than ideal overall clustering structure. It doesn't globally optimize the cluster assignments.\n",
    "3.  **Sensitivity to Linkage Criterion and Distance Metric:**\n",
    "    * The choice of linkage criterion (single, complete, average, Ward's, etc.) and the distance metric (Euclidean, Manhattan, etc.) can significantly affect the resulting cluster hierarchy and the shapes of the clusters identified. There's often no single \"best\" choice, and it might require experimentation or domain knowledge.\n",
    "4.  **Difficulty Handling Outliers:**\n",
    "    * Some linkage methods (like single linkage) can be very sensitive to outliers, as a single outlier can bridge two otherwise distinct clusters. Ward's linkage, being variance-based, can also be affected by outliers.\n",
    "5.  **Dendrogram Interpretation Can Be Subjective:**\n",
    "    * While dendrograms are informative, deciding where to \"cut\" the dendrogram to obtain the final set of clusters can sometimes be subjective, especially if there isn't a very clear and distinct separation (long vertical line) in the dendrogram.\n",
    "6.  **Scalability Issues:**\n",
    "    * Due to its computational complexity and memory requirements (storing the proximity matrix), standard hierarchical clustering doesn't scale well to very large datasets with tens of thousands or hundreds of thousands of points. (Specialized algorithms and approximations exist for larger datasets).\n",
    "7.  **No Direct Cluster Centroids (like K-Means):**\n",
    "    * While you can calculate centroids for the resulting clusters after cutting the dendrogram, the algorithm itself doesn't explicitly work with or optimize centroid positions in the same way K-Means does.\n",
    "\n",
    "Despite its computational cost for large datasets, hierarchical clustering remains a valuable tool for exploratory data analysis, for understanding nested relationships, and when the number of clusters is not known a priori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b617c-389e-4d53-bb7d-07112a4ac920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
