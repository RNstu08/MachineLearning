{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b280eb-55f4-4dd3-a0ad-946a553740bf",
   "metadata": {},
   "source": [
    "**A. Based on the Confusion Matrix (Threshold-Dependent)**\n",
    "\n",
    "These metrics are typically calculated after a classification threshold has been applied (e.g., if predicted probability \\> 0.5, classify as \"Positive\"). They all derive from the counts within the Confusion Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef457f-08c3-4d13-a1fa-982ef33eb963",
   "metadata": {},
   "source": [
    "**5. Confusion Matrix**\n",
    "\n",
    "  * **Concept:** The Confusion Matrix is a table that summarizes the performance of a classification algorithm by comparing its predictions against the actual true labels. It's the foundation for most other threshold-based classification metrics. For a binary classification problem (predicting Positive vs. Negative), it looks like this:\n",
    "\n",
    "    |                 | Predicted: Negative | Predicted: Positive |\n",
    "    | :-------------- | :------------------ | :------------------ |\n",
    "    | **Actual: Negative** | True Negative (TN)  | False Positive (FP) |\n",
    "    | **Actual: Positive** | False Negative (FN) | True Positive (TP)  |\n",
    "\n",
    "      * **True Positive (TP):** Actual is Positive, Predicted is Positive (Correct Hit)\n",
    "      * **True Negative (TN):** Actual is Negative, Predicted is Negative (Correct Rejection)\n",
    "      * **False Positive (FP):** Actual is Negative, Predicted is Positive (Type I Error, \"False Alarm\")\n",
    "      * **False Negative (FN):** Actual is Positive, Predicted is Negative (Type II Error, \"Miss\")\n",
    "\n",
    "  * **Formula:** Not a formula, but the structure: `[[TN, FP], [FN, TP]]` (as returned by scikit-learn). Note the order.\n",
    "\n",
    "  * **Interpretation:** It shows *where* the model is making errors. Are you incorrectly flagging negatives as positives (high FP)? Or are you failing to find the positives (high FN)?\n",
    "\n",
    "  * **Pros:**\n",
    "\n",
    "      * Provides a detailed breakdown of correct and incorrect predictions.\n",
    "      * Fundamental for calculating many other metrics.\n",
    "      * Helps understand the *types* of errors the model makes.\n",
    "\n",
    "  * **Cons:**\n",
    "\n",
    "      * Not a single summary score, requires interpretation.\n",
    "      * Can be less intuitive at first glance than a single metric like accuracy.\n",
    "\n",
    "  * **Example:**\n",
    "    Suppose we have 10 loan applications. True status: `[Fraud, Not Fraud, Fraud, Not Fraud, Not Fraud, Fraud, Not Fraud, Not Fraud, Fraud, Not Fraud]` (4 Fraud, 6 Not Fraud). Let's say Fraud is 'Positive' (1) and Not Fraud is 'Negative' (0).\n",
    "    `y_true = [1, 0, 1, 0, 0, 1, 0, 0, 1, 0]`\n",
    "    Model predictions: `[1, 0, 0, 0, 1, 1, 0, 0, 1, 0]`\n",
    "    `y_pred = [1, 0, 0, 0, 1, 1, 0, 0, 1, 0]`\n",
    "\n",
    "    Let's tally:\n",
    "\n",
    "      * TP: Actual=1, Predicted=1 (Instances 1, 6, 9) -\\> **TP = 3**\n",
    "      * TN: Actual=0, Predicted=0 (Instances 2, 4, 7, 8, 10) -\\> **TN = 5**\n",
    "      * FP: Actual=0, Predicted=1 (Instance 5) -\\> **FP = 1**\n",
    "      * FN: Actual=1, Predicted=0 (Instance 3) -\\> **FN = 1**\n",
    "        The Confusion Matrix: `[[TN, FP], [FN, TP]] = [[5, 1], [1, 3]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3b0c986-743f-4534-b71a-6d8341d00658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[5 1]\n",
      " [1 3]]\n",
      "TN: 5, FP: 1, FN: 1, TP: 3\n"
     ]
    }
   ],
   "source": [
    "# Implementation (Scikit-learn):**\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# To get individual values (optional)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    "\n",
    "# Context:** Always the first step in evaluating a classifier's predictions. Essential when the cost of FP errors differs significantly from the cost of FN errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5bada6-c17b-4673-9459-21855fbbb576",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7380c-fa20-40b3-9a51-ff555a4c3051",
   "metadata": {},
   "source": [
    "**6. Accuracy**\n",
    "\n",
    "* **Concept:** The most straightforward metric – what proportion of total predictions did the model get right?\n",
    "\n",
    "* **Formula:**\n",
    "  $Accuracy = \\frac{Number \\ of \\ Correct \\ Predictions}{Total \\ Number \\ of \\ Predictions} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "* **Interpretation:** A value between 0 and 1 (or 0% to 100%), where 1 means perfect prediction for all instances.\n",
    "\n",
    "* **Pros:**\n",
    "    * Simple and highly intuitive to understand and explain.\n",
    "\n",
    "* **Cons:**\n",
    "    * **Highly misleading for imbalanced datasets.** If 99% of cases are `'Negative'`, a model that always predicts `'Negative'` gets 99% accuracy but is useless for finding `'Positive'` cases.\n",
    "    * Treats all errors equally, which is often not the case in reality.\n",
    "\n",
    "* **Example:**\n",
    "  Using our previous example: `TP`=3, `TN`=5, `FP`=1, `FN`=1. Total = 10.\n",
    "  $Accuracy = \\frac{3 + 5}{3 + 5 + 1 + 1} = \\frac{8}{10} = 0.8$ (or 80%)\n",
    "\n",
    "  *Imbalanced Example:* Suppose 100 patients, 99 healthy (Negative=0), 1 has disease (Positive=1).\n",
    "  `y_true = [0]*99 + [1]`\n",
    "  A lazy model predicts everyone is healthy: `y_pred = [0]*100`\n",
    "  `TP`=0, `TN`=99, `FP`=0, `FN`=1.\n",
    "  $Accuracy = \\frac{0 + 99}{0 + 99 + 0 + 1} = \\frac{99}{100} = 0.99$ (99% accuracy!)\n",
    "  But this model completely failed to find the single positive case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5140c441-7250-4f2e-9eb0-565c4b779115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Imbalanced Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Implementation (Scikit-learn):\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using the first example\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {acc:.2f}\") # Output: Accuracy: 0.80\n",
    "\n",
    "# Using the imbalanced example\n",
    "y_true_imb = np.array([0]*99 + [1])\n",
    "y_pred_imb = np.array([0]*100)\n",
    "acc_imb = accuracy_score(y_true_imb, y_pred_imb)\n",
    "print(f\"Imbalanced Accuracy: {acc_imb:.2f}\") # Output: Imbalanced Accuracy: 0.99\n",
    "\n",
    "#Context: Use with caution. Best suited for balanced datasets where all types of errors have similar costs. Often reported, but rarely the *only* metric you should rely on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aaecae-a423-415b-90f2-7b3d39c0ebfd",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb5842-0072-455c-a0a9-83d590f367b1",
   "metadata": {},
   "source": [
    "**7. Precision (Positive Predictive Value)**\n",
    "\n",
    "  * **Concept:** Answers the question: \"Of all the instances the model predicted as Positive, what proportion actually were Positive?\" Measures the reliability of positive predictions.\n",
    "\n",
    "  * **Formula:**\n",
    "    $Precision = \\\\frac{TP}{TP + FP} = \\\\frac{True \\\\ Positives}{Total \\\\ Predicted \\\\ Positives}$\n",
    "\n",
    "  * **Interpretation:** A value between 0 and 1. High precision (near 1) means that when the model predicts Positive, it is very likely to be correct. Low precision means many False Positives.\n",
    "\n",
    "  * **Pros:**\n",
    "\n",
    "      * Focuses on the cost/consequence of False Positives.\n",
    "      * Useful when you want to be sure about your positive predictions.\n",
    "\n",
    "  * **Cons:**\n",
    "\n",
    "      * Completely ignores False Negatives (doesn't tell you if you missed many positives).\n",
    "      * Can be trivially 1.0 if the model predicts only one instance as positive and gets it right (but misses all others).\n",
    "\n",
    "  * **Example:**\n",
    "    Using our loan example: TP=3, FP=1.\n",
    "    $Precision = \\\\frac{3}{3 + 1} = \\\\frac{3}{4} = 0.75$\n",
    "    This means 75% of the applications the model flagged as 'Fraud' were actually fraudulent.\n",
    "\n",
    "    *Scenario where Precision is key:* Spam detection. A False Positive means an important email lands in the spam folder (high cost). You want high precision – ensuring that emails flagged as spam really *are* spam. You might tolerate some spam getting through (False Negatives) to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ce795e-df94-4a6b-9541-09b2e8ade8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Implementation (Scikit-learn):\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
    "\n",
    "# By default, calculates precision for the class labeled '1'\n",
    "prec = precision_score(y_true, y_pred)\n",
    "print(f\"Precision: {prec:.2f}\") # Output: Precision: 0.75\n",
    "\n",
    "# Can specify the positive label if it's not 1\n",
    "# prec_for_0 = precision_score(y_true, y_pred, pos_label=0)\n",
    "# print(f\"Precision for class 0: {prec_for_0:.2f}\") # Output: Precision for class 0: 0.83 (5 TN / (5 TN + 1 FN))\n",
    "# Context: Crucial when the cost of a False Positive is high (e.g., classifying a non-spam email as spam, recommending a product the user hates, accusing someone innocent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65b550d-71e1-42b5-a667-fbaa3ad715a3",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a015cb-a51e-40c1-98cd-6a9adb2dfbe2",
   "metadata": {},
   "source": [
    "**8. Recall (Sensitivity, True Positive Rate - TPR)**\n",
    "\n",
    "* **Concept:** Answers the question: \"Of all the actual `Positive` instances, what proportion did the model correctly identify?\" Measures the model's ability to \"find\" or \"catch\" all positive samples.\n",
    "\n",
    "* **Formula:**\n",
    "  $Recall = \\frac{TP}{TP + FN} = \\frac{True \\ Positives}{Total \\ Actual \\ Positives}$\n",
    "\n",
    "* **Interpretation:** A value between 0 and 1. High recall (near 1) means the model identifies most of the actual positive instances. Low recall means the model misses many positive instances (many `False Negatives`).\n",
    "\n",
    "* **Pros:**\n",
    "    * Focuses on the cost/consequence of `False Negatives`.\n",
    "    * Useful when finding positive instances is paramount.\n",
    "\n",
    "* **Cons:**\n",
    "    * Completely ignores `False Positives` (doesn't tell you if many of your positive predictions were wrong).\n",
    "    * Can be trivially 1.0 if the model predicts *everything* as positive (but `precision` would likely be low).\n",
    "\n",
    "* **Example:**\n",
    "  Using our loan example: `TP`=3, `FN`=1. Total Actual Positives = `TP` + `FN` = 4.\n",
    "  $Recall = \\frac{3}{3 + 1} = \\frac{3}{4} = 0.75$\n",
    "  This means the model found 75% of all the truly fraudulent applications.\n",
    "\n",
    "  *Scenario where Recall is key:* Detecting a critical, contagious disease. A `False Negative` means a sick person goes undiagnosed (very high cost). You want high recall – catching as many actual cases as possible, even if it means some healthy people are initially flagged for more tests (`False Positives`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "980694e6-3649-41d1-ab06-89429c87d8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Implementation (Scikit-learn):**\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
    "\n",
    "# By default, calculates recall for the class labeled '1'\n",
    "rec = recall_score(y_true, y_pred)\n",
    "print(f\"Recall: {rec:.2f}\") # Output: Recall: 0.75\n",
    "\n",
    "# Can specify the positive label if it's not 1\n",
    "# rec_for_0 = recall_score(y_true, y_pred, pos_label=0) # This is Specificity!\n",
    "# print(f\"Recall for class 0 (Specificity): {rec_for_0:.2f}\") # Output: Recall for class 0 (Specificity): 0.83 (5 TN / (5 TN + 1 FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc2f54-76ca-4982-9dda-b4215e0e8ccf",
   "metadata": {},
   "source": [
    "* **Context:** Crucial when the cost of a False Negative is high (e.g., missing a fraudulent transaction, failing to detect a serious illness, airport security missing a threat).\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f678f1-e67e-4f6a-aabc-f4e9e70b2e21",
   "metadata": {},
   "source": [
    "\n",
    "**9. Specificity (True Negative Rate - TNR)**\n",
    "\n",
    "  * **Concept:** Answers the question: \"Of all the actual Negative instances, what proportion did the model correctly identify?\" It's essentially the Recall for the negative class.\n",
    "\n",
    "  * **Formula:**\n",
    "    $Specificity = \\frac{TN}{TN + FP} = \\frac{True \\\\ Negatives}{Total \\ Actual \\ Negatives}$\n",
    "\n",
    "  * **Interpretation:** A value between 0 and 1. High specificity (near 1) means the model correctly identifies most of the actual negative instances, leading to few False Positives.\n",
    "\n",
    "* **Pros:**\n",
    "    * Focuses on correctly identifying negative instances.\n",
    "    * Directly related to the `False Positive Rate` (`Specificity` = 1 - `FPR`).\n",
    "\n",
    "* **Cons:**\n",
    "    * Ignores performance on the positive class (`TP`, `FN`).\n",
    "\n",
    "  * **Example:**\n",
    "    Using our loan example: `TN=5`, `FP=1`. Total Actual Negatives = `TN` + `FP` = `6`.\n",
    "    $Specificity = \\frac{5}{5 + 1} = \\frac{5}{6} \\approx 0.833$\n",
    "    This means the model correctly identified `83.3%` of the non-fraudulent applications.\n",
    "\n",
    "    *Scenario where Specificity is key:* A medical test to confirm a patient is *healthy* after treatment. You want high specificity to confidently tell recovered patients they are clear, minimizing false alarms (FP) that would cause undue stress and further testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9d4ce15-afab-405c-a5a6-577e2df2ced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity (from CM): 0.833\n",
      "Specificity (using recall_score pos_label=0): 0.833\n"
     ]
    }
   ],
   "source": [
    "# Implementation (Scikit-learn):\n",
    "# There's no direct `specificity_score` function.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score\n",
    "\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
    "\n",
    "# Method 1: Calculate from confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Specificity (from CM): {specificity:.3f}\") # Output: Specificity (from CM): 0.833\n",
    "\n",
    "# Method 2: Use recall_score on the negative class (assuming 0 is negative)\n",
    "# Ensure y_true contains 0s and 1s\n",
    "specificity_alt = recall_score(y_true, y_pred, pos_label=0)\n",
    "print(f\"Specificity (using recall_score pos_label=0): {specificity_alt:.3f}\") # Output: Specificity (using recall_score pos_label=0): 0.833"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b83428-c23d-43cf-a8f3-43745888769f",
   "metadata": {},
   "source": [
    "* **Context:** Important when correctly identifying negatives is crucial and False Positives are costly or undesirable. Often considered alongside Sensitivity (Recall) in diagnostic testing.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395004e-6aaa-4e8d-852e-d59e76b8bbba",
   "metadata": {},
   "source": [
    "**10. F1-Score**\n",
    "\n",
    "  * **Concept:** The harmonic mean of Precision and Recall. It tries to find a balance between the two.\n",
    "  * **Formula:**\n",
    "    $F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} = \\frac{2 \\times TP}{2 \\times TP + FP + FN}$\n",
    "  * **Interpretation:** A value between 0 and 1. It's high only if *both* Precision and Recall are high. It penalizes extreme values more than a simple arithmetic mean. If Precision or Recall is zero, F1 is zero.\n",
    "  * **Pros:**\n",
    "      * Provides a single score that balances Precision and Recall.\n",
    "      * Useful when both False Positives and False Negatives are important.\n",
    "      * Often better than Accuracy on imbalanced datasets.\n",
    "  * **Cons:**\n",
    "      * Less interpretable than Precision or Recall individually – the meaning of a 0.7 F1 score is less direct.\n",
    "      * Assumes equal importance for Precision and Recall. (Note: The generalized F-beta score allows weighting).\n",
    "  * **Example:**\n",
    "    Using our loan example: Precision = 0.75, Recall = 0.75.\n",
    "    $F1 = 2 \\times \\frac{0.75 \\times 0.75}{0.75 + 0.75} = 2 \\times \\frac{0.5625}{1.5} = 2 \\times 0.375 = 0.75$\n",
    "    (In this case, since P=R, F1 is also the same. If P=0.6, R=0.8, then $F1 = 2 \\times (0.6 \\times 0.8) / (0.6 + 0.8) = 2 \\times 0.48 / 1.4 \\approx 0.686$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50885ffc-919d-4fac-9e23-dd7cc0acc464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.75\n"
     ]
    }
   ],
   "source": [
    "#Implementation (Scikit-learn):\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
    "\n",
    "# By default, calculates F1 for the class labeled '1'\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"F1 Score: {f1:.2f}\") # Output: F1 Score: 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046cc28-a447-4eee-884d-a8e703b541d1",
   "metadata": {},
   "source": [
    "* **Context:** Very commonly used metric for classification tasks, especially when dealing with imbalanced classes or when you need a single metric that considers both types of errors.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68127d-c799-4d7c-990d-a677a6495670",
   "metadata": {},
   "source": [
    "**11. False Positive Rate (FPR)**\n",
    "\n",
    "  * **Concept:** Answers the question: \"Of all the actual Negative instances, what proportion did the model incorrectly predict as Positive?\" How often does the model generate a false alarm?\n",
    "  * **Formula:**\n",
    "    $FPR = \\frac{FP}{TN + FP} = \\frac{False \\ Positives}{Total \\ Actual \\ Negatives}$\n",
    "    Note: $FPR = 1 - Specificity$\n",
    "  * **Interpretation:** A value between 0 and 1. Lower is better. FPR = 0 means no negative instances were misclassified as positive. FPR = 1 means all negative instances were misclassified as positive.\n",
    "  * **Pros:**\n",
    "      * Directly measures the rate of false alarms among negative instances.\n",
    "      * Crucial component (x-axis) of the ROC curve.\n",
    "  * **Cons:**\n",
    "      * Doesn't provide information about how well positive instances are identified.\n",
    "  * **Example:**\n",
    "    Using our loan example: FP=1, TN=5. Total Actual Negatives = 6.\n",
    "    $FPR = \\frac{1}{5 + 1} = \\frac{1}{6} \\approx 0.167$\n",
    "    This means about 16.7% of the non-fraudulent applications were incorrectly flagged as fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e00e032-9290-49ea-a0bd-0dd6afdca75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate (FPR): 0.167\n"
     ]
    }
   ],
   "source": [
    "#Implementation (Scikit-learn):** Usually calculated from the confusion matrix.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fpr = fp / (tn + fp)\n",
    "print(f\"False Positive Rate (FPR): {fpr:.3f}\") # Output: False Positive Rate (FPR): 0.167"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5de854-5fdc-4627-9f07-ff9fb1e89630",
   "metadata": {},
   "source": [
    "* **Context:** Essential for understanding the trade-off in ROC analysis (TPR vs. FPR). Important in domains where false alarms have significant costs (e.g., triggering expensive security protocols, unnecessary medical procedures).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c96dc5-69d2-4dc8-ac37-11f5c423a138",
   "metadata": {},
   "source": [
    "**12. False Negative Rate (FNR)**\n",
    "\n",
    "  * **Concept:** Answers the question: \"Of all the actual Positive instances, what proportion did the model incorrectly predict as Negative?\" How often does the model miss a positive case?\n",
    "  * **Formula:**\n",
    "    $FNR = \\frac{FN}{TP + FN} = \\frac{False \\ Negatives}{Total \\ Actual \\ Positives}$\n",
    "    Note: $FNR = 1 - Recall$ (or $1 - TPR$)\n",
    "  * **Interpretation:** A value between 0 and 1. Lower is better. FNR = 0 means no positive instances were missed. FNR = 1 means all positive instances were missed.\n",
    "  * **Pros:**\n",
    "      * Directly measures the rate of missed positive instances.\n",
    "  * **Cons:**\n",
    "      * Doesn't provide information about how well negative instances are identified (ignores FP).\n",
    "  * **Example:**\n",
    "    Using our loan example: FN=1, TP=3. Total Actual Positives = 4.\n",
    "    $FNR = \\frac{1}{3 + 1} = \\frac{1}{4} = 0.25$\n",
    "    This means the model missed 25% of the truly fraudulent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11d4b90a-a60a-4a6b-86c7-fbcdee6f7e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Rate (FNR): 0.25\n"
     ]
    }
   ],
   "source": [
    "#Implementation (Scikit-learn):** Usually calculated from the confusion matrix.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fnr = fn / (tp + fn)\n",
    "print(f\"False Negative Rate (FNR): {fnr:.2f}\") # Output: False Negative Rate (FNR): 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f5135-5552-4aa6-a374-3ab8582c715f",
   "metadata": {},
   "source": [
    "* **Context:** Important when missing positive instances is critical (see Recall context). Directly quantifies the proportion of misses.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91407b1b-66ec-4506-ac8d-b8fe894ee89e",
   "metadata": {},
   "source": [
    "**13. Prevalence**\n",
    "\n",
    "  * **Concept:** The proportion of actual positive instances in the dataset. It's a property of the *data*, not the model.\n",
    "  * **Formula:**\n",
    "    $Prevalence = \\frac{Total \\ Actual \\ Positives}{Total \\ Population} = \\frac{TP + FN}{TP + TN + FP + FN}$\n",
    "  * **Interpretation:** A value between 0 and 1. Indicates how common the positive class is in the sample. Prevalence = 0.01 means 1% of the instances are positive.\n",
    "  * **Pros:**\n",
    "      * Provides crucial context for interpreting other metrics, especially on imbalanced datasets.\n",
    "      * Helps establish a baseline (e.g., random guessing performance, no-skill classifier performance on PR curve).\n",
    "  * **Cons:**\n",
    "      * Not a measure of model performance itself.\n",
    "  * **Example:**\n",
    "    Using our loan example: TP=3, FN=1. Total = 10.\n",
    "    $Prevalence = \\frac{3 + 1}{10} = \\frac{4}{10} = 0.4$\n",
    "    This means 40% of the applications in our sample were actually fraudulent.\n",
    "    In the imbalanced medical example (1 positive, 99 negative):\n",
    "    $Prevalence = \\frac{0 + 1}{100} = 0.01$\n",
    "* **Implementation:** Calculated directly from the true labels or confusion matrix counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdfc3e5a-9206-4444-b359-008b506fe87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalence (direct): 0.40\n",
      "Prevalence (from CM): 0.40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = np.array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # Not needed for prevalence\n",
    "\n",
    "# Method 1: From y_true directly\n",
    "prevalence_direct = np.mean(y_true) # Calculates sum(y_true)/len(y_true)\n",
    "print(f\"Prevalence (direct): {prevalence_direct:.2f}\") # Output: Prevalence (direct): 0.40\n",
    "\n",
    "# Method 2: From CM (if available)\n",
    "cm = confusion_matrix(y_true, y_pred) # Need predictions here just to get cm\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "prevalence_cm = (tp + fn) / (tp + tn + fp + fn)\n",
    "print(f\"Prevalence (from CM): {prevalence_cm:.2f}\") # Output: Prevalence (from CM): 0.40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25319711-aca5-4255-af53-9f18bf45898c",
   "metadata": {},
   "source": [
    "* **Context:** Essential background information. A very low or high prevalence signals class imbalance, which heavily influences the interpretation and choice of other metrics (Accuracy becomes unreliable, PR curves become more informative than ROC curves for low prevalence).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14112245-6045-4e77-8809-f945c8838f68",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2312172137.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m**14. Matthews Correlation Coefficient (MCC)**\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "**14. Matthews Correlation Coefficient (MCC)**\n",
    "\n",
    "  * **Concept:** Measures the correlation between the true classes and the predicted classes. It takes into account all four entries of the confusion matrix (TP, TN, FP, FN).\n",
    "\n",
    "  * **Formula:**\n",
    "    $MCC = \\\\frac{(TP \\\\times TN) - (FP \\\\times FN)}{\\\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$\n",
    "    (The denominator involves the product of the sizes of all four marginal totals of the confusion matrix).\n",
    "\n",
    "  * **Interpretation:** Ranges from -1 to +1.\n",
    "\n",
    "      * \\+1: Perfect prediction.\n",
    "      * 0: Random prediction (no better than chance).\n",
    "      * \\-1: Perfect inverse prediction (predicts everything exactly wrong).\n",
    "        Considered one of the most balanced single-summary scores, especially for imbalanced datasets.\n",
    "\n",
    "  * **Pros:**\n",
    "\n",
    "      * Generally regarded as a balanced measure, even if the classes are of very different sizes.\n",
    "      * Returns a high score only if the classifier did well on both the positive and negative classes.\n",
    "      * Symmetric: Swapping positive/negative classes gives the same score.\n",
    "\n",
    "  * **Cons:**\n",
    "\n",
    "      * Less intuitive or directly interpretable than Accuracy, Precision, or Recall.\n",
    "      * The formula is more complex.\n",
    "      * Undefined if any of the sums in the denominator are zero (e.g., if the model predicts only one class). Scikit-learn returns 0 in this case.\n",
    "\n",
    "  * **Example:**\n",
    "    Using our loan example: TP=3, TN=5, FP=1, FN=1.\n",
    "    Numerator: $(3 \\\\times 5) - (1 \\\\times 1) = 15 - 1 = 14$\n",
    "    Denominator terms:\n",
    "    (TP + FP) = 3 + 1 = 4 (Total Predicted Positive)\n",
    "    (TP + FN) = 3 + 1 = 4 (Total Actual Positive)\n",
    "    (TN + FP) = 5 + 1 = 6 (Total Actual Negative)\n",
    "    (TN + FN) = 5 + 1 = 6 (Total Predicted Negative)\n",
    "    Denominator: $\\\\sqrt{4 \\\\times 4 \\\\times 6 \\\\times 6} = \\\\sqrt{16 \\\\times 36} = \\\\sqrt{576} = 24$\n",
    "    $MCC = \\\\frac{14}{24} \\\\approx 0.583$\n",
    "\n",
    "    *Imbalanced Example (Lazy Model):* TP=0, TN=99, FP=0, FN=1.\n",
    "    Numerator: $(0 \\\\times 99) - (0 \\\\times 1) = 0$\n",
    "    Denominator terms: (0+0=0), (0+1=1), (99+0=99), (99+1=100)\n",
    "    Since (TP+FP)=0, the denominator is 0. MCC is typically reported as 0 in this edge case. This correctly reflects the model has no correlation with the true outcome, unlike the misleading 99% accuracy.\n",
    "\n",
    "  * **Implementation (Scikit-learn):**\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "    y_true = np.array([1, 0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "    y_pred = np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    print(f\"Matthews Correlation Coefficient (MCC): {mcc:.3f}\") # Output: Matthews Correlation Coefficient (MCC): 0.583\n",
    "\n",
    "    # Imbalanced example\n",
    "    y_true_imb = np.array([0]*99 + [1])\n",
    "    y_pred_imb = np.array([0]*100)\n",
    "    mcc_imb = matthews_corrcoef(y_true_imb, y_pred_imb)\n",
    "    print(f\"Imbalanced MCC: {mcc_imb:.3f}\") # Output: Imbalanced MCC: 0.000\n",
    "    ```\n",
    "\n",
    "  * **Context:** A very good choice for a single performance score, particularly recommended when dealing with imbalanced datasets, as it requires the model to perform well on both majority and minority classes to achieve a high score.\n",
    "\n",
    "-----\n",
    "\n",
    "**B. Threshold-Independent Metrics & Curves**\n",
    "\n",
    "These metrics evaluate classifier performance *without* relying on a single, fixed decision threshold (like 0.5). They are particularly useful for models that output probabilities or confidence scores, as they assess performance across the *entire range* of possible thresholds.\n",
    "\n",
    "-----\n",
    "\n",
    "**15. Receiver Operating Characteristic (ROC) Curve**\n",
    "\n",
    "  * **Concept:** A graphical plot illustrating the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It plots the True Positive Rate (Recall) against the False Positive Rate (FPR) at various threshold settings.\n",
    "  * **Formula:** Not a single formula, but a curve generated by plotting points `(FPR, TPR)` for every possible threshold:\n",
    "      * X-axis: False Positive Rate (FPR) = `FP / (TN + FP)` (Ranges 0 to 1)\n",
    "      * Y-axis: True Positive Rate (TPR) = Recall = `TP / (TP + FN)` (Ranges 0 to 1)\n",
    "  * **Interpretation:**\n",
    "      * **Top-Left Corner (0, 1):** Represents a perfect classifier (TPR=1, FPR=0).\n",
    "      * **Diagonal Line (y=x):** Represents random guessing (TPR = FPR). A model whose curve falls below this line is worse than random.\n",
    "      * **Shape:** The closer the curve follows the top-left border, the better the classifier's ability to discriminate between positive and negative classes across thresholds. A curve bowed towards the top-left indicates good performance.\n",
    "  * **Pros:**\n",
    "      * Visualizes the trade-off between benefit (finding positives, TPR) and cost (false alarms, FPR).\n",
    "      * Threshold-independent: Shows performance across all possible operating points.\n",
    "      * Useful for comparing different models' overall discriminative power (curves closer to top-left are better).\n",
    "  * **Cons:**\n",
    "      * Can be misleadingly optimistic on highly imbalanced datasets, especially if the focus is on the minority positive class. A large number of True Negatives can keep the FPR low even if the absolute number of False Positives is high relative to True Positives.\n",
    "      * Doesn't show Precision or the impact of threshold changes on it.\n",
    "      * Doesn't reflect the calibration of the probabilities (how well the predicted probabilities match the true likelihoods).\n",
    "  * **Example:**\n",
    "    Imagine `y_true = [0, 0, 1, 1]` and a model outputs scores `y_scores = [0.1, 0.4, 0.35, 0.8]`.\n",
    "    We test different thresholds:\n",
    "      * Threshold \\> 0.8: Predict all 0 -\\> `[0,0,0,0]`. TP=0, FP=0, TN=2, FN=2. TPR=0/2=0, FPR=0/2=0. Point (0, 0).\n",
    "      * Threshold \\> 0.4: Predict `[0,0,0,1]`. TP=1, FP=0, TN=2, FN=1. TPR=1/2=0.5, FPR=0/2=0. Point (0, 0.5).\n",
    "      * Threshold \\> 0.35: Predict `[0,1,0,1]`. TP=1, FP=1, TN=1, FN=1. TPR=1/2=0.5, FPR=1/2=0.5. Point (0.5, 0.5).\n",
    "      * Threshold \\> 0.1: Predict `[0,1,1,1]`. TP=2, FP=1, TN=1, FN=0. TPR=2/2=1, FPR=1/2=0.5. Point (0.5, 1).\n",
    "      * Threshold \\<= 0.1: Predict all 1 -\\> `[1,1,1,1]`. TP=2, FP=2, TN=0, FN=0. TPR=2/2=1, FPR=2/2=1. Point (1, 1).\n",
    "        Plotting these `(FPR, TPR)` points and connecting them forms the ROC curve.\n",
    "  * **Implementation (Scikit-learn):** Requires true labels and the predicted scores/probabilities.\n",
    "    ```python\n",
    "    from sklearn.metrics import roc_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    y_true = np.array([0, 0, 1, 1])\n",
    "    y_scores = np.array([0.1, 0.4, 0.35, 0.8]) # Probabilities or decision scores\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--') # Random guessing line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.grid(True)\n",
    "    # plt.show() # Uncomment to display plot\n",
    "\n",
    "    print(\"FPR:\", fpr)\n",
    "    print(\"TPR:\", tpr)\n",
    "    print(\"Thresholds:\", thresholds) # Note: Thresholds are decreasing\n",
    "    # Output (values may slightly differ based on implementation details):\n",
    "    # FPR: [0.  0.  0.5 0.5 1. ]\n",
    "    # TPR: [0.  0.5 0.5 1.  1. ]\n",
    "    # Thresholds: [1.8  0.8  0.4  0.35 0.1 ]\n",
    "    ```\n",
    "  * **Context:** A standard tool for evaluating binary classifiers, widely used in machine learning and medical diagnostics. Excellent for understanding the TPR/FPR trade-off and comparing the *potential* performance of models independent of a specific threshold choice.\n",
    "\n",
    "-----\n",
    "\n",
    "**16. Area Under the ROC Curve (ROC AUC or AUC)**\n",
    "\n",
    "  * **Concept:** A single scalar value that measures the total area underneath the ROC curve.\n",
    "  * **Formula:** The integral of the ROC curve ($AUC = \\\\int\\_{0}^{1} TPR(FPR) dFPR$).\n",
    "  * **Interpretation:**\n",
    "      * Represents the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance.\n",
    "      * Ranges from 0 to 1.\n",
    "      * AUC = 1: Perfect classifier.\n",
    "      * AUC = 0.5: Classifier performs no better than random guessing (corresponds to the diagonal line).\n",
    "      * AUC \\< 0.5: Classifier performs worse than random guessing (predictions might be inverted).\n",
    "  * **Pros:**\n",
    "      * Provides a single, aggregate measure of performance across all classification thresholds.\n",
    "      * Threshold-independent: Doesn't require choosing a specific operating point.\n",
    "      * Classification-threshold-invariant: Not dependent on the chosen threshold to convert scores to binary predictions.\n",
    "      * Scale-invariant: Measures how well predictions are ranked, not their absolute values.\n",
    "  * **Cons:**\n",
    "      * Loses the detailed trade-off information visible in the ROC curve itself.\n",
    "      * A high AUC doesn't guarantee good performance at a *specific* practical threshold.\n",
    "      * Can be insensitive to significant performance changes in regions of the curve far from the diagonal (especially if a large part of the curve is near the top-left).\n",
    "      * Like the ROC curve, can be overly optimistic on imbalanced datasets compared to PR AUC.\n",
    "  * **Example:**\n",
    "    For the ROC curve plotted in the previous example, the area under it would be calculated. Visually, the area is clearly greater than 0.5 (the area under the diagonal) but less than 1.0 (the perfect square). Calculating it precisely involves summing the areas of the trapezoids formed by the points: Area ≈ (0-0)*(0.5-0) + (0.5-0)*(0.5-0.5) + (0.5-0.5)*(1-0.5) + (1-0.5)*(1-1) ??? No, this is wrong. Use trapezoidal rule: `0.5 * (tpr[i] + tpr[i-1]) * (fpr[i] - fpr[i-1])`.\n",
    "    Points: (0,0), (0, 0.5), (0.5, 0.5), (0.5, 1), (1, 1).\n",
    "    Area = 0.5\\*(0+0.5)*(0-0) + 0.5*(0.5+0.5)*(0.5-0) + 0.5*(0.5+1)*(0.5-0.5) + 0.5*(1+1)*(1-0.5)\n",
    "    Area = 0 + 0.5*(1)*(0.5) + 0 + 0.5*(2)\\*(0.5) = 0.25 + 0.5 = 0.75.\n",
    "  * **Implementation (Scikit-learn):** Requires true labels and predicted scores/probabilities.\n",
    "    ```python\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    y_true = np.array([0, 0, 1, 1])\n",
    "    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "\n",
    "    auc_score = roc_auc_score(y_true, y_scores)\n",
    "    print(f\"ROC AUC Score: {auc_score:.2f}\") # Output: ROC AUC Score: 0.75\n",
    "    ```\n",
    "  * **Context:** A very popular and widely reported metric for comparing the overall discriminative ability of classifiers, especially in academic papers and competitions. Useful when you care about ranking performance more than achieving a specific P/R balance at a fixed threshold.\n",
    "\n",
    "-----\n",
    "\n",
    "**17. Precision-Recall (PR) Curve**\n",
    "\n",
    "  * **Concept:** A graphical plot showing the trade-off between Precision and Recall for a classifier as its discrimination threshold is varied.\n",
    "  * **Formula:** Not a single formula, but a curve generated by plotting points `(Recall, Precision)` for every possible threshold:\n",
    "      * X-axis: Recall (TPR) = `TP / (TP + FN)` (Ranges 0 to 1)\n",
    "      * Y-axis: Precision = `TP / (TP + FP)` (Ranges 0 to 1)\n",
    "  * **Interpretation:**\n",
    "      * **Top-Right Corner (1, 1):** Represents a perfect classifier (Recall=1, Precision=1).\n",
    "      * **Baseline:** For a random classifier, the PR curve is roughly a horizontal line at the level of the positive class prevalence (`P = TP / (TP + FP)` becomes `Prevalence` if predictions are random). A good classifier's curve stays well above this baseline.\n",
    "      * **Shape:** The closer the curve follows the top-right border, the better the classifier. It explicitly shows how much precision you might lose to gain more recall (or vice-versa) by changing the threshold.\n",
    "  * **Pros:**\n",
    "      * More informative than the ROC curve for highly imbalanced datasets, especially when the minority (positive) class is the focus. This is because Precision (`TP / (TP+FP)`) is directly sensitive to the number of False Positives relative to True Positives, unlike FPR which can remain low if TN is huge.\n",
    "      * Directly visualizes the Precision/Recall trade-off, which is often critical for business decisions (e.g., how many false alarms vs. missed cases).\n",
    "  * **Cons:**\n",
    "      * The baseline performance (random guessing) depends on the dataset's prevalence, making comparisons across datasets with different prevalence less direct than with ROC curves (where the baseline is always y=x).\n",
    "      * Curves can be more jagged or less smooth than ROC curves.\n",
    "      * Less commonly used/standardized than ROC in some fields.\n",
    "  * **Example:**\n",
    "    Using `y_true = [0, 0, 1, 1]` and `y_scores = [0.1, 0.4, 0.35, 0.8]`. Prevalence = 2/4 = 0.5.\n",
    "    Let's reuse the thresholds/predictions from the ROC example:\n",
    "      * Threshold \\> 0.8: `[0,0,0,0]`. TP=0, FP=0, FN=2. Recall=0/2=0, Precision=0/0 (often undefined or 1/0 -\\> maybe plot as 1 or handle carefully). Sklearn typically starts Recall=0, Precision=1. Point (0, 1).\n",
    "      * Threshold \\> 0.4: `[0,0,0,1]`. TP=1, FP=0, FN=1. Recall=1/2=0.5, Precision=1/1=1. Point (0.5, 1).\n",
    "      * Threshold \\> 0.35: `[0,1,0,1]`. TP=1, FP=1, FN=1. Recall=1/2=0.5, Precision=1/2=0.5. Point (0.5, 0.5). Note: Recall didn't increase here, precision dropped. Sklearn handles this.\n",
    "      * Threshold \\> 0.1: `[0,1,1,1]`. TP=2, FP=1, FN=0. Recall=2/2=1, Precision=2/3≈0.67. Point (1, 0.67).\n",
    "      * Threshold \\<= 0.1: `[1,1,1,1]`. TP=2, FP=2, FN=0. Recall=2/2=1, Precision=2/4=0.5. Point (1, 0.5).\n",
    "        Plotting these `(Recall, Precision)` points forms the PR curve.\n",
    "  * **Implementation (Scikit-learn):** Requires true labels and predicted scores/probabilities.\n",
    "    ```python\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    y_true = np.array([0, 0, 1, 1])\n",
    "    y_scores = np.array([0.1, 0.4, 0.35, 0.8]) # Probabilities or decision scores\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "    plt.figure()\n",
    "    # Note: Plot precision vs recall. The arrays might include a (0,1) point for clarity.\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    # Add baseline: prevalence = sum(y_true) / len(y_true)\n",
    "    prevalence = np.mean(y_true)\n",
    "    plt.axhline(prevalence, linestyle='--', color='grey', label=f'Baseline (Prevalence={prevalence:.2f})')\n",
    "    plt.xlabel('Recall (True Positive Rate)')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall (PR) Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show() # Uncomment to display plot\n",
    "\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    # Output (values may slightly differ based on implementation details):\n",
    "    # Precision: [0.5        0.66666667 0.5        1.         1.        ]\n",
    "    # Recall:    [1.  1.  0.5 0.5 0. ]\n",
    "\n",
    "    # Scikit-learn often returns arrays sorted by threshold, plot might need adjustment\n",
    "    # Usually plotted as precision[idx] vs recall[idx]\n",
    "    # from sklearn.metrics import PrecisionRecallDisplay\n",
    "    # display = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "    # display.plot() # This handles plotting correctly\n",
    "    ```\n",
    "  * **Context:** Highly recommended for imbalanced classification tasks where the positive (minority) class performance is crucial (e.g., fraud detection, search engine result ranking, anomaly detection). Provides a more sensitive view of performance changes affecting the positive class than ROC.\n",
    "\n",
    "-----\n",
    "\n",
    "**18. Area Under the PR Curve (PR AUC) / Average Precision (AP)**\n",
    "\n",
    "  * **Concept:** A single scalar value that summarizes the Precision-Recall curve. While one could compute the geometric area (like ROC AUC), the standard metric is \"Average Precision\" (AP), which calculates a weighted mean of precisions achieved at each threshold, using the increase in recall from the previous threshold as the weight. This better reflects the shape of the PR curve.\n",
    "  * **Formula:** $AP = \\\\sum\\_{n} (Recall\\_n - Recall\\_{n-1}) \\\\times Precision\\_n$\n",
    "    Where $Recall\\_n$ and $Precision\\_n$ are the recall and precision at the nth threshold (often sorted by recall). This corresponds to a rectangular approximation of the area under the curve.\n",
    "  * **Interpretation:**\n",
    "      * Ranges from 0 to 1.\n",
    "      * A higher score indicates better performance, meaning the model maintains high precision across different recall levels.\n",
    "      * A score equal to the prevalence indicates random performance.\n",
    "      * A perfect classifier has AP = 1.\n",
    "  * **Pros:**\n",
    "      * Provides a single number summary of the PR curve.\n",
    "      * More sensitive than ROC AUC to improvements in classifying the positive class, especially when it's rare.\n",
    "      * Directly relates to the Precision-Recall trade-off.\n",
    "  * **Cons:**\n",
    "      * Less commonly reported or universally understood than ROC AUC in some domains.\n",
    "      * The baseline value depends on the class prevalence, making comparisons across datasets harder.\n",
    "      * The calculation (Average Precision) is less geometrically intuitive than the area under the ROC curve.\n",
    "  * **Example:**\n",
    "    Based on the PR curve points from the previous example. Calculating AP precisely involves using the `precision_recall_curve` output and applying the summation formula or letting the dedicated function handle it. For our points (approximate): Recall steps might be 0 -\\> 0.5 -\\> 1. Precision at these steps (roughly) 1 -\\> 0.5 -\\> 0.67.\n",
    "    AP ≈ (0.5 - 0) \\* 1.0 + (0.5 - 0.5) \\* 0.5 + (1.0 - 0.5) \\* 0.67 = 0.5 + 0 + 0.5 \\* 0.67 = 0.5 + 0.335 = 0.835. (This is illustrative; use the function for accuracy).\n",
    "  * **Implementation (Scikit-learn):** Requires true labels and predicted scores/probabilities. Use `average_precision_score`. *Do not* use `auc(recall, precision)` from `sklearn.metrics.auc` as it uses the trapezoidal rule which is not the standard AP calculation for PR curves.\n",
    "    ```python\n",
    "    from sklearn.metrics import average_precision_score\n",
    "\n",
    "    y_true = np.array([0, 0, 1, 1])\n",
    "    y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "\n",
    "    ap_score = average_precision_score(y_true, y_scores)\n",
    "    print(f\"Average Precision (PR AUC): {ap_score:.3f}\") # Output: Average Precision (PR AUC): 0.833 (matches manual calculation better here)\n",
    "    ```\n",
    "  * **Context:** The preferred summary metric when evaluating models on imbalanced datasets with a focus on positive class performance (see PR Curve context). Often used in information retrieval and object detection (mAP - mean Average Precision across classes/queries).\n",
    "\n",
    "-----\n",
    "\n",
    "**C. Metrics for Probabilistic Predictions**\n",
    "\n",
    "These metrics directly evaluate the quality of the predicted *probabilities* output by a classifier, rather than just the final assigned class labels after applying a threshold. They assess how well the predicted probabilities reflect the true likelihood of outcomes.\n",
    "\n",
    "-----\n",
    "\n",
    "**19. Log Loss (Binary Cross-Entropy)**\n",
    "\n",
    "  * **Concept:** Measures the performance of a classification model where the prediction input is a probability value between 0 and 1. It quantifies the \"uncertainty\" of the model's predictions based on how much they deviate from the true labels, heavily penalizing confident but incorrect predictions.\n",
    "  * **Formula:** For binary classification:\n",
    "    $LogLoss = - \\\\frac{1}{N} \\\\sum\\_{i=1}^{N} [ y\\_i \\\\log(p\\_i) + (1 - y\\_i) \\\\log(1 - p\\_i) ]$\n",
    "    Where:\n",
    "      * $N$ is the number of samples.\n",
    "      * $y\\_i$ is the true label for sample $i$ (0 or 1).\n",
    "      * $p\\_i$ is the predicted probability that sample $i$ belongs to class 1.\n",
    "      * $\\\\log$ is the natural logarithm (though base 2 or 10 can be used, natural is standard in ML).\n",
    "        (Note: To avoid $\\\\log(0)$, implementations typically clip probabilities slightly away from exact 0 and 1, e.g., to `[epsilon, 1-epsilon]`).\n",
    "  * **Interpretation:**\n",
    "      * Ranges from 0 to $\\\\infty$.\n",
    "      * Lower values are better. Log Loss = 0 means the predicted probabilities perfectly match the true labels (predicting 1 for all true 1s, 0 for all true 0s).\n",
    "      * It increases as predicted probabilities diverge from the actual labels. A prediction of 0.9 when the true label is 0 incurs a larger penalty than predicting 0.6. A prediction of 0.99 when the true label is 0 incurs a very large penalty.\n",
    "  * **Pros:**\n",
    "      * Evaluates the accuracy of the predicted probabilities themselves, not just the final classification.\n",
    "      * Rewards well-calibrated models (models whose probabilities reflect true likelihoods).\n",
    "      * Widely used as a loss function during the training of many probabilistic models (like Logistic Regression, Neural Networks).\n",
    "  * **Cons:**\n",
    "      * Less intuitive to interpret than accuracy-based metrics. What does a log loss of 0.3 mean in practical terms?\n",
    "      * Very sensitive to highly confident incorrect predictions (predicting close to 0 or 1 for the wrong class). A single such prediction can dominate the score.\n",
    "  * **Example:**\n",
    "    `y_true = [1, 0]`\n",
    "      * Model A predicts `p = [0.8, 0.3]`:\n",
    "        Loss = `- (1/2) * [ (1*log(0.8) + (1-1)*log(1-0.8)) + (0*log(0.3) + (1-0)*log(1-0.3)) ]`\n",
    "        Loss = `-0.5 * [ log(0.8) + log(0.7) ]`\n",
    "        Loss = `-0.5 * [ -0.223 + (-0.357) ] = -0.5 * [-0.580] = 0.290`\n",
    "      * Model B predicts `p = [0.99, 0.01]` (more confident, and correct):\n",
    "        Loss = `-0.5 * [ log(0.99) + log(0.99) ]`\n",
    "        Loss = `-0.5 * [ -0.010 + (-0.010) ] = -0.5 * [-0.020] = 0.010` (Much lower loss)\n",
    "      * Model C predicts `p = [0.1, 0.9]` (confident but WRONG):\n",
    "        Loss = `-0.5 * [ log(0.1) + log(0.1) ]`\n",
    "        Loss = `-0.5 * [ -2.303 + (-2.303) ] = -0.5 * [-4.606] = 2.303` (Much higher loss\\!)\n",
    "  * **Implementation (Scikit-learn):** Requires true labels and predicted probabilities *for each class*.\n",
    "    ```python\n",
    "    from sklearn.metrics import log_loss\n",
    "\n",
    "    y_true = np.array([1, 0])\n",
    "    # Probabilities need to be provided for each class: [prob_class_0, prob_class_1]\n",
    "    # Assumes classes are ordered 0, 1\n",
    "    y_pred_proba_A = np.array([[0.2, 0.8], [0.7, 0.3]]) # Corresponds to p=[0.8, 0.3] for class 1\n",
    "    y_pred_proba_B = np.array([[0.01, 0.99], [0.99, 0.01]])# Corresponds to p=[0.99, 0.01] for class 1\n",
    "    y_pred_proba_C = np.array([[0.9, 0.1], [0.1, 0.9]])  # Corresponds to p=[0.1, 0.9] for class 1\n",
    "\n",
    "    loss_A = log_loss(y_true, y_pred_proba_A)\n",
    "    loss_B = log_loss(y_true, y_pred_proba_B)\n",
    "    loss_C = log_loss(y_true, y_pred_proba_C)\n",
    "\n",
    "    print(f\"Log Loss (Model A): {loss_A:.3f}\") # Output: Log Loss (Model A): 0.290\n",
    "    print(f\"Log Loss (Model B): {loss_B:.3f}\") # Output: Log Loss (Model B): 0.010\n",
    "    print(f\"Log Loss (Model C): {loss_C:.3f}\") # Output: Log Loss (Model C): 2.303\n",
    "    ```\n",
    "  * **Context:** Standard metric for evaluating probabilistic classifiers, especially when the confidence level of predictions is important. Often the objective function minimized during model training.\n",
    "\n",
    "-----\n",
    "\n",
    "**20. Brier Score**\n",
    "\n",
    "  * **Concept:** Measures the accuracy of probabilistic predictions. It's defined as the mean squared error between the predicted probabilities for the positive class and the actual outcomes (coded as 0 or 1).\n",
    "  * **Formula:** For binary classification:\n",
    "    $BrierScore = \\\\frac{1}{N} \\\\sum\\_{i=1}^{N} (p\\_i - y\\_i)^2$\n",
    "    Where:\n",
    "      * $N$ is the number of samples.\n",
    "      * $p\\_i$ is the predicted probability of the positive class (class 1) for sample $i$.\n",
    "      * $y\\_i$ is the actual outcome for sample $i$ (1 if positive, 0 if negative).\n",
    "  * **Interpretation:**\n",
    "      * Ranges from 0 to 1 (or 0 to 2 if not scaled by 1/N, but sklearn uses the mean).\n",
    "      * Lower scores are better. Brier Score = 0 indicates perfect probability predictions (predicting 1 for all true 1s, 0 for all true 0s).\n",
    "      * Measures both *calibration* (how well probabilities match actual frequencies) and *resolution* (how well the probabilities separate the classes).\n",
    "  * **Pros:**\n",
    "      * Proper scoring rule: Encourages the model to report its true probabilities accurately.\n",
    "      * Intuitive interpretation as the mean squared error applied to probabilities.\n",
    "      * Can be decomposed into components related to calibration and refinement (though this is more advanced).\n",
    "  * **Cons:**\n",
    "      * Less sensitive to highly confident errors than Log Loss (squaring vs. logarithm). A prediction of 0.99 for a true 0 gives a Brier component of $(0.99-0)^2 \\\\approx 0.98$, while Log Loss goes towards infinity.\n",
    "      * Less commonly used in general machine learning than Log Loss or AUC, though standard in fields like meteorology.\n",
    "  * **Example:**\n",
    "    `y_true = [1, 0]`\n",
    "      * Model A predicts `p = [0.8, 0.3]` (probabilities for class 1):\n",
    "        Score = `(1/2) * [ (0.8 - 1)^2 + (0.3 - 0)^2 ]`\n",
    "        Score = `0.5 * [ (-0.2)^2 + (0.3)^2 ] = 0.5 * [ 0.04 + 0.09 ] = 0.5 * 0.13 = 0.065`\n",
    "      * Model B predicts `p = [0.99, 0.01]` (more confident, correct):\n",
    "        Score = `0.5 * [ (0.99 - 1)^2 + (0.01 - 0)^2 ]`\n",
    "        Score = `0.5 * [ (-0.01)^2 + (0.01)^2 ] = 0.5 * [ 0.0001 + 0.0001 ] = 0.0001` (Lower score)\n",
    "      * Model C predicts `p = [0.1, 0.9]` (confident, WRONG):\n",
    "        Score = `0.5 * [ (0.1 - 1)^2 + (0.9 - 0)^2 ]`\n",
    "        Score = `0.5 * [ (-0.9)^2 + (0.9)^2 ] = 0.5 * [ 0.81 + 0.81 ] = 0.81` (Higher score)\n",
    "  * **Implementation (Scikit-learn):** Requires true labels and predicted probabilities *for the positive class*.\n",
    "    ```python\n",
    "    from sklearn.metrics import brier_score_loss\n",
    "\n",
    "    y_true = np.array([1, 0])\n",
    "    y_pred_prob_A = np.array([0.8, 0.3]) # Probabilities for the positive class (class 1)\n",
    "    y_pred_prob_B = np.array([0.99, 0.01])\n",
    "    y_pred_prob_C = np.array([0.1, 0.9])\n",
    "\n",
    "    brier_A = brier_score_loss(y_true, y_pred_prob_A)\n",
    "    brier_B = brier_score_loss(y_true, y_pred_prob_B)\n",
    "    brier_C = brier_score_loss(y_true, y_pred_prob_C)\n",
    "\n",
    "    print(f\"Brier Score (Model A): {brier_A:.3f}\") # Output: Brier Score (Model A): 0.065\n",
    "    print(f\"Brier Score (Model B): {brier_B:.3f}\") # Output: Brier Score (Model B): 0.000\n",
    "    print(f\"Brier Score (Model C): {brier_C:.3f}\") # Output: Brier Score (Model C): 0.810\n",
    "    ```\n",
    "  * **Context:** Useful when assessing the quality and calibration of probability forecasts, common in weather forecasting and other domains requiring accurate probability estimates. Provides an alternative to Log Loss with different sensitivity properties.\n",
    "\n",
    "-----\n",
    "\n",
    "**D. Multi-Class Classification Specifics**\n",
    "\n",
    "These address evaluation when you have more than two possible categories (e.g., classifying images as \"cat\", \"dog\", or \"bird\").\n",
    "\n",
    "-----\n",
    "\n",
    "**21. Averaging Strategies for Multi-Class Metrics**\n",
    "\n",
    "  * **Concept:** Binary metrics like Precision, Recall, and F1-score don't directly apply to multi-class problems. Averaging strategies extend these metrics by calculating them on a per-class basis (treating each class as 'positive' against all others - One-vs-Rest) and then combining these per-class scores.\n",
    "  * **Formulas/Methods:**\n",
    "      * **Macro Average:** Calculate the metric (e.g., F1) independently for each class and then compute the unweighted average of these scores.\n",
    "        $MacroAvg = \\\\frac{1}{K} \\\\sum\\_{k=1}^{K} Metric\\_k$ (where K is the number of classes)\n",
    "      * **Weighted Average:** Calculate the metric independently for each class, then compute the average, weighted by the number of true instances (support) for each class.\n",
    "        $WeightedAvg = \\\\sum\\_{k=1}^{K} (\\\\frac{Support\\_k}{N} \\\\times Metric\\_k)$ (where $Support\\_k$ is the number of true instances of class k, N is total instances)\n",
    "      * **Micro Average:** Calculate the metric globally by summing up the individual True Positives, False Negatives, and False Positives across all classes *before* calculating the final metric.\n",
    "        $MicroPrecision = \\\\frac{\\\\sum TP\\_k}{\\\\sum TP\\_k + \\\\sum FP\\_k}$\n",
    "        $MicroRecall = \\\\frac{\\\\sum TP\\_k}{\\\\sum TP\\_k + \\\\sum FN\\_k}$\n",
    "        $MicroF1 = \\\\frac{2 \\\\times \\\\sum TP\\_k}{2 \\\\times \\\\sum TP\\_k + \\\\sum FP\\_k + \\\\sum FN\\_k}$\n",
    "        (Note: For Precision, Recall, F1, the Micro-averaged scores will all be equal to the overall Accuracy).\n",
    "      * **(Samples Average - for Multi-Label):** Used in multi-label classification (where one instance can belong to multiple classes). Calculates metrics for each instance and then averages. Not typically used for standard multi-class.\n",
    "  * **Interpretation:**\n",
    "      * **Macro:** Treats all classes equally, regardless of their size. Good if you care about performance on infrequent classes.\n",
    "      * **Weighted:** Accounts for class imbalance. Gives more weight to the performance on larger classes. Better reflects overall performance if classes have different importance based on frequency.\n",
    "      * **Micro:** Measures aggregate performance across all samples/predictions. Useful for a global performance view but can mask poor performance on small classes if the large classes are handled well. In multi-class (not multi-label), it's equivalent to accuracy.\n",
    "  * **Pros and Cons:**\n",
    "      * **Macro:** Pro: Treats all classes equally. Con: Can be overly influenced by performance on rare classes.\n",
    "      * **Weighted:** Pro: Accounts for imbalance, reflects typical performance. Con: Can hide poor performance on rare classes if they are weighted low.\n",
    "      * **Micro:** Pro: Easy to calculate, gives global performance. Con: Equivalent to accuracy in multi-class, less informative than macro/weighted for class-specific insights.\n",
    "  * **Example:**\n",
    "    `y_true = [0, 1, 2, 0, 1, 2]`\n",
    "    `y_pred = [0, 2, 1, 0, 0, 1]`\n",
    "    Classes: 0, 1, 2. Support: Class 0: 2, Class 1: 2, Class 2: 2. Total N=6.\n",
    "    Let's find P, R, F1 per class (One-vs-Rest):\n",
    "      * **Class 0:** TP=2 (instances 0, 3), FP=1 (instance 4 predicted 0 was 1), FN=0, TN=3.\n",
    "        P0 = 2/(2+1)=0.67, R0 = 2/(2+0)=1.0, F1\\_0 = 2\\*(0.67\\*1)/(0.67+1) ≈ 0.80\n",
    "\n",
    "      * **Class 1:** TP=0, FP=2 (instances 2, 5 predicted 1 were 2), FN=2 (instances 1, 4 predicted non-1 were 1), TN=2.\n",
    "        P1 = 0/(0+2)=0, R1 = 0/(0+2)=0, F1\\_1 = 0\n",
    "\n",
    "      * **Class 2:** TP=0, FP=1 (instance 1 predicted 2 was 1), FN=2 (instances 2, 5 predicted non-2 were 2), TN=3.\n",
    "        P2 = 0/(0+1)=0, R2 = 0/(0+2)=0, F1\\_2 = 0\n",
    "\n",
    "      * **Macro F1:** (0.80 + 0 + 0) / 3 ≈ 0.267\n",
    "\n",
    "      * **Weighted F1:** (2/6 \\* 0.80) + (2/6 \\* 0) + (2/6 \\* 0) ≈ 0.267 (Same as Macro here because supports are equal)\n",
    "\n",
    "      * **Micro:** Total TP = 2+0+0 = 2. Total FP = 1+2+1 = 4. Total FN = 0+2+2 = 4.\n",
    "        Micro P = 2/(2+4) = 0.33. Micro R = 2/(2+4) = 0.33. Micro F1 = 2*2 / (2*2 + 4 + 4) = 4 / 12 = 0.33.\n",
    "        Accuracy = (TP0+TP1+TP2) / N = 2/6 = 0.33. (Micro scores = Accuracy)\n",
    "  * **Implementation (Scikit-learn):** Use the `average` parameter in metric functions.\n",
    "    ```python\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "    y_true = np.array([0, 1, 2, 0, 1, 2])\n",
    "    y_pred = np.array([0, 2, 1, 0, 0, 1])\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro') # Equals accuracy\n",
    "    per_class_f1 = f1_score(y_true, y_pred, average=None) # Get score for each class\n",
    "\n",
    "    print(f\"Macro F1: {macro_f1:.3f}\")       # Output: Macro F1: 0.267\n",
    "    print(f\"Weighted F1: {weighted_f1:.3f}\")   # Output: Weighted F1: 0.267\n",
    "    print(f\"Micro F1 (Accuracy): {micro_f1:.3f}\")# Output: Micro F1 (Accuracy): 0.333\n",
    "    print(f\"Per-class F1: {per_class_f1}\")    # Output: Per-class F1: [0.8 0.  0. ]\n",
    "    ```\n",
    "  * **Context:** Essential for reporting multi-class classification results meaningfully. The choice between macro and weighted depends on whether you value performance on all classes equally or proportionally to their frequency. Micro is often just reported as overall accuracy.\n",
    "\n",
    "-----\n",
    "\n",
    "**22. Cohen's Kappa**\n",
    "\n",
    "  * **Concept:** A statistic that measures inter-rater agreement for categorical items. In machine learning, it's adapted to measure the agreement between the predicted classifications and the true classifications, while correcting for agreement that might occur purely by chance.\n",
    "  * **Formula:**\n",
    "    $\\\\kappa = \\\\frac{p\\_o - p\\_e}{1 - p\\_e}$\n",
    "    Where:\n",
    "      * $p\\_o$ is the observed agreement (same as overall accuracy): $p\\_o = \\\\frac{TP + TN}{Total}$ for binary, or $\\\\sum \\\\frac{C\\_{ii}}{N}$ for multi-class (sum of diagonal elements / total).\n",
    "      * $p\\_e$ is the expected agreement by chance. For multi-class, it's calculated based on the marginal frequencies of the confusion matrix: $p\\_e = \\\\frac{1}{N^2} \\\\sum\\_{k=1}^{K} (RowSum\\_k \\\\times ColSum\\_k)$, where $RowSum\\_k$ is the total number of actual instances of class k, and $ColSum\\_k$ is the total number of predicted instances of class k.\n",
    "  * **Interpretation:**\n",
    "      * Ranges from -1 to +1.\n",
    "      * $\\\\kappa = 1$: Perfect agreement.\n",
    "      * $\\\\kappa = 0$: Agreement equivalent to chance.\n",
    "      * $\\\\kappa \\< 0$: Agreement worse than chance (rare).\n",
    "      * Common (but arbitrary) interpretation scale: \\<0 Poor, 0-0.2 Slight, 0.2-0.4 Fair, 0.4-0.6 Moderate, 0.6-0.8 Substantial, 0.8-1.0 Almost Perfect.\n",
    "  * **Pros:**\n",
    "      * Corrects for agreement that could happen by chance, potentially giving a better assessment than raw accuracy, especially when class distributions are skewed or one class is predicted much more often than others.\n",
    "  * **Cons:**\n",
    "      * The interpretation scale is subjective.\n",
    "      * Sensitive to prevalence and bias (the \"Kappa paradox\" - kappa can be low even with high accuracy if prevalence is very high/low).\n",
    "      * Less commonly used in modern ML literature compared to AUC, F1, etc.\n",
    "  * **Example:**\n",
    "    Using the multi-class example:\n",
    "    `y_true = [0, 1, 2, 0, 1, 2]`\n",
    "    `y_pred = [0, 2, 1, 0, 0, 1]`\n",
    "    Confusion Matrix:\n",
    "    [[2, 0, 0], \\# Actual 0 -\\> Pred 0,1,2\n",
    "    [1, 0, 1], \\# Actual 1 -\\> Pred 0,1,2\n",
    "    [0, 2, 0]] \\# Actual 2 -\\> Pred 0,1,2\n",
    "    N = 6.\n",
    "    Observed Agreement ($p\\_o$): Accuracy = (2+0+0)/6 = 2/6 = 0.333\n",
    "    Expected Agreement ($p\\_e$):\n",
    "    Row Sums (Actuals): R0=2, R1=2, R2=2\n",
    "    Col Sums (Predicted): C0=3, C1=2, C2=1\n",
    "    $p\\_e = \\\\frac{1}{6^2} [ (R0 \\\\times C0) + (R1 \\\\times C1) + (R2 \\\\times C2) ]$\n",
    "    $p\\_e = \\\\frac{1}{36} [ (2 \\\\times 3) + (2 \\\\times 2) + (2 \\\\times 1) ] = \\\\frac{1}{36} [ 6 + 4 + 2 ] = \\\\frac{12}{36} = 0.333$\n",
    "    Kappa: $\\\\kappa = \\\\frac{p\\_o - p\\_e}{1 - p\\_e} = \\\\frac{0.333 - 0.333}{1 - 0.333} = \\\\frac{0}{0.667} = 0$\n",
    "    In this case, Kappa is 0, suggesting the model's performance is no better than chance agreement, even though accuracy was 33%. This happened because the expected chance agreement was also 33%.\n",
    "  * **Implementation (Scikit-learn):**\n",
    "    ```python\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "    y_true = np.array([0, 1, 2, 0, 1, 2])\n",
    "    y_pred = np.array([0, 2, 1, 0, 0, 1])\n",
    "\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    print(f\"Cohen's Kappa: {kappa:.3f}\") # Output: Cohen's Kappa: 0.000\n",
    "    ```\n",
    "  * **Context:** Can be useful as an alternative to accuracy when chance agreement is a concern, for example, when evaluating algorithms against human raters or in domains with strong prior probabilities. However, interpret with caution due to its sensitivity to prevalence.\n",
    "\n",
    "-----\n",
    "\n",
    "**23. Multi-Class Confusion Matrix**\n",
    "\n",
    "  * **Concept:** An extension of the binary confusion matrix to handle K \\> 2 classes. It's a KxK matrix where rows typically represent the actual (true) classes and columns represent the predicted classes.\n",
    "\n",
    "  * **Structure:**\n",
    "    |                 | Predicted: Class 0 | Predicted: Class 1 | ... | Predicted: Class K-1 |\n",
    "    | :-------------- | :----------------- | :----------------- | :-: | :------------------- |\n",
    "    | **Actual: Class 0** | C\\<sub\\>00\\</sub\\> (TP\\<sub\\>0\\</sub\\>) | C\\<sub\\>01\\</sub\\>         | ... | C\\<sub\\>0,K-1\\</sub\\>        |\n",
    "    | **Actual: Class 1** | C\\<sub\\>10\\</sub\\>         | C\\<sub\\>11\\</sub\\> (TP\\<sub\\>1\\</sub\\>) | ... | C\\<sub\\>1,K-1\\</sub\\>        |\n",
    "    | ...             | ...                | ...                | ... | ...                  |\n",
    "    | **Actual: Class K-1**| C\\<sub\\>K-1,0\\</sub\\>      | C\\<sub\\>K-1,1\\</sub\\>      | ... | C\\<sub\\>K-1,K-1\\</sub\\> (TP\\<sub\\>K-1\\</sub\\>)|\n",
    "\n",
    "      * $C\\_{ij}$: Number of instances belonging to actual class `i` that were predicted as class `j`.\n",
    "      * Diagonal elements $C\\_{ii}$: Correct classifications (True Positives for class `i`).\n",
    "      * Off-diagonal elements $C\\_{ij}$ (where $i \\\\neq j$): Misclassifications. An instance of class `i` was wrongly predicted as class `j`. This count contributes to False Negatives for class `i` and False Positives for class `j`.\n",
    "\n",
    "  * **Interpretation:** Allows a detailed view of performance:\n",
    "\n",
    "      * Diagonal shows correctly classified counts per class.\n",
    "      * Off-diagonals reveal *which* classes are being confused with each other. Large values off-diagonal indicate common confusion points.\n",
    "\n",
    "  * **Pros:**\n",
    "\n",
    "      * Provides the most complete picture of classification results.\n",
    "      * Directly shows error patterns between specific classes.\n",
    "      * Foundation for calculating per-class metrics (P, R, F1) and macro/weighted averages.\n",
    "\n",
    "  * **Cons:**\n",
    "\n",
    "      * Not a single summary score.\n",
    "      * Can become large and difficult to visualize/interpret if the number of classes (K) is very high.\n",
    "\n",
    "  * **Example:**\n",
    "    Using the multi-class example again:\n",
    "    `y_true = [0, 1, 2, 0, 1, 2]`\n",
    "    `y_pred = [0, 2, 1, 0, 0, 1]`\n",
    "    The confusion matrix is:\n",
    "    Predicted: 0  1  2\n",
    "    Actual 0: [[**2**, 0, 0],\n",
    "    Actual 1:  [ 1, **0**, 1],\n",
    "    Actual 2:  [ 0, 2, **0**]]\n",
    "\n",
    "    Interpretation:\n",
    "\n",
    "      * Class 0: 2 correctly predicted as 0. No errors *from* class 0.\n",
    "      * Class 1: 0 correctly predicted as 1. 1 instance misclassified as 0, 1 instance misclassified as 2.\n",
    "      * Class 2: 0 correctly predicted as 2. 2 instances misclassified as 1.\n",
    "        We can see the model struggles with classes 1 and 2, often confusing them (predicting 1 when it's 2, predicting 2 when it's 1). It also mistakes class 1 for class 0 once. Class 0 seems easiest for this model.\n",
    "\n",
    "  * **Implementation (Scikit-learn):** Handles multi-class input directly.\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns # For better visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    y_true = np.array([0, 1, 2, 0, 1, 2])\n",
    "    y_pred = np.array([0, 2, 1, 0, 0, 1])\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Multi-Class Confusion Matrix:\\n\", cm)\n",
    "    # Output:\n",
    "    # [[2 0 0]\n",
    "    #  [1 0 1]\n",
    "    #  [0 2 0]]\n",
    "\n",
    "    # Optional: Visualize for clarity\n",
    "    # plt.figure(figsize=(6,4))\n",
    "    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "    #             xticklabels=['Pred 0', 'Pred 1', 'Pred 2'],\n",
    "    #             yticklabels=['Actual 0', 'Actual 1', 'Actual 2'])\n",
    "    # plt.xlabel('Predicted Label')\n",
    "    # plt.ylabel('True Label')\n",
    "    # plt.title('Multi-Class Confusion Matrix')\n",
    "    # plt.show()\n",
    "    ```\n",
    "\n",
    "  * **Context:** An essential tool for diagnosing the specific strengths and weaknesses of a multi-class classifier. Examining the off-diagonal elements is key to understanding error patterns.\n",
    "\n",
    "-----\n",
    "\n",
    "**E. Utility & Reporting**\n",
    "\n",
    "-----\n",
    "\n",
    "**24. Classification Report (Scikit-learn)**\n",
    "\n",
    "  * **Concept:** A utility function in Scikit-learn that builds a text report showing the main classification metrics for each class in a multi-class problem (or for the positive class in binary).\n",
    "  * **Content:** The report typically includes:\n",
    "      * **Precision:** Per class.\n",
    "      * **Recall:** Per class.\n",
    "      * **F1-score:** Per class.\n",
    "      * **Support:** The number of true instances for each class.\n",
    "      * **Accuracy:** Overall accuracy of the model.\n",
    "      * **Macro Avg:** Macro average for Precision, Recall, F1.\n",
    "      * **Weighted Avg:** Weighted average for Precision, Recall, F1.\n",
    "  * **Interpretation:** Provides a quick, comprehensive text summary of the model's performance, breaking it down by class and providing useful averages. Allows for easy comparison of P/R/F1 across classes and overall performance assessment.\n",
    "  * **Pros:**\n",
    "      * Concise and convenient overview of key metrics.\n",
    "      * Includes per-class scores and relevant averages automatically.\n",
    "      * Easy to generate and include in logs or reports.\n",
    "  * **Cons:**\n",
    "      * Output is text-based, less suitable for direct programmatic use compared to individual metric functions.\n",
    "      * Doesn't include all possible metrics (e.g., Specificity, MCC, AUC, Log Loss, Brier Score).\n",
    "  * **Example:**\n",
    "    Using the multi-class example:\n",
    "    `y_true = [0, 1, 2, 0, 1, 2]`\n",
    "    `y_pred = [0, 2, 1, 0, 0, 1]`\n",
    "  * **Implementation (Scikit-learn):**\n",
    "    ```python\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    y_true = np.array([0, 1, 2, 0, 1, 2])\n",
    "    y_pred = np.array([0, 2, 1, 0, 0, 1])\n",
    "    target_names = ['Class 0', 'Class 1', 'Class 2'] # Optional: names for labels\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=target_names)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Output:\n",
    "    # Classification Report:\n",
    "    #                precision    recall  f1-score   support\n",
    "    #\n",
    "    #      Class 0       0.67      1.00      0.80         2  <- P0, R0, F1_0, Support0\n",
    "    #      Class 1       0.00      0.00      0.00         2  <- P1, R1, F1_1, Support1\n",
    "    #      Class 2       0.00      0.00      0.00         2  <- P2, R2, F1_2, Support2\n",
    "    #\n",
    "    #     accuracy                           0.33         6  <- Overall Accuracy\n",
    "    #    macro avg       0.22      0.33      0.27         6  <- Macro Avg P, R, F1\n",
    "    # weighted avg       0.22      0.33      0.27         6  <- Weighted Avg P, R, F1\n",
    "    # (Note: Small calculation differences vs manual example due to rounding/implementation details are possible)\n",
    "    ```\n",
    "  * **Context:** A standard and highly useful first step for reporting the performance of a classification model. It gives a good balance of detail (per-class) and summary (averages).\n",
    "\n",
    "-----\n",
    "\n",
    "This covers the core classification metrics in detail. Remember, the *best* metric depends heavily on your specific problem, the costs associated with different types of errors, and the characteristics of your data (especially class balance). Often, looking at multiple metrics provides the most complete understanding of your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec3cfb-fc79-4126-bb13-de954a519eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
