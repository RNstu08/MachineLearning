{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13977945-1aaa-4d4f-ac1a-4987777b6457",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Feature Engineering Roadmap\n",
    "\n",
    "This roadmap outlines the essential steps, techniques, and tools for cleaning, transforming, and enhancing data to build effective machine learning models using Python (`Pandas`, `NumPy`, `Scikit-learn`).\n",
    "\n",
    "## I. Introduction & Initial Exploration\n",
    "\n",
    "* **Why Preprocess?** The \"Garbage In, Garbage Out\" principle. Understanding ML algorithm requirements (numerical input, scale sensitivity, handling missing values).\n",
    "* **Goals:** Improve data quality, extract meaningful patterns, meet algorithm assumptions, enhance model performance.\n",
    "* **Loading Data:** Using `Pandas` (`pd.read_csv`, `pd.read_excel`, `pd.read_sql`, etc.).\n",
    "* **Initial Inspection (Crucial First Step):**\n",
    "    * Viewing data: `.head()`, `.tail()`, `.sample()`.\n",
    "    * Understanding structure & types: `.info()`, `.shape`, `.dtypes`.\n",
    "    * Summary statistics: `.describe(include='all')`.\n",
    "    * Checking unique values & counts: `.nunique()`, `.value_counts()`.\n",
    "    * Visual exploration (briefly): Using `Matplotlib`/`Seaborn` for histograms, box plots, scatter plots to initially identify distributions, outliers, and relationships.\n",
    "\n",
    "## II. Handling Missing Data\n",
    "\n",
    "* **Identifying Missing Values:** Using `.isnull().sum()` or `.isna().sum()`. Visualizing missingness (e.g., using `missingno` library - optional).\n",
    "* **Strategies:**\n",
    "    * **Deletion:**\n",
    "        * Listwise Deletion (Row Removal): `df.dropna()`. Pros: Simple. Cons: Data loss, potential bias.\n",
    "        * Column Deletion (Feature Removal): `df.dropna(axis=1, thresh=...)`. Pros: Removes uninformative features. Cons: Information loss.\n",
    "    * **Imputation (Filling Values):** Often preferred over deletion.\n",
    "        * **Simple Imputation:**\n",
    "            * Mean: `SimpleImputer(strategy='mean')` (Numerical only, sensitive to outliers).\n",
    "            * Median: `SimpleImputer(strategy='median')` (Numerical only, robust to outliers).\n",
    "            * Mode: `SimpleImputer(strategy='most_frequent')` (Categorical or numerical).\n",
    "            * Constant: `SimpleImputer(strategy='constant', fill_value=...)`.\n",
    "        * **Advanced Imputation:**\n",
    "            * KNN Imputation: `KNNImputer` (Uses nearest neighbors).\n",
    "            * Multivariate Imputation: `IterativeImputer` (Models features to predict missing values).\n",
    "        * **Missing Indicator Feature:** Creating a binary column indicating missingness (`SimpleImputer(add_indicator=True)` or manually). Can help models learn from the missingness pattern.\n",
    "* **Implementation:** Using `Pandas` `.fillna()` or `Scikit-learn` imputers (preferred within pipelines). Fit imputers on training data only.\n",
    "\n",
    "## III. Encoding Categorical Data\n",
    "\n",
    "* **Understanding Types:** Nominal (no order) vs. Ordinal (inherent order).\n",
    "* **Techniques:**\n",
    "    * **Ordinal Encoding:** `OrdinalEncoder` (for ordinal features, specify category order). `LabelEncoder` (typically only for the target variable `y`). Cons: Implies potentially false order if used on nominal data.\n",
    "    * **One-Hot Encoding (OHE):** `OneHotEncoder`, `pd.get_dummies()` (for nominal features). Creates binary columns. Pros: No order implied. Cons: High dimensionality for high cardinality features (many unique categories). Handle `drop` parameter (`'first'`, `'if_binary'`) to avoid multicollinearity. Handle `handle_unknown='ignore'` for unseen test set categories.\n",
    "    * **Other Techniques (for High Cardinality):**\n",
    "        * Binary Encoding.\n",
    "        * Feature Hashing (`FeatureHasher`).\n",
    "        * Target Encoding (uses target information, risk of leakage if not done carefully within CV).\n",
    "\n",
    "## IV. Feature Scaling (Numerical Data)\n",
    "\n",
    "* **Why Scale?** Importance for distance-based algorithms (KNN, SVM), gradient descent (Linear/Logistic Regression, NNs), and regularization. Tree-based models are less sensitive.\n",
    "* **Techniques (`sklearn.preprocessing`):**\n",
    "    * **Standardization (Z-score):** `StandardScaler` (mean=0, std=1). Default choice generally.\n",
    "    * **Normalization (Min-Max):** `MinMaxScaler` (scales to a range, e.g., `[0, 1]`). Sensitive to outliers. Useful for specific cases (e.g., image pixels).\n",
    "    * **Robust Scaling:** `RobustScaler` (uses median and IQR). Less sensitive to outliers.\n",
    "* **Implementation Note:** Fit scaler on training data ONLY, then transform both training and test data.\n",
    "\n",
    "## V. Handling Outliers\n",
    "\n",
    "* **Identifying Outliers:**\n",
    "    * Visualization: Box plots, scatter plots, histograms.\n",
    "    * Statistical Methods: Z-score, IQR (Interquartile Range) method.\n",
    "* **Strategies:**\n",
    "    * **Removal:** Delete outlier data points (use with caution, understand why they are outliers).\n",
    "    * **Transformation:** Apply non-linear transformations (e.g., `log`, `sqrt`, `Box-Cox`) to reduce skewness and outlier impact.\n",
    "    * **Capping/Winsorizing:** Limit extreme values to a certain percentile (e.g., replace values above 99th percentile with the 99th percentile value).\n",
    "    * **Using Robust Algorithms:** Employ models less sensitive to outliers (e.g., `RobustScaler`, tree-based models, `HuberRegressor`).\n",
    "    * **Treat as Missing:** Consider treating extreme outliers as missing data and impute them.\n",
    "\n",
    "## VI. Feature Engineering\n",
    "\n",
    "* **Goal:** Create new features from existing ones to improve model performance by providing more relevant information or capturing non-linear relationships. Often requires domain knowledge.\n",
    "* **Techniques:**\n",
    "    * **Interaction Features:** Combining features (e.g., `X1 * X2`, `X1 / X2`). `PolynomialFeatures` generates polynomial and interaction terms automatically.\n",
    "    * **Transformations:** Applying mathematical functions (`log`, `sqrt`, `exp`, `Box-Cox`) to numerical features to stabilize variance, handle skewness, or linearize relationships.\n",
    "    * **Binning/Discretization:** Grouping continuous features into discrete bins (`KBinsDiscretizer`, `pd.cut`, `pd.qcut`). Can help capture non-linearities for linear models.\n",
    "    * **Date/Time Features:** Extracting components like year, month, day, day of week, hour, is_weekend, time differences from datetime columns (`Pandas` `.dt` accessor).\n",
    "    * **Domain-Specific Features:** Creating features based on understanding the problem context (e.g., distance calculations, text-based features like word counts/sentiment, aggregation from related data).\n",
    "\n",
    "## VII. Feature Selection\n",
    "\n",
    "* **Goal:** Select a subset of the most relevant features to improve model performance (reduce overfitting, decrease training time) and interpretability.\n",
    "* **Techniques (`sklearn.feature_selection`):**\n",
    "    * **Filter Methods:** Evaluate features independently of the model.\n",
    "        * `VarianceThreshold`: Remove low-variance (e.g., constant) features.\n",
    "        * Univariate Statistical Tests: `SelectKBest`, `SelectPercentile` using tests like `f_classif`/`f_regression`, `chi2`, `mutual_info_classif`/`mutual_info_regression`.\n",
    "    * **Wrapper Methods:** Use a specific model to evaluate subsets of features.\n",
    "        * Recursive Feature Elimination (`RFE`, `RFECV`): Iteratively remove the least important features based on model performance.\n",
    "    * **Embedded Methods:** Feature selection is part of the model training process.\n",
    "        * L1 Regularization (`Lasso`): Coefficients of irrelevant features are shrunk to zero.\n",
    "        * Tree-based Importances: Accessing `feature_importances_` from tree models (Decision Tree, Random Forest, Gradient Boosting).\n",
    "\n",
    "## VIII. Pipelines & ColumnTransformer (Revisited)\n",
    "\n",
    "* **Importance:** Essential for applying preprocessing and feature engineering steps correctly and consistently, especially within cross-validation loops to prevent data leakage.\n",
    "* **Tools:** `Pipeline`, `make_pipeline`, `ColumnTransformer`, `make_column_transformer`. Allows chaining all steps (imputation, encoding, scaling, feature engineering, selection, final model) into a single `Scikit-learn` estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008bd68-f2e7-4f72-91b2-2fcf9a74e5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
