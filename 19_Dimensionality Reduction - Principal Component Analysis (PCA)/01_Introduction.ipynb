{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0261766c-50e8-403f-9179-a81d743ed694",
   "metadata": {},
   "source": [
    "**IV. Dimensionality Reduction**.\n",
    "\n",
    "The first and most common technique we'll discuss here is:\n",
    "\n",
    "**Topic 19: Dimensionality Reduction - Principal Component Analysis (PCA)**\n",
    "\n",
    "**1. Introduction to Dimensionality Reduction**\n",
    "\n",
    "* **What is Dimensionality?** In machine learning, \"dimensionality\" refers to the number of features (or variables) in your dataset. For example, a dataset with 100 features is considered to have 100 dimensions.\n",
    "* **The \"Curse of Dimensionality\":** Working with very high-dimensional data can lead to several problems:\n",
    "    * **Increased Computational Cost:** More features mean more computations for training models, making them slower.\n",
    "    * **Increased Memory Usage:** Storing and processing high-dimensional data requires more memory.\n",
    "    * **Overfitting:** With many features, models (especially complex ones) are more likely to fit the noise in the training data rather than the underlying signal, leading to poor generalization on unseen data.\n",
    "    * **Sparsity of Data:** In high-dimensional spaces, data points tend to become very sparse. The volume of the space increases exponentially with the number of dimensions, so you need exponentially more data to maintain the same density of points. This makes it harder to find meaningful patterns or define local neighborhoods (an issue for algorithms like KNN or DBSCAN).\n",
    "    * **Multicollinearity:** High-dimensional data often contains redundant or highly correlated features, which can make models unstable and harder to interpret.\n",
    "* **What is Dimensionality Reduction?**\n",
    "    * It's the process of reducing the number of features (dimensions) in a dataset while trying to preserve as much of the important information or structure as possible.\n",
    "    * The goal is to obtain a lower-dimensional representation of the data that is easier to work with, less prone to the curse of dimensionality, and can sometimes even improve model performance by removing noise or redundant information.\n",
    "* **Two Main Approaches:**\n",
    "    1.  **Feature Selection:** Select a subset of the original features that are most relevant to the problem. We discard the less important features. (e.g., using filter methods, wrapper methods, or embedded methods like Lasso).\n",
    "    2.  **Feature Extraction (Projection):** Create a new, smaller set of features (called \"components\" or \"latent variables\") by combining or transforming the original features. PCA is a prime example of this. These new features are usually linear or non-linear combinations of the original ones.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Principal Component Analysis (PCA): The Goal**\n",
    "\n",
    "* **PCA is a linear feature extraction technique.** It aims to transform a dataset with many (possibly correlated) features into a new dataset with a smaller number of **uncorrelated features**, called **principal components (PCs)**.\n",
    "* **Objective:**\n",
    "    1.  **Maximize Variance:** PCA finds the directions (principal components) in the feature space along which the data varies the most. The first principal component (PC1) is the direction that captures the maximum variance in the data. The second principal component (PC2) is the direction, orthogonal (perpendicular) to PC1, that captures the maximum *remaining* variance, and so on.\n",
    "    2.  **Minimize Reconstruction Error:** Equivalently, PCA finds a lower-dimensional projection of the data that minimizes the squared error between the original data points and their projections onto this lower-dimensional subspace.\n",
    "* **Key Idea:** By projecting the original data onto a lower-dimensional subspace formed by the principal components that capture the most variance, we can reduce dimensionality while retaining the most significant information (patterns and relationships) present in the data. Features with low variance are often considered less informative or noisy.\n",
    "\n",
    "**Conceptual Diagram of PCA (2D to 1D):**\n",
    "Imagine a scatter plot of 2D data points that form an elongated cloud (indicating correlation between Feature 1 and Feature 2).\n",
    "* **PC1:** Would be an axis (a line) passing through the center of the cloud, aligned with its longest direction (the direction of maximum variance).\n",
    "* **PC2:** Would be an axis perpendicular to PC1, aligned with the shorter direction of the cloud (capturing the remaining variance).\n",
    "* **Dimensionality Reduction:** If we decide to reduce to 1 dimension, we would project all the data points onto the PC1 axis. This 1D representation (the positions of the projected points along PC1) would capture most of the variability of the original 2D data.\n",
    "\n",
    "```\n",
    "Feature 2 ^\n",
    "          |         .\n",
    "          |       .\n",
    "          |     .  <-- PC1 (Direction of max variance)\n",
    "          |   .\n",
    "          | .\n",
    "          |.\n",
    "          ----------------------------> Feature 1\n",
    "                 \\\n",
    "                  \\ PC2 (Orthogonal to PC1, captures remaining variance)\n",
    "```\n",
    "Projecting onto PC1 effectively \"flattens\" the data onto that line, reducing it from 2D to 1D while keeping the most spread.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Key Concepts in PCA**\n",
    "\n",
    "* **Principal Components (PCs):**\n",
    "    * These are the new, uncorrelated features derived by PCA. They are linear combinations of the original features.\n",
    "    * They are ordered by the amount of variance they explain: PC1 explains the most variance, PC2 explains the second most (and is orthogonal to PC1), and so on.\n",
    "    * The number of principal components is less than or equal to the number of original features.\n",
    "* **Eigenvectors and Eigenvalues (from the Covariance Matrix):**\n",
    "    * The directions of the principal components are given by the **eigenvectors** of the covariance matrix of the original data.\n",
    "    * The amount of variance explained by each principal component is given by its corresponding **eigenvalue**. Larger eigenvalues correspond to principal components that capture more variance.\n",
    "* **Covariance Matrix:**\n",
    "    * A square matrix that describes the variance of each feature and the covariance between pairs of features.\n",
    "    * $Cov(X, Y) = E[(X - E[X])(Y - E[Y])]$\n",
    "    * The diagonal elements are the variances of individual features. Off-diagonal elements are the covariances between pairs of features.\n",
    "    * PCA essentially performs an eigendecomposition (or Singular Value Decomposition - SVD, which is more numerically stable) of this covariance matrix.\n",
    "* **Explained Variance Ratio:**\n",
    "    * This is the percentage of the total variance in the original dataset that is captured by each principal component.\n",
    "    * It's calculated as: (Eigenvalue of PC$_i$) / (Sum of all Eigenvalues).\n",
    "    * By looking at the cumulative explained variance ratio, we can decide how many principal components to keep to retain a desired percentage of the total variance (e.g., 95% or 99%).\n",
    "\n",
    "---\n",
    "\n",
    "**4. How PCA Works Mathematically (The Steps)**\n",
    "\n",
    "PCA finds the principal components by analyzing the covariance structure of the data. Here's a breakdown of the typical steps involved:\n",
    "\n",
    "1.  **Standardize the Data (Feature Scaling):**\n",
    "    * This is a **crucial preprocessing step** for PCA.\n",
    "    * PCA is sensitive to the variances of the initial variables. If features are on different scales, features with larger variances will dominate the principal components, even if they are not inherently \"more important.\"\n",
    "    * Standardization transforms the data so that each feature has a mean of 0 and a standard deviation of 1.\n",
    "        $$X_{scaled} = \\frac{X - \\text{mean}(X)}{\\text{std}(X)}$$\n",
    "    * **Conceptual Diagram:** Imagine one feature ranging from 0-1 and another from 0-1000. Without scaling, the second feature's variance would dwarf the first, and PCA would likely align PC1 mostly with the second feature. Scaling puts them on equal footing.\n",
    "\n",
    "2.  **Compute the Covariance Matrix:**\n",
    "    * Once the data is standardized, the next step is to compute the covariance matrix ($\\Sigma$) of the features.\n",
    "    * For a dataset with $p$ features, the covariance matrix will be a $p \\times p$ symmetric matrix.\n",
    "    * The element $\\Sigma_{ij}$ in the matrix is the covariance between feature $i$ and feature $j$.\n",
    "    * The diagonal elements $\\Sigma_{ii}$ are the variances of feature $i$.\n",
    "    * The covariance matrix captures how features vary together.\n",
    "\n",
    "3.  **Perform Eigendecomposition (or SVD) on the Covariance Matrix:**\n",
    "    * The core of PCA is to find the directions (principal components) of maximum variance. These directions are the **eigenvectors** of the covariance matrix.\n",
    "    * The amount of variance explained by each of these directions is given by the corresponding **eigenvalue**.\n",
    "    * **Eigendecomposition:** If $\\Sigma$ is the covariance matrix, we solve:\n",
    "        $$\\Sigma v = \\lambda v$$\n",
    "        where:\n",
    "        * $v$ is an eigenvector (representing a principal component direction).\n",
    "        * $\\lambda$ is the corresponding eigenvalue (representing the variance along that eigenvector direction).\n",
    "    * **Singular Value Decomposition (SVD):** In practice, SVD is often used to compute the principal components, especially for numerical stability and efficiency. SVD decomposes the (standardized) data matrix $X_{scaled}$ directly: $X_{scaled} = U S V^T$. The principal components can be derived from $V$, and the eigenvalues are related to the singular values in $S$. Scikit-learn's PCA implementation typically uses SVD.\n",
    "\n",
    "4.  **Sort Eigenvectors by Eigenvalues:**\n",
    "    * The eigenvectors are sorted in descending order based on their corresponding eigenvalues.\n",
    "    * The eigenvector with the largest eigenvalue is the direction of maximum variance – this is the **first principal component (PC1)**.\n",
    "    * The eigenvector with the second largest eigenvalue is orthogonal to PC1 and captures the second most variance – this is the **second principal component (PC2)**, and so on.\n",
    "\n",
    "5.  **Select Principal Components (Dimensionality Reduction Step):**\n",
    "    * This is where we decide how many principal components to keep. We want to keep the components that capture a significant amount of the total variance while discarding those that capture little variance (often considered noise).\n",
    "    * We'll discuss methods for choosing the number of components (e.g., explained variance ratio, scree plot) in the next section.\n",
    "    * Let's say we decide to keep $k$ principal components (where $k < p$, the original number of features). We select the top $k$ eigenvectors (those with the $k$ largest eigenvalues). These $k$ eigenvectors form a new feature subspace.\n",
    "\n",
    "6.  **Transform the Data (Project onto the New Subspace):**\n",
    "    * The final step is to project the original standardized data onto the subspace defined by the selected $k$ principal components.\n",
    "    * This is done by taking the dot product of the standardized data matrix ($X_{scaled}$) with the matrix formed by the selected $k$ eigenvectors (also called the projection matrix $W$):\n",
    "        $$X_{PCA} = X_{scaled} W$$\n",
    "    * $X_{PCA}$ is the new, lower-dimensional dataset where each row is a data point represented by its $k$ principal component scores. These new features (the principal components) are uncorrelated.\n",
    "\n",
    "---\n",
    "\n",
    "**Conceptual Diagram of Projection:**\n",
    "Imagine our 2D data cloud and PC1 (the line of max variance).\n",
    "* To get the PC1 score for a data point, we project that point perpendicularly onto the PC1 line.\n",
    "* The position of this projected point on the PC1 line (its distance from the origin along PC1) is its score for the first principal component.\n",
    "* If we keep only PC1, our new dataset is just this set of scores.\n",
    "\n",
    "**5. Choosing the Number of Principal Components ($k$)**\n",
    "\n",
    "This is a critical decision in PCA. Keeping too few components might lead to significant information loss, while keeping too many might not achieve sufficient dimensionality reduction.\n",
    "\n",
    "* **a) Explained Variance Ratio Plot (Cumulative Explained Variance):**\n",
    "    1.  Calculate the explained variance ratio for each principal component:\n",
    "        $$\\text{Explained Variance Ratio (PC}_i\\text{)} = \\frac{\\text{Eigenvalue of PC}_i}{\\text{Sum of all Eigenvalues}}$$\n",
    "    2.  Plot the cumulative explained variance as you add more principal components (from PC1, PC1+PC2, PC1+PC2+PC3, etc.).\n",
    "    * **Conceptual Diagram (Explained Variance Plot):**\n",
    "        ```\n",
    "        Cumulative Explained Variance ^\n",
    "                                  |\n",
    "                               1.0 +-----------------------\n",
    "                                  |                     .**\n",
    "                                  |                  .*\n",
    "                                  |                .*\n",
    "                                  |             .*\n",
    "                                  |          .*\n",
    "                                  |       .*\n",
    "                                  |    .*\n",
    "                                  | .*\n",
    "                                  +----------------------------> Number of Components (k)\n",
    "        ```\n",
    "    * **Interpretation:** Look for the number of components ($k$) that explain a desired percentage of the total variance (e.g., 90%, 95%, or 99%). For instance, if the first 5 components explain 95% of the variance, you might choose $k=5$. The plot will show how quickly the cumulative variance approaches 100%.\n",
    "\n",
    "* **b) Scree Plot (Plot of Eigenvalues):**\n",
    "    1.  Plot the eigenvalues of the principal components in descending order.\n",
    "    * **Conceptual Diagram (Scree Plot):**\n",
    "        ```\n",
    "        Eigenvalue ^\n",
    "                   |\n",
    "                   | *\n",
    "                   |  *\n",
    "                   |   *\n",
    "                   |    * <-- \"Elbow\" or point where slope flattens\n",
    "                   |     .\n",
    "                   |      .\n",
    "                   |       .\n",
    "                   |        .\n",
    "                   +----------------------------> Component Number\n",
    "        ```\n",
    "    * **Interpretation:** Look for an \"elbow\" or a point where the eigenvalues start to level off. The components *before* this elbow are generally considered the most significant ones to keep. The idea is that components after the elbow contribute much less to the overall variance and might represent noise.\n",
    "\n",
    "* **c) Arbitrary Percentage of Variance:**\n",
    "    * Decide on a threshold for the total variance you want to retain (e.g., 95%).\n",
    "    * Add principal components one by one until the cumulative explained variance exceeds this threshold.\n",
    "\n",
    "* **d) Based on Application:** Sometimes the number of dimensions is chosen based on the requirements of a subsequent task (e.g., for visualization, you'd choose $k=2$ or $k=3$).\n",
    "\n",
    "It's often good to use a combination of these methods and consider the trade-off between dimensionality reduction and information loss.\n",
    "\n",
    "---\n",
    "\n",
    "**6. Use Cases of PCA**\n",
    "\n",
    "PCA is a versatile technique used in many fields and for various purposes:\n",
    "\n",
    "1.  **Dimensionality Reduction for Machine Learning Models:**\n",
    "    * This is the most common use case. By reducing the number of features, PCA can:\n",
    "        * **Speed up training time:** Fewer features mean less computation for subsequent model training.\n",
    "        * **Reduce model complexity:** Simpler models (with fewer input features) can sometimes generalize better and are less prone to overfitting, especially if the original features were highly correlated or contained noise.\n",
    "        * **Combat the Curse of Dimensionality:** Makes it easier for algorithms to find patterns in high-dimensional spaces.\n",
    "    * The transformed principal components (which are uncorrelated) can then be fed as input to supervised learning algorithms (like Linear Regression, Logistic Regression, SVMs, Neural Networks, etc.) or even other unsupervised learning algorithms.\n",
    "\n",
    "2.  **Data Visualization:**\n",
    "    * Humans can easily visualize data in 2 or 3 dimensions. If you have high-dimensional data, PCA can be used to reduce it to 2 or 3 principal components.\n",
    "    * Plotting these top 2 or 3 components can help reveal the underlying structure, clusters, or patterns in the data that would be impossible to see in the original high-dimensional space.\n",
    "    * **Conceptual Diagram:** Imagine a 100-dimensional dataset. PCA reduces it to PC1 and PC2. A scatter plot of PC1 vs. PC2 might show distinct groups of data points, suggesting potential clusters.\n",
    "\n",
    "3.  **Noise Reduction / Denoising:**\n",
    "    * Principal components associated with smaller eigenvalues (less variance) often capture noise or minor variations in the data.\n",
    "    * By discarding these low-variance components and reconstructing the data using only the high-variance components, PCA can effectively filter out some noise.\n",
    "    * **Conceptual Diagram:** Imagine a signal with some high-frequency noise. PCA might separate the main signal into the first few PCs and the noise into later PCs. Reconstructing with only the early PCs can give a cleaner signal.\n",
    "\n",
    "4.  **Feature Engineering / Feature Extraction:**\n",
    "    * The principal components themselves can be considered new, synthetic features that are linear combinations of the original features.\n",
    "    * These new features are uncorrelated, which can be beneficial for some machine learning algorithms that are sensitive to multicollinearity (e.g., Linear Regression).\n",
    "\n",
    "5.  **Image Compression:**\n",
    "    * An image can be represented as a matrix of pixel values. PCA can be applied to reduce the dimensionality of this data.\n",
    "    * By keeping only the principal components that capture most of the variance (i.e., the most important visual information) and discarding the rest, the image can be reconstructed with some loss of detail but requiring significantly less storage space.\n",
    "\n",
    "6.  **Anomaly Detection:**\n",
    "    * Outliers or anomalies might be far from the main distribution of data along the principal component axes, or they might have large reconstruction errors when projected onto a lower-dimensional PCA subspace.\n",
    "\n",
    "7.  **Bioinformatics (e.g., Gene Expression Analysis):**\n",
    "    * Gene expression datasets often have a very large number of genes (features) and a relatively small number of samples. PCA is widely used to reduce dimensionality, visualize samples, and identify patterns in gene expression.\n",
    "\n",
    "8.  **Finance (e.g., Portfolio Management, Risk Analysis):**\n",
    "    * Can be used to identify underlying factors driving asset returns or to reduce the dimensionality of risk factors.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Pros and Cons of PCA**\n",
    "\n",
    "Like any technique, PCA has its strengths and weaknesses.\n",
    "\n",
    "**Pros of PCA:**\n",
    "\n",
    "1.  **Reduces Dimensionality:** This is its primary benefit, leading to faster computations, reduced memory usage, and potentially mitigating the curse of dimensionality.\n",
    "2.  **Removes Multicollinearity:** The resulting principal components are orthogonal (uncorrelated) to each other. This can be very beneficial for models that are sensitive to correlated features (e.g., linear regression).\n",
    "3.  **Noise Reduction:** By discarding components with low variance, PCA can help filter out noise from the data.\n",
    "4.  **Data Compression:** Can be used to compress data by storing only the most important components.\n",
    "5.  **Improves Visualization:** Allows high-dimensional data to be visualized in 2D or 3D.\n",
    "6.  **No Need for Target Variable (Unsupervised):** PCA is an unsupervised technique; it only looks at the relationships between features (the covariance structure) and doesn't require a target variable.\n",
    "7.  **Mathematical Foundation:** Based on well-understood linear algebra (eigenvectors, eigenvalues, SVD).\n",
    "\n",
    "**Cons of PCA:**\n",
    "\n",
    "1.  **Information Loss:** Dimensionality reduction inherently involves some loss of information. While PCA tries to minimize this by retaining components with the most variance, some details will be lost. The amount of loss depends on how many components are discarded.\n",
    "2.  **Reduced Interpretability of Features:** The principal components are linear combinations of the original features. While you can look at the \"loadings\" (coefficients of the original features in each PC) to understand what a PC represents, the PCs themselves are often less interpretable than the original, domain-specific features.\n",
    "    * *Example:* If original features were \"height\" and \"weight,\" PC1 might be a combination like \"0.7*height + 0.7*weight,\" which could be interpreted as a \"size\" component, but it's less direct.\n",
    "3.  **Assumes Linearity:** PCA is a linear transformation. It assumes that the underlying structure of the data can be well represented by linear combinations of features. It may not perform well if the data has highly non-linear structures. (For non-linear dimensionality reduction, techniques like t-SNE, UMAP, or Kernel PCA are used).\n",
    "4.  **Sensitive to Feature Scaling:** As mentioned, PCA is highly sensitive to the scale of the original features. Features with larger variances will dominate the principal components if the data is not standardized before applying PCA. **Standardization (mean=0, std=1) is a crucial preprocessing step.**\n",
    "5.  **Variance Might Not Always Equate to Importance:** PCA prioritizes directions of high variance. However, a direction of high variance does not always mean it's the most informative for a *specific supervised learning task*. Sometimes, a lower-variance component might be crucial for separating classes or predicting a target, but PCA might discard it.\n",
    "6.  **Can Be Influenced by Outliers:** Since PCA deals with variances and covariances, outliers can significantly affect the calculation of principal components.\n",
    "\n",
    "Despite its limitations, PCA is a widely used and foundational dimensionality reduction technique due to its simplicity, effectiveness in many scenarios, and the benefits it offers in terms of computational efficiency and model performance.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
