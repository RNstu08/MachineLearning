{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcb8144-5c0b-4856-8bcb-6237958b7cae",
   "metadata": {},
   "source": [
    "let's move on to the second major category of ensemble methods: **Topic 15: Ensemble Methods - Boosting**.\n",
    "\n",
    "While Bagging and Random Forests build independent models in parallel and combine them (primarily to reduce variance), Boosting methods build models **sequentially**, where each new model attempts to correct the errors made by the previous ones. This typically leads to models that have lower bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0225f-6718-4e40-8a28-d31b75b6c73e",
   "metadata": {},
   "source": [
    "\n",
    "**1. Introduction: The Concept of Boosting**\n",
    "\n",
    "* **Core Idea:** Boosting is an ensemble technique that combines multiple \"weak learners\" (models that perform slightly better than random guessing, often simple decision trees called \"stumps\" or shallow trees) into a single \"strong learner.\"\n",
    "* **Sequential Learning:** Unlike Bagging, where models are trained independently and in parallel, Boosting trains models sequentially. Each new model in the sequence focuses on the instances that were misclassified or had large errors by the previous models.\n",
    "* **Weighted Instances/Errors:** Boosting algorithms typically assign weights to training instances. In each iteration, instances that were incorrectly predicted by the previous model are given higher weights, forcing the next model in the sequence to pay more attention to these \"hard-to-learn\" examples.\n",
    "* **Goal:** Primarily to **reduce bias** and also variance, leading to a model that fits the training data more accurately and often generalizes well.\n",
    "\n",
    "**Conceptual Diagram of Boosting (General Idea):**\n",
    "\n",
    "```\n",
    "Original Training Dataset (with initial equal weights for all samples)\n",
    "        |\n",
    "        ---------------------------------\n",
    "        |                               |\n",
    "Train Model 1 (Weak Learner 1)  --->  Evaluate Model 1, Identify Errors/Misclassifications\n",
    "        |                               |\n",
    "        ---------------------------------\n",
    "        |                               |\n",
    "Update Sample Weights                   Adjust Model 1's contribution (or learn next model based on residuals)\n",
    "(Increase weights for misclassified/   (e.g., give Model 1 a weight based on its accuracy)\n",
    " hard samples, decrease for easy ones)\n",
    "        |\n",
    "        ---------------------------------\n",
    "        |                               |\n",
    "Train Model 2 (Weak Learner 2)  --->  Evaluate Model 2, Identify its Errors\n",
    "(Focuses on previously misclassified    (on re-weighted data or residuals)\n",
    " samples or remaining errors)\n",
    "        |                               |\n",
    "        ---------------------------------\n",
    "        |                               |\n",
    "Update Sample Weights Again             Adjust Model 2's contribution\n",
    "        |\n",
    "        ... (Repeat for M iterations/models) ...\n",
    "        |\n",
    "        ---------------------------------\n",
    "        |\n",
    "Final Strong Learner: Weighted Combination of all Weak Learners\n",
    "(e.g., Weighted Majority Vote for Classification,\n",
    "       Weighted Sum for Regression)\n",
    "```\n",
    "\n",
    "**Key Characteristics of Boosting:**\n",
    "* **Iterative:** Models are added one by one.\n",
    "* **Adaptive:** Subsequent models adapt to the performance of previous models, focusing on their weaknesses.\n",
    "* **Focus on Difficult Examples:** By re-weighting samples or fitting to residuals, boosting emphasizes the examples that are harder to classify or predict correctly.\n",
    "\n",
    "---\n",
    "\n",
    "**2. AdaBoost (Adaptive Boosting)**\n",
    "\n",
    "AdaBoost is one of the earliest and most influential boosting algorithms. It's primarily used for binary classification but can be extended to multi-class problems.\n",
    "\n",
    "* **How AdaBoost Works (for Binary Classification):**\n",
    "    1.  **Initialize Weights:** Assign equal weights to all $N$ training samples: $w_i = 1/N$ for $i=1, \\dots, N$.\n",
    "    2.  **Iterate for $M$ Classifiers (Weak Learners, often Decision Stumps):**\n",
    "        For $m = 1$ to $M$:\n",
    "        a.  **Train a Weak Learner ($h_m$):** Fit a weak classifier $h_m(x)$ to the training data using the current sample weights $w_i$. The classifier is trained to minimize the weighted error rate.\n",
    "        b.  **Calculate Weighted Error Rate ($\\epsilon_m$):** Compute the weighted error rate of $h_m$:\n",
    "            $$\\epsilon_m = \\sum_{i=1}^{N} w_i \\cdot I(y_i \\neq h_m(x_i))$$\n",
    "            where $I(\\cdot)$ is the indicator function (1 if true, 0 if false).\n",
    "        c.  **Calculate Classifier Weight ($\\alpha_m$):** Determine the \"say\" or importance of this classifier in the final ensemble. Classifiers that perform better (lower error) get a higher weight:\n",
    "            $$\\alpha_m = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\right)$$\n",
    "            (Note: If a classifier is worse than random guessing, $\\epsilon_m > 0.5$, then $\\alpha_m$ becomes negative, effectively flipping its predictions. Some versions use $\\alpha_m = \\ln((1-\\epsilon_m)/\\epsilon_m)$ directly or other similar forms. The key is that $\\alpha_m$ increases as $\\epsilon_m$ decreases.)\n",
    "        d.  **Update Sample Weights:** Increase the weights of misclassified samples and decrease the weights of correctly classified samples, so the next classifier focuses more on the mistakes:\n",
    "            $$w_i \\leftarrow w_i \\cdot \\exp(\\alpha_m \\cdot I(y_i \\neq h_m(x_i)))$$\n",
    "            Then, **normalize** the weights $w_i$ so they sum to 1.\n",
    "    3.  **Final Prediction (Ensemble Output):**\n",
    "        To classify a new instance $x$, combine the predictions of all $M$ weak learners using their calculated weights $\\alpha_m$:\n",
    "        $$H(x) = \\text{sign}\\left(\\sum_{m=1}^{M} \\alpha_m h_m(x)\\right)$$\n",
    "        (Where $h_m(x)$ outputs +1 or -1 for the classes). This is a weighted majority vote.\n",
    "\n",
    "* **Base Learners:** AdaBoost often uses **decision stumps** (decision trees with only one split and two leaf nodes) as weak learners. These are very simple models.\n",
    "* **Focus:** AdaBoost focuses on misclassified examples by increasing their weights.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f70c9-1d06-485d-a230-8b50a50539ec",
   "metadata": {},
   "source": [
    "**3. Gradient Boosting Machines (GBM)**\n",
    "\n",
    "Gradient Boosting is another powerful and widely used ensemble technique that builds models sequentially. Like AdaBoost, each new model attempts to correct the errors of its predecessors. However, Gradient Boosting takes a more generalized approach by using the **gradient** of the loss function to guide the learning process.\n",
    "\n",
    "* **Core Idea:** Gradient Boosting builds an additive model in a stage-wise fashion. It fits each new weak learner to the **negative gradient** of the loss function with respect to the predictions of the current ensemble. For squared error loss (common in regression), the negative gradient is simply the **residual errors** ($actual - predicted$) of the previous model.\n",
    "* **General Framework:**\n",
    "    1.  **Initialize the Model:** Start with a simple initial model, often just the mean of the target variable (for regression) or the log-odds (for classification). Let this be $F_0(x)$.\n",
    "    2.  **Iterate for $M$ Trees (Weak Learners):**\n",
    "        For $m = 1$ to $M$:\n",
    "        a.  **Compute Pseudo-Residuals:** Calculate the \"errors\" or \"pseudo-residuals\" for each training instance based on the current ensemble's predictions $F_{m-1}(x)$. These residuals represent the part of the target variable that the current ensemble has not yet learned.\n",
    "            For squared error loss in regression: $r_{im} = y_i - F_{m-1}(x_i)$.\n",
    "            For other loss functions (like deviance for classification), the pseudo-residuals are the negative gradients of the loss function:\n",
    "            $$r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)}$$\n",
    "        b.  **Train a Weak Learner ($h_m$):** Fit a new weak learner (typically a decision tree, often a regression tree even for classification tasks) to these pseudo-residuals $r_{im}$. The goal of this tree $h_m(x)$ is to predict these residuals.\n",
    "        c.  **Find Optimal Multiplier ($\\gamma_m$) (Optional but common for some variants):** Determine the optimal multiplier (or learning rate, sometimes called shrinkage factor) for this weak learner's contribution. This step often involves a line search to minimize the overall loss function when adding $h_m(x)$.\n",
    "        d.  **Update the Ensemble Model:** Add the new weak learner (scaled by a learning rate/shrinkage factor $\\eta$) to the ensemble:\n",
    "            $$F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$$\n",
    "            (If an optimal $\\gamma_m$ was found in step c, it might be used instead of or in conjunction with $\\eta$).\n",
    "    3.  **Final Prediction:** The final prediction is $F_M(x)$.\n",
    "\n",
    "* **Loss Functions:** Gradient Boosting is a general framework that can be used with different loss functions depending on the task:\n",
    "    * **Regression:** Squared error (L2 loss), Absolute error (L1 loss), Huber loss (robust to outliers).\n",
    "    * **Classification:** Deviance (Log Loss or Binary Cross-Entropy for binary classification, multinomial deviance for multi-class), Exponential loss (similar to AdaBoost).\n",
    "* **Weak Learners:** Typically, **decision trees** (specifically regression trees, even for classification tasks where they predict class probabilities or log-odds) are used as weak learners. These trees are usually kept shallow (e.g., `max_depth` between 1 and 8) to prevent individual trees from overfitting and to keep them \"weak.\"\n",
    "* **Shrinkage (Learning Rate $\\eta$):**\n",
    "    * This is a crucial hyperparameter (often denoted as `learning_rate` in libraries). It scales the contribution of each new tree added to the ensemble.\n",
    "    * Values are typically small (e.g., 0.01, 0.05, 0.1).\n",
    "    * **Effect:** Smaller learning rates reduce the impact of each individual tree, requiring more trees (`n_estimators`) to be added to the model. This generally leads to better generalization and reduces overfitting, but at the cost of increased computation time. It's a trade-off between the number of trees and the learning rate.\n",
    "\n",
    "**Key Differences from AdaBoost:**\n",
    "\n",
    "| Feature             | AdaBoost                                                                 | Gradient Boosting                                                                 |\n",
    "| :------------------ | :----------------------------------------------------------------------- | :-------------------------------------------------------------------------------- |\n",
    "| **Error Correction** | Focuses on misclassified instances by increasing their weights.          | Fits new models to the **residual errors** (or negative gradients of the loss) of the previous ensemble. |\n",
    "| **Weak Learner Weighting** | Assigns explicit weights ($\\alpha_m$) to each weak learner's vote/prediction. | Typically, each tree contributes, scaled by a (usually fixed) learning rate (shrinkage). |\n",
    "| **Loss Function** | Traditionally associated with exponential loss (though can be generalized). | Can be used with a variety of differentiable loss functions (squared error, deviance, etc.). |\n",
    "| **Flexibility** | More specific in its formulation.                                      | More general framework, highly flexible due to choice of loss functions.            |\n",
    "\n",
    "**Conceptual Illustration of Fitting to Residuals (Regression):**\n",
    "\n",
    "1.  **Model 1 (e.g., mean):**\n",
    "    * Data: (x1, y1), (x2, y2), ...\n",
    "    * Prediction: $\\hat{y}_{\\text{model1}}$ (e.g., overall mean of y)\n",
    "    * Residuals: $r1_i = y_i - \\hat{y}_{\\text{model1},i}$\n",
    "    * *Imagine plotting these residuals. They show what Model 1 got wrong.*\n",
    "\n",
    "2.  **Model 2 (Tree 1):**\n",
    "    * Train Tree 1 to predict $r1_i$.\n",
    "    * Prediction from Tree 1: $\\hat{r}1_i$\n",
    "    * New Ensemble Prediction: $\\hat{y}_{\\text{ensemble2},i} = \\hat{y}_{\\text{model1},i} + \\eta \\cdot \\hat{r}1_i$\n",
    "    * New Residuals: $r2_i = y_i - \\hat{y}_{\\text{ensemble2},i}$\n",
    "    * *Imagine these new residuals. They should be smaller on average than $r1_i$. Tree 1 tried to correct the errors of Model 1.*\n",
    "\n",
    "3.  **Model 3 (Tree 2):**\n",
    "    * Train Tree 2 to predict $r2_i$.\n",
    "    * ...and so on.\n",
    "\n",
    "Each tree learns to correct the remaining errors of the ensemble built so far.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860716d-f4ab-4a23-b27f-dcfff5b89d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efacea3-0c9f-4970-9c0c-59c9ebd3fe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b2353-300a-449d-b22b-1d9b90787006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
