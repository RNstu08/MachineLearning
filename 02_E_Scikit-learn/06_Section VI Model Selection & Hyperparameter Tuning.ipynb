{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b74b35-9c60-46fc-93a3-b88825e05853",
   "metadata": {},
   "source": [
    "Evaluating a model on a single train-test split can be sensitive to how the split was made. Cross-validation (CV) provides a more robust estimate of model performance. Hyperparameter tuning involves finding the best settings for a model (e.g., the `C` parameter in SVC, or `n_neighbors` in KNN) that aren't learned during `fit()`. `Scikit-learn` provides excellent tools for both.\n",
    "\n",
    "## Scikit-learn: Model Selection & Hyperparameter Tuning\n",
    "\n",
    "This document covers:\n",
    "\n",
    "* **Cross-Validation (CV):** Explains the need for CV and demonstrates using `cross_val_score` (for quick scoring) and `cross_validate` (for more detailed results including timing and multiple metrics). It also shows how to use different CV splitting strategies like `KFold` and `StratifiedKFold`.\n",
    "* **Hyperparameter Tuning:** Explains the concept and demonstrates two common techniques:\n",
    "    * `GridSearchCV`: Exhaustively searches a predefined grid of parameters.\n",
    "    * `RandomizedSearchCV`: Samples a fixed number of combinations from parameter distributions or lists, often more efficient for large search spaces.\n",
    "* **Best Model:** Shows how to access the best parameters (`best_params_`) and the refitted best estimator (`best_estimator_`) found during the search.\n",
    "* **Learning & Validation Curves:** Briefly introduces these tools for diagnosing model performance issues like bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "Mastering these techniques is crucial for building robust machine learning models and selecting appropriate hyperparameters.Evaluating a model on a single train-test split can be sensitive to how the split was made. Cross-validation (CV) provides a more robust estimate of model performance. Hyperparameter tuning involves finding the best settings for a model (e.g., the `C` parameter in SVC, or `n_neighbors` in KNN) that aren't learned during `fit()`. `Scikit-learn` provides excellent tools for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f7df6-6c11-4b2e-89b5-ac890df26a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Iris Dataset ---\n",
      "Scaled Data shape: X=(150, 4), y=(150,)\n",
      "------------------------------\n",
      "--- Cross-Validation ---\n",
      "\n",
      "--- a) cross_val_score ---\n",
      "Cross-Validation Accuracy Scores (5 folds): [0.96666667 0.96666667 0.96666667 0.93333333 1.        ]\n",
      "Mean CV Accuracy: 0.9667 (+/- 0.0422)\n",
      "\n",
      "CV Accuracy Scores (KFold, 5 folds): [1.         0.96666667 0.96666667 0.93333333 0.96666667]\n",
      "Mean CV Accuracy (KFold): 0.9667\n",
      "\n",
      "CV Accuracy Scores (StratifiedKFold, 5 folds): [1.         0.96666667 0.9        1.         0.9       ]\n",
      "Mean CV Accuracy (StratifiedKFold): 0.9533\n",
      "--------------------\n",
      "\n",
      "--- b) cross_validate ---\n",
      "Cross-Validate Results (Dictionary):\n",
      "   fit_time  score_time  test_accuracy  train_accuracy  test_f1_macro  \\\n",
      "0  0.002250    0.003982       1.000000        0.966667       1.000000   \n",
      "1  0.001369    0.002678       0.966667        0.975000       0.966583   \n",
      "2  0.001482    0.002837       0.900000        0.983333       0.899749   \n",
      "3  0.001537    0.002853       1.000000        0.966667       1.000000   \n",
      "4  0.001473    0.002780       0.900000        0.975000       0.899749   \n",
      "\n",
      "   train_f1_macro  \n",
      "0        0.966646  \n",
      "1        0.974996  \n",
      "2        0.983333  \n",
      "3        0.966667  \n",
      "4        0.974996  \n",
      "\n",
      "Average Test Accuracy: 0.9533\n",
      "Average Test F1 (Macro): 0.9532\n",
      "Average Fit Time: 0.0016s\n",
      "------------------------------\n",
      "--- Hyperparameter Tuning ---\n",
      "\n",
      "--- a) GridSearchCV ---\n",
      "Starting GridSearchCV...\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, cross_validate,\n",
    "                                     KFold, StratifiedKFold, LeaveOneOut,\n",
    "                                     GridSearchCV, RandomizedSearchCV,\n",
    "                                     validation_curve, learning_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC # Example model for tuning\n",
    "from sklearn.ensemble import RandomForestClassifier # Another example\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "print(\"--- Loading Iris Dataset ---\")\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "# We'll use the *full* dataset for cross-validation and tuning examples,\n",
    "# though often you'd still hold out a final test set *after* tuning.\n",
    "# For simplicity here, we scale the whole dataset. In practice, scaling\n",
    "# should ideally happen *inside* each CV fold (using Pipelines, Section VIII).\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Scaled Data shape: X={X_scaled.shape}, y={y.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 2. Cross-Validation (CV) ---\n",
    "# Evaluate model performance more robustly by splitting data into multiple 'folds'.\n",
    "# Train on K-1 folds, test on the remaining fold, repeat K times.\n",
    "\n",
    "print(\"--- Cross-Validation ---\")\n",
    "\n",
    "# a) cross_val_score: Simple way to get scores for each fold.\n",
    "print(\"\\n--- a) cross_val_score ---\")\n",
    "svc = SVC(kernel='rbf', C=1.0, random_state=42) # Basic SVC model\n",
    "\n",
    "# cv parameter determines the splitting strategy:\n",
    "# - Integer (e.g., 5 or 10): Uses KFold (regression) or StratifiedKFold (classification) by default.\n",
    "# - CV splitter object (e.g., KFold(n_splits=5), StratifiedKFold(n_splits=5))\n",
    "# scoring: Metric to evaluate (e.g., 'accuracy', 'neg_mean_squared_error', 'r2', 'f1_macro')\n",
    "#          See sklearn.metrics.SCORERS.keys() for options.\n",
    "\n",
    "# Default CV for classifiers is StratifiedKFold\n",
    "cv_scores_acc = cross_val_score(svc, X_scaled, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy Scores (5 folds): {cv_scores_acc}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores_acc.mean():.4f} (+/- {cv_scores_acc.std() * 2:.4f})\") # Often report mean +/- 2*std dev\n",
    "\n",
    "# Using a specific CV iterator (KFold - generally not ideal for classification)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_kf = cross_val_score(svc, X_scaled, y, cv=kf, scoring='accuracy')\n",
    "print(f\"\\nCV Accuracy Scores (KFold, 5 folds): {cv_scores_kf}\")\n",
    "print(f\"Mean CV Accuracy (KFold): {cv_scores_kf.mean():.4f}\")\n",
    "\n",
    "# Using StratifiedKFold explicitly (good practice for classification)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_skf = cross_val_score(svc, X_scaled, y, cv=skf, scoring='accuracy')\n",
    "print(f\"\\nCV Accuracy Scores (StratifiedKFold, 5 folds): {cv_scores_skf}\")\n",
    "print(f\"Mean CV Accuracy (StratifiedKFold): {cv_scores_skf.mean():.4f}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# b) cross_validate: More detailed results (fit time, score time, multiple metrics).\n",
    "print(\"\\n--- b) cross_validate ---\")\n",
    "scoring_metrics = ['accuracy', 'f1_macro'] # Evaluate multiple metrics\n",
    "cv_results = cross_validate(svc, X_scaled, y, cv=skf, scoring=scoring_metrics, return_train_score=True)\n",
    "\n",
    "print(\"Cross-Validate Results (Dictionary):\")\n",
    "# Convert results dict to DataFrame for nice printing\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "print(cv_results_df)\n",
    "print(f\"\\nAverage Test Accuracy: {cv_results_df['test_accuracy'].mean():.4f}\")\n",
    "print(f\"Average Test F1 (Macro): {cv_results_df['test_f1_macro'].mean():.4f}\")\n",
    "print(f\"Average Fit Time: {cv_results_df['fit_time'].mean():.4f}s\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 3. Hyperparameter Tuning ---\n",
    "# Finding the best parameters for a model (e.g., C and kernel for SVC).\n",
    "\n",
    "print(\"--- Hyperparameter Tuning ---\")\n",
    "\n",
    "# Define the parameter grid to search\n",
    "# Example for SVC\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10, 100],             # Regularization parameter\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],     # Kernel coefficient for 'rbf'\n",
    "    'kernel': ['rbf', 'linear']         # Kernel type\n",
    "}\n",
    "\n",
    "# Example for RandomForest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],    # Number of trees\n",
    "    'max_depth': [None, 5, 10, 20],    # Max depth of trees\n",
    "    'min_samples_split': [2, 5, 10]    # Min samples required to split a node\n",
    "}\n",
    "\n",
    "# Use Stratified K-Fold for cross-validation during tuning\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# a) GridSearchCV: Exhaustive search over all parameter combinations.\n",
    "print(\"\\n--- a) GridSearchCV ---\")\n",
    "# Instantiate the model (without specific C, gamma here)\n",
    "svc_grid = SVC(random_state=42, probability=True) # probability needed if using AUC later\n",
    "\n",
    "# Set up GridSearchCV\n",
    "# estimator: The model to tune.\n",
    "# param_grid: Dictionary of parameters to try.\n",
    "# scoring: Metric to optimize (e.g., 'accuracy').\n",
    "# cv: Cross-validation strategy.\n",
    "# n_jobs: Number of CPU cores to use (-1 uses all available).\n",
    "# verbose: Controls the verbosity (amount of messages).\n",
    "grid_search = GridSearchCV(estimator=svc_grid,\n",
    "                           param_grid=param_grid_svc,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv_strategy,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit GridSearchCV to the data (this performs the search)\n",
    "print(\"Starting GridSearchCV...\")\n",
    "grid_search.fit(X_scaled, y)\n",
    "print(\"GridSearchCV finished.\")\n",
    "\n",
    "# Print the best parameters and best score found\n",
    "print(f\"\\nBest Parameters found by GridSearchCV: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score (Accuracy): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best estimator found\n",
    "best_svc = grid_search.best_estimator_\n",
    "print(f\"\\nBest Estimator: {best_svc}\")\n",
    "\n",
    "# Optionally, evaluate the best estimator on a separate test set\n",
    "# (We didn't hold one out here, but in practice you would)\n",
    "# X_train_full, X_final_test, y_train_full, y_final_test = train_test_split(...)\n",
    "# grid_search.fit(X_train_full_scaled, y_train_full)\n",
    "# best_model = grid_search.best_estimator_\n",
    "# final_accuracy = best_model.score(X_final_test_scaled, y_final_test)\n",
    "# print(f\"Accuracy on final held-out test set: {final_accuracy:.4f}\")\n",
    "\n",
    "# You can inspect all results\n",
    "# grid_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "# print(\"\\nGrid Search CV Results Summary (first 5 rows):\\n\", grid_results_df.head())\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "# b) RandomizedSearchCV: Samples a fixed number of parameter settings from specified distributions.\n",
    "# More efficient than GridSearchCV when the search space is large.\n",
    "print(\"\\n--- b) RandomizedSearchCV ---\")\n",
    "# Define parameter distributions (can mix lists and distributions)\n",
    "from scipy.stats import expon, randint\n",
    "\n",
    "param_dist_svc = {\n",
    "    'C': expon(scale=10),             # Sample from exponential distribution\n",
    "    'gamma': expon(scale=0.1),\n",
    "    'kernel': ['rbf', 'linear']         # Choose from list\n",
    "}\n",
    "\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 250), # Sample integers uniformly from 50 to 249\n",
    "    'max_depth': [None, 5, 10, 15, 20, 30],\n",
    "    'min_samples_split': randint(2, 11) # Sample integers uniformly from 2 to 10\n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "rf_rand = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "# n_iter: Number of parameter settings that are sampled. Controls the search budget.\n",
    "random_search = RandomizedSearchCV(estimator=rf_rand,\n",
    "                                   param_distributions=param_dist_rf,\n",
    "                                   n_iter=50,        # Try 50 random combinations\n",
    "                                   scoring='accuracy',\n",
    "                                   cv=cv_strategy,\n",
    "                                   n_jobs=-1,\n",
    "                                   verbose=1,\n",
    "                                   random_state=42) # For reproducibility of sampling\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "print(\"Starting RandomizedSearchCV...\")\n",
    "random_search.fit(X_scaled, y)\n",
    "print(\"RandomizedSearchCV finished.\")\n",
    "\n",
    "# Print best results\n",
    "print(f\"\\nBest Parameters found by RandomizedSearchCV: {random_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Score (Accuracy): {random_search.best_score_:.4f}\")\n",
    "best_rf = random_search.best_estimator_\n",
    "print(f\"\\nBest Estimator: {best_rf}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 4. Learning Curves & Validation Curves (Conceptual) ---\n",
    "# Tools to diagnose model performance (bias vs. variance).\n",
    "\n",
    "print(\"--- Learning & Validation Curves (Conceptual) ---\")\n",
    "# - Learning Curve (learning_curve): Shows training and validation scores as a\n",
    "#   function of the number of training samples. Helps identify if more data\n",
    "#   would help, or if the model suffers from high bias or high variance.\n",
    "# - Validation Curve (validation_curve): Shows training and validation scores\n",
    "#   as a function of a *single* hyperparameter's value. Helps understand how\n",
    "#   sensitive the model is to that parameter and find a good range.\n",
    "\n",
    "# Example usage (plotting is usually needed to interpret):\n",
    "# train_sizes, train_scores, test_scores = learning_curve(\n",
    "#     estimator=best_svc, X=X_scaled, y=y, cv=cv_strategy, n_jobs=-1,\n",
    "#     train_sizes=np.linspace(0.1, 1.0, 10), scoring='accuracy')\n",
    "#\n",
    "# param_range = np.logspace(-3, 2, 6) # Example range for SVC 'C' parameter\n",
    "# train_scores_vc, test_scores_vc = validation_curve(\n",
    "#     estimator=SVC(random_state=42, kernel='rbf'), X=X_scaled, y=y,\n",
    "#     param_name='C', param_range=param_range, cv=cv_strategy,\n",
    "#     scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"Use learning_curve and validation_curve to diagnose bias/variance\")\n",
    "print(\"and understand hyperparameter sensitivity (plotting required).\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761f4fb-0912-4145-a298-ccd0ffdbfafa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
