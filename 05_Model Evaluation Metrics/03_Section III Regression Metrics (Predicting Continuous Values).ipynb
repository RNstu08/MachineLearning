{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d1bf06-7ecd-4129-acac-e57c5f14ec0e",
   "metadata": {},
   "source": [
    "**Error-Based Metrics**\n",
    "\n",
    "These metrics focus directly on the magnitude of the errors (residuals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362392f1-f341-44d4-b461-904f5d324d4a",
   "metadata": {},
   "source": [
    "**25. Mean Absolute Error (MAE)**\n",
    "\n",
    "* **Concept:** Calculates the average of the absolute differences between the predicted values and the actual values. It tells you, on average, how far off your predictions are.\n",
    "* **Formula:**\n",
    "    $MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "    Where $n$ is the number of samples.\n",
    "* **Interpretation:**\n",
    "    * Represents the average absolute prediction error.\n",
    "    * Measured in the **same units** as the target variable (e.g., dollars, degrees Celsius).\n",
    "    * Ranges from 0 to $\\infty$. A score of 0 means perfect prediction. Lower values are better.\n",
    "    * An MAE of 5 means, on average, the predictions are off by 5 units from the true values.\n",
    "* **Pros:**\n",
    "    * **Easy to understand and interpret** due to being in the original units.\n",
    "    * **Robust to outliers:** Doesn't disproportionately penalize large errors because it doesn't square the errors. Each error contributes proportionally to its magnitude.\n",
    "* **Cons:**\n",
    "    * Doesn't penalize large errors significantly more than small ones, which might be undesirable if large errors are particularly costly.\n",
    "    * The absolute value function is not smoothly differentiable at zero, which can be a disadvantage mathematically (e.g., as a direct loss function for some gradient-based optimization methods).\n",
    "* **Example:**\n",
    "    Suppose true house prices (`y_true`) and predicted prices (`y_pred`) in $1000s are:\n",
    "    `y_true = [200, 350, 150, 500, 275]`\n",
    "    `y_pred = [210, 330, 165, 480, 280]`\n",
    "\n",
    "    Errors ($y_i - \\hat{y}_i$): `[-10, 20, -15, 20, -5]`\n",
    "    Absolute Errors ($|y_i - \\hat{y}_i|$): `[10, 20, 15, 20, 5]`\n",
    "    $MAE = \\frac{10 + 20 + 15 + 20 + 5}{5} = \\frac{70}{5} = 14$\n",
    "    The MAE is $14k (or $14,000). On average, the price prediction is off by $14,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0154ecbc-78df-4aa4-84c6-30bdf7cfd1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 14.0\n"
     ]
    }
   ],
   "source": [
    "# Implementation (Scikit-learn):**\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([200, 350, 150, 500, 275])\n",
    "y_pred = np.array([210, 330, 165, 480, 280])\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38903f-747b-43e9-a07c-13a67f47ee04",
   "metadata": {},
   "source": [
    "* **Context:** A good choice when you need a metric that is easily interpretable in the original units and when you don't want outliers to dominate the error measure. Useful for reporting prediction accuracy to stakeholders.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0f49cb-dbca-4229-860b-29e5c3fe9b19",
   "metadata": {},
   "source": [
    "**26. Mean Squared Error (MSE)**\n",
    "\n",
    "* **Concept:** Calculates the average of the *squared* differences between the predicted values and the actual values.\n",
    "* **Formula:**\n",
    "    $MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "* **Interpretation:**\n",
    "    * Represents the average squared prediction error.\n",
    "    * Measured in the **square of the units** of the target variable (e.g., dollars squared, degrees Celsius squared). This makes direct interpretation difficult.\n",
    "    * Ranges from 0 to $\\infty$. A score of 0 means perfect prediction. Lower values are better.\n",
    "    * Penalizes large errors much more heavily than small errors due to the squaring. An error of 10 contributes 100 to the sum, while an error of 2 contributes only 4.\n",
    "* **Pros:**\n",
    "    * **Penalizes large errors significantly,** which is often desirable.\n",
    "    * **Mathematically convenient:** The squared term makes it smoothly differentiable, which is useful for optimization algorithms (it's the standard loss function for linear regression).\n",
    "* **Cons:**\n",
    "    * **Highly sensitive to outliers:** A single large error can inflate the MSE substantially.\n",
    "    * **Units are squared,** making it hard to interpret the value directly in the context of the problem (e.g., an MSE of 250 dollars-squared doesn't have an intuitive meaning).\n",
    "* **Example:**\n",
    "    Using the same house price data:\n",
    "    `y_true = [200, 350, 150, 500, 275]`\n",
    "    `y_pred = [210, 330, 165, 480, 280]`\n",
    "    Errors: `[-10, 20, -15, 20, -5]`\n",
    "    Squared Errors ($(y_i - \\hat{y}_i)^2$): `[100, 400, 225, 400, 25]`\n",
    "    $MSE = \\frac{100 + 400 + 225 + 400 + 25}{5} = \\frac{1150}{5} = 230$\n",
    "    The MSE is 230 (in units of thousands-of-dollars squared).\n",
    "\n",
    "    *Outlier Impact:* Let's say the last prediction was way off: `y_pred = [210, 330, 165, 480, 575]`. True value was 275.\n",
    "    New Errors: `[-10, 20, -15, 20, -300]`\n",
    "    New Abs Errors: `[10, 20, 15, 20, 300]` -> New MAE = (10+20+15+20+300)/5 = 365/5 = 73 (Increased significantly, but linearly)\n",
    "    New Squared Errors: `[100, 400, 225, 400, 90000]` -> New MSE = (100+400+225+400+90000)/5 = 91125/5 = 18225 (Exploded due to the outlier!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5194b9-7246-4400-b951-08eae155a94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 230.0\n",
      "MSE with Outlier: 18225.0\n"
     ]
    }
   ],
   "source": [
    "# **Implementation (Scikit-learn):**\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_true = np.array([200, 350, 150, 500, 275])\n",
    "y_pred = np.array([210, 330, 165, 480, 280])\n",
    "y_pred_outlier = np.array([210, 330, 165, 480, 575]) # With outlier\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "mse_outlier = mean_squared_error(y_true, y_pred_outlier)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "# Output: Mean Squared Error (MSE): 230.0\n",
    "print(f\"MSE with Outlier: {mse_outlier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5138d-335b-421e-a5d5-73f126261fe5",
   "metadata": {},
   "source": [
    "* **Context:** Commonly used as a loss function for training models. Useful as an evaluation metric when large errors should be penalized heavily. Be cautious about its sensitivity to outliers and the non-intuitive units.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a67f1-7bd6-4f72-b956-2fb05c294e2d",
   "metadata": {},
   "source": [
    "**27. Root Mean Squared Error (RMSE)**\n",
    "\n",
    "* **Concept:** The square root of the Mean Squared Error (MSE). This effectively brings the units back to the original scale of the target variable.\n",
    "* **Formula:**\n",
    "    $RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$\n",
    "* **Interpretation:**\n",
    "    * Represents the **standard deviation of the residuals** (prediction errors). It measures the typical magnitude of the error.\n",
    "    * Measured in the **same units** as the target variable (like MAE).\n",
    "    * Ranges from 0 to $\\infty$. A score of 0 means perfect prediction. Lower values are better.\n",
    "    * An RMSE of 15 means the typical deviation of the prediction from the true value is about 15 units.\n",
    "* **Pros:**\n",
    "    * **Interpretable units:** Same units as the target variable, easier to understand than MSE.\n",
    "    * **Penalizes large errors:** Retains the property of MSE where large errors have a disproportionately large impact (though dampened by the square root).\n",
    "    * Very commonly used and reported metric.\n",
    "* **Cons:**\n",
    "    * **Sensitive to outliers:** Like MSE, it can be significantly affected by outliers (though the impact is somewhat reduced compared to MSE due to the square root).\n",
    "    * Mathematically slightly more complex than MAE.\n",
    "* **Example:**\n",
    "    Using the MSE values from the previous example:\n",
    "    * Original data: $MSE = 230$.\n",
    "        $RMSE = \\sqrt{230} \\approx 15.17$\n",
    "        The RMSE is $15.17k (or $15,170).\n",
    "    * Data with outlier: $MSE = 18225$.\n",
    "        $RMSE = \\sqrt{18225} = 135$\n",
    "        The RMSE is $135k. Compare this to the MAE of 73k for the outlier case. RMSE is larger, reflecting the stronger penalty for the large error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97bb922a-c73a-49cd-9d84-be1275d626a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE - Method 1): 15.17\n"
     ]
    }
   ],
   "source": [
    "#* **Implementation (Scikit-learn):**\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np # Needed for np.sqrt\n",
    "\n",
    "y_true = np.array([200, 350, 150, 500, 275])\n",
    "y_pred = np.array([210, 330, 165, 480, 280])\n",
    "y_pred_outlier = np.array([210, 330, 165, 480, 575]) # With outlier\n",
    "\n",
    "# Method 1: Calculate MSE then take sqrt\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE - Method 1): {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d45a22-cf7c-46d7-8356-e435ae901e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with Outlier (Method 1): 135.00\n"
     ]
    }
   ],
   "source": [
    "mse_outlier = mean_squared_error(y_true, y_pred_outlier)\n",
    "rmse_outlier = np.sqrt(mse_outlier)\n",
    "print(f\"RMSE with Outlier (Method 1): {rmse_outlier:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5721854-8233-4825-b0a6-1d712fb5bcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in g:\\learning\\machine_learning\\pandas\\venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in g:\\learning\\machine_learning\\pandas\\venv\\lib\\site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in g:\\learning\\machine_learning\\pandas\\venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in g:\\learning\\machine_learning\\pandas\\venv\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in g:\\learning\\machine_learning\\pandas\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1030467f-ad4d-49dd-957a-869e9a4923e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: Could not use squared=False parameter. Error: got an unexpected keyword argument 'squared'\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Use squared=False argument (newer sklearn versions 0.24+)\n",
    "try:\n",
    "    rmse_direct = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    rmse_outlier_direct = mean_squared_error(y_true, y_pred_outlier, squared=False)\n",
    "    print(f\"RMSE (Method 2 - requires sklearn 0.24+): {rmse_direct:.2f}\")\n",
    "    print(f\"RMSE with Outlier (Method 2 - requires sklearn 0.24+): {rmse_outlier_direct:.2f}\")\n",
    "except TypeError as e:\n",
    "    print(f\"\\nNote: Could not use squared=False parameter. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e7701-b233-4a42-8f06-6a32efcd2674",
   "metadata": {},
   "source": [
    "* **Context:** Perhaps the most frequently used regression metric. It offers a good balance between interpretability (original units) and sensitivity to large errors. It's often the default metric reported for regression tasks, but always consider the potential impact of outliers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb585b76-0375-42b0-8d1a-3d93012b7660",
   "metadata": {},
   "source": [
    "**28. Median Absolute Error (MedAE)**\n",
    "\n",
    "* **Concept:** Calculates the median of all the absolute differences between the predicted values and the actual values.\n",
    "* **Formula:**\n",
    "    $MedAE = \\text{median}(|y_1 - \\hat{y}_1|, |y_2 - \\hat{y}_2|, ..., |y_n - \\hat{y}_n|)$\n",
    "* **Interpretation:**\n",
    "    * Represents the median absolute prediction error. Tells you the error magnitude for the \"middle\" data point if you were to sort all absolute errors.\n",
    "    * Measured in the **same units** as the target variable.\n",
    "    * Ranges from 0 to $\\infty$. Lower is better.\n",
    "* **Pros:**\n",
    "    * **Highly robust to outliers:** The median is not affected by extreme values, making this metric excellent when outliers are present and shouldn't influence the overall error assessment.\n",
    "    * Easy to interpret units.\n",
    "* **Cons:**\n",
    "    * Ignores the magnitude and distribution of errors beyond the median point. A model could have very large errors for half the data, but MedAE would only reflect the error of the middle value.\n",
    "    * Less common than MAE or RMSE.\n",
    "* **Example:**\n",
    "    Using the house price data:\n",
    "    * Original data: Absolute Errors: `[10, 20, 15, 20, 5]`. Sorted: `[5, 10, 15, 20, 20]`.\n",
    "        $MedAE = 15$ (the middle value). The median error is $15k.\n",
    "    * Data with outlier: Absolute Errors: `[10, 20, 15, 20, 300]`. Sorted: `[10, 15, 20, 20, 300]`.\n",
    "        $MedAE = 20$ (the middle value). The median error is $20k. Notice how the huge outlier (300) had very little impact on MedAE (it only shifted from 15 to 20), unlike its drastic effect on MAE and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc6f0a71-e2ba-4c40-906d-1b681b437b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error (MedAE): 15.0\n",
      "MedAE with Outlier: 20.0\n"
     ]
    }
   ],
   "source": [
    "#**Implementation (Scikit-learn):**\n",
    "\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "y_true = np.array([200, 350, 150, 500, 275])\n",
    "y_pred = np.array([210, 330, 165, 480, 280])\n",
    "y_pred_outlier = np.array([210, 330, 165, 480, 575]) # With outlier\n",
    "\n",
    "medae = median_absolute_error(y_true, y_pred)\n",
    "medae_outlier = median_absolute_error(y_true, y_pred_outlier)\n",
    "\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "\n",
    "print(f\"MedAE with Outlier: {medae_outlier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1bd63-f375-4fcd-b124-7ad725ddeb9c",
   "metadata": {},
   "source": [
    "* **Context:** Use when you need a measure of central tendency for the error that is insensitive to outliers. Good for understanding the typical error magnitude in skewed or outlier-prone datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4aae8d-fbb5-457a-b10e-c9819a9d117a",
   "metadata": {},
   "source": [
    "**29. Max Error**\n",
    "\n",
    "* **Concept:** Identifies the single largest absolute difference between any predicted value and its corresponding actual value across the entire dataset.\n",
    "* **Formula:**\n",
    "    $MaxError = \\max_{i} (|y_i - \\hat{y}_i|)$\n",
    "* **Interpretation:**\n",
    "    * Represents the **worst-case scenario** error for any single prediction.\n",
    "    * Measured in the **same units** as the target variable.\n",
    "    * Ranges from 0 to $\\infty$. Lower is better.\n",
    "* **Pros:**\n",
    "    * Directly captures the magnitude of the largest prediction error.\n",
    "    * Useful for understanding the upper bound of the model's errors.\n",
    "* **Cons:**\n",
    "    * **Extremely sensitive to outliers:** Determined by just one data point.\n",
    "    * Provides no information about the typical error or the distribution of errors.\n",
    "* **Example:**\n",
    "    Using the house price data:\n",
    "    * Original data: Absolute Errors: `[10, 20, 15, 20, 5]`.\n",
    "        $MaxError = 20$. The worst prediction was off by $20k.\n",
    "    * Data with outlier: Absolute Errors: `[10, 20, 15, 20, 300]`.\n",
    "        $MaxError = 300$. The worst prediction was off by $300k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a77491e-3783-4723-a101-737e6bdf85e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Error: 20\n",
      "Max Error with Outlier: 300\n"
     ]
    }
   ],
   "source": [
    "#* **Implementation (Scikit-learn):**\n",
    "from sklearn.metrics import max_error\n",
    "\n",
    "y_true = np.array([200, 350, 150, 500, 275])\n",
    "y_pred = np.array([210, 330, 165, 480, 280])\n",
    "y_pred_outlier = np.array([210, 330, 165, 480, 575]) # With outlier\n",
    "\n",
    "max_err = max_error(y_true, y_pred)\n",
    "max_err_outlier = max_error(y_true, y_pred_outlier)\n",
    "\n",
    "print(f\"Max Error: {max_err}\")\n",
    "\n",
    "print(f\"Max Error with Outlier: {max_err_outlier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff0717-bfc2-4d25-92cb-eb7da0daf48c",
   "metadata": {},
   "source": [
    "* **Context:** Relevant in applications where the maximum possible error is critical, such as in engineering safety tolerances, financial predictions needing guarantees, or any domain where large individual errors are unacceptable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079cb9ee-174a-4c64-ad93-374f4d334c2d",
   "metadata": {},
   "source": [
    "**B. Relative Performance Metrics**\n",
    "\n",
    "These metrics evaluate the model's performance relative to the variability inherent in the data itself.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906c45a-2a1f-432d-b725-a8b4073e5301",
   "metadata": {},
   "source": [
    "**30. R-squared (R²) - Coefficient of Determination**\n",
    "\n",
    "* **Concept:** Measures the proportion of the total variance in the target variable ($y$) that is explained by the model's predictions ($\\hat{y}$). It compares the model's errors ($SS_{res}$) to the variance of the target variable around its mean ($SS_{tot}$).\n",
    "* **Formula:**\n",
    "    $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n",
    "    Where:\n",
    "    * $SS_{res} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ is the Sum of Squared Residuals (model errors).\n",
    "    * $SS_{tot} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$ is the Total Sum of Squares (proportional to the variance of $y$).\n",
    "    * $\\bar{y}$ is the mean of the true values $y_i$.\n",
    "* **Interpretation:**\n",
    "    * Ranges theoretically from $-\\infty$ to 1. Practically often seen between 0 and 1 on training data.\n",
    "    * $R^2 = 1$: The model perfectly explains all the variance in the target variable. ($SS_{res}=0$).\n",
    "    * $R^2 = 0$: The model explains none of the variance; it performs no better than simply predicting the mean $\\bar{y}$ for all instances. ($SS_{res}=SS_{tot}$).\n",
    "    * $R^2 < 0$: The model performs *worse* than predicting the mean. This can happen on test data or with cross-validation if the model fits the training data poorly or makes systematically worse predictions than the mean.\n",
    "    * Often expressed as a percentage: $R^2 = 0.75$ means \"the model explains 75% of the variability in the target variable\".\n",
    "* **Pros:**\n",
    "    * Provides a **relative measure of fit** (unitless).\n",
    "    * Gives an intuitive percentage interpretation of how much variance the model accounts for.\n",
    "    * Very common in statistical modeling, especially linear regression.\n",
    "* **Cons:**\n",
    "    * **R² always increases or stays the same** when more features (predictors) are added to the model, even if they are irrelevant. This makes it unsuitable for comparing models with different numbers of features, as it encourages overfitting.\n",
    "    * A high R² doesn't necessarily mean the model makes accurate predictions in an absolute sense (MAE/RMSE could still be high).\n",
    "    * Doesn't indicate if the model is biased or if the relationship is truly linear (if assuming linear regression).\n",
    "* **Example:**\n",
    "    Using the original house price data:\n",
    "    `y_true = [200, 350, 150, 500, 275]` -> Mean $\\bar{y} = (200+350+150+500+275)/5 = 1475/5 = 295$.\n",
    "    `y_pred = [210, 330, 165, 480, 280]`\n",
    "    $SS_{res}$: We know $MSE = 230$, and $MSE = SS_{res}/n$, so $SS_{res} = MSE \\times n = 230 \\times 5 = 1150$.\n",
    "    $SS_{tot} = (200-295)^2 + (350-295)^2 + (150-295)^2 + (500-295)^2 + (275-295)^2$\n",
    "    $SS_{tot} = (-95)^2 + (55)^2 + (-145)^2 + (205)^2 + (-20)^2$\n",
    "    $SS_{tot} = 9025 + 3025 + 21025 + 42025 + 400 = 75500$.\n",
    "    $R^2 = 1 - \\frac{1150}{75500} = 1 - 0.01523... \\approx 0.9848$\n",
    "    The model explains about 98.5% of the variance in house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fb3401f-1c05-4dce-b57f-f05a3b2285ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2): 0.9848\n"
     ]
    }
   ],
   "source": [
    "# **Implementation (Scikit-learn):**\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = np.array([200, 350, 150, 500, 275])\n",
    "y_pred = np.array([210, 330, 165, 480, 280])\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R-squared (R2): {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81541256-87aa-421d-9e4e-bd459a9d4faa",
   "metadata": {},
   "source": [
    "* **Context:** A standard measure of goodness-of-fit, particularly in linear regression contexts. Useful for understanding how much of the data's variability is captured by the model, but should be used cautiously for model comparison, especially if models differ in complexity (number of features). Always complement with absolute error metrics (MAE, RMSE).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff311c-a6d2-47c4-8943-ac84ebb561d4",
   "metadata": {},
   "source": [
    "**31. Adjusted R-squared**\n",
    "\n",
    "* **Concept:** A modified version of R² that penalizes the score for including extra predictors (features) that do not significantly improve the model's fit. It adjusts R² based on the number of data points ($n$) and the number of features ($p$).\n",
    "* **Formula:**\n",
    "    $Adjusted \\ R^2 = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1}$\n",
    "    Where:\n",
    "    * $R^2$ is the standard R-squared value.\n",
    "    * $n$ is the number of samples (data points).\n",
    "    * $p$ is the number of predictors (features) in the model.\n",
    "* **Interpretation:**\n",
    "    * Similar interpretation to R², but the value will only increase if adding a new feature improves $R^2$ enough to compensate for the penalty of adding a feature.\n",
    "    * Adjusted $R^2$ is always less than or equal to $R^2$.\n",
    "    * Can be negative.\n",
    "    * More suitable for comparing models with different numbers of features. A higher adjusted R² suggests a better model considering complexity.\n",
    "* **Pros:**\n",
    "    * **Accounts for model complexity:** Penalizes the addition of non-informative features.\n",
    "    * **Better for model comparison:** More reliable than R² when comparing models with different numbers of predictors.\n",
    "* **Cons:**\n",
    "    * Interpretation is slightly less direct than the R² percentage.\n",
    "    * Still doesn't indicate absolute prediction accuracy or model bias.\n",
    "    * Requires knowing the number of features ($p$), which might not always be straightforward (e.g., after complex feature engineering).\n",
    "* **Example:**\n",
    "    Using the previous R² = 0.9848. Let $n=5$. Assume our model used $p=2$ features.\n",
    "    $Adjusted \\ R^2 = 1 - (1 - 0.9848) \\frac{5 - 1}{5 - 2 - 1} = 1 - (0.0152) \\frac{4}{2}$\n",
    "    $Adjusted \\ R^2 = 1 - (0.0152 \\times 2) = 1 - 0.0304 = 0.9696$\n",
    "\n",
    "    Now, suppose we added another useless feature ($p=3$) and $R^2$ only slightly increased to 0.9850.\n",
    "    $Adjusted \\ R^2_{new} = 1 - (1 - 0.9850) \\frac{5 - 1}{5 - 3 - 1} = 1 - (0.0150) \\frac{4}{1}$\n",
    "    $Adjusted \\ R^2_{new} = 1 - 0.0600 = 0.9400$\n",
    "    Even though $R^2$ slightly increased, Adjusted $R^2$ decreased, correctly indicating that adding the third feature wasn't worthwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66d81d0e-3f27-4518-b152-c9a1ee0f917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9848\n",
      "Adjusted R-squared (n=5, p=2): 0.9695\n"
     ]
    }
   ],
   "source": [
    "#*Implementation (Scikit-learn):** Not a direct function. Calculate manually.\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = np.array([200, 350, 150, 500, 275])\n",
    "y_pred = np.array([210, 330, 165, 480, 280])\n",
    "n = len(y_true) # Number of samples\n",
    "p = 2 # Assume 2 features were used for this model\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Calculate Adjusted R-squared manually\n",
    "if n - p - 1 != 0: # Avoid division by zero\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "else:\n",
    "    adj_r2 = np.nan # Or handle as appropriate\n",
    "\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "print(f\"Adjusted R-squared (n={n}, p={p}): {adj_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613efeb-38da-41f0-8a46-d987885610a6",
   "metadata": {},
   "source": [
    "* **Context:** Use Adjusted R² instead of R² when comparing models with different numbers of features or during feature selection processes. It provides a more honest assessment of model fit by penalizing unnecessary complexity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a6df4-bffd-41d1-9a28-2806e95dd6a0",
   "metadata": {},
   "source": [
    "**C. Percentage Error Metrics**\n",
    "\n",
    "These metrics express the error relative to the magnitude of the true value, often resulting in a percentage.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335794b-6fc3-4944-94a3-0f9615eb7466",
   "metadata": {},
   "source": [
    "**32. Mean Absolute Percentage Error (MAPE)**\n",
    "\n",
    "* **Concept:** Calculates the average of the absolute errors taken as a percentage of the actual values.\n",
    "* **Formula:**\n",
    "    $MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\\%$\n",
    "* **Interpretation:**\n",
    "    * Represents the average percentage deviation of the predictions from the actual values.\n",
    "    * Unitless (a percentage). Lower is better. Ranges from 0% to $\\infty$.\n",
    "    * A MAPE of 10% suggests the model's predictions are, on average, within 10% of the true values.\n",
    "* **Pros:**\n",
    "    * **Intuitive percentage interpretation,** making it easy to communicate.\n",
    "    * Scale-independent, allowing comparison across datasets or variables with different scales.\n",
    "* **Cons:**\n",
    "    * **Undefined if any true value $y_i$ is zero.** Can explode if $y_i$ is close to zero.\n",
    "    * **Asymmetric:** It penalizes under-predictions ($\\hat{y}_i < y_i$) less heavily than over-predictions ($\\hat{y}_i > y_i$) of the same absolute magnitude, relative to the true value. For example, if True=10, Pred=5 (error -5), |error/true| = 50%. If True=5, Pred=10 (error +5), |error/true| = 100%.\n",
    "    * Assumes percentage errors are meaningful (e.g., a 10% error on 1,000,000 is much larger in absolute terms than a 10% error on 10).\n",
    "* **Example:**\n",
    "    Using the original house price data (ensure no zeros):\n",
    "    `y_true = [200, 350, 150, 500, 275]`\n",
    "    `y_pred = [210, 330, 165, 480, 280]`\n",
    "    Errors: `[-10, 20, -15, 20, -5]`\n",
    "    Percentage Errors ($ (y_i - \\hat{y}_i) / y_i $):\n",
    "    `[-10/200, 20/350, -15/150, 20/500, -5/275]`\n",
    "    `[-0.05, 0.057, -0.10, 0.04, -0.018]`\n",
    "    Absolute Percentage Errors: `[0.05, 0.057, 0.10, 0.04, 0.018]`\n",
    "    $MAPE = \\frac{0.05 + 0.057 + 0.10 + 0.04 + 0.018}{5} \\times 100\\%$\n",
    "    $MAPE = \\frac{0.265}{5} \\times 100\\% = 0.053 \\times 100\\% = 5.3\\%$\n",
    "    On average, the predictions are about 5.3% off the actual price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07da552f-a9f1-4cf6-a90f-52237dd1cb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error (MAPE): 5.31%\n"
     ]
    }
   ],
   "source": [
    "# **Implementation (Scikit-learn):**\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "y_true = np.array([200, 350, 150, 500, 275])\n",
    "y_pred = np.array([210, 330, 165, 480, 280])\n",
    "\n",
    "# Ensure no zeros in y_true if calculating manually or using libraries that might not handle it\n",
    "if np.any(y_true == 0):\n",
    "    print(\"Warning: y_true contains zeros, MAPE is undefined or problematic.\")\n",
    "    mape = np.nan\n",
    "else:\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "# Output from sklearn is a proportion, multiply by 100 for percentage\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape * 100:.2f}%\")\n",
    "\n",
    "# Note: Slight diff from manual due to precision in intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c32fa-b1bb-418b-a317-a5241463b6e5",
   "metadata": {},
   "source": [
    "* **Context:** Often used in business forecasting (e.g., sales, demand) because of its intuitive percentage interpretation. However, be extremely careful if your target variable can be zero or close to zero, and be aware of its asymmetric penalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad5c24-9c1c-43bf-8d0f-012205d4f4e9",
   "metadata": {},
   "source": [
    "**33. Symmetric Mean Absolute Percentage Error (sMAPE)**\n",
    "\n",
    "* **Concept:** An alternative to MAPE that attempts to correct its asymmetry and division-by-zero issues by normalizing the absolute error by the *average of the absolute values* of the actual and predicted figures.\n",
    "* **Formula:** (Using the definition common in recent forecasting competitions and aligned with how one might implement based on descriptions, although *not* directly in sklearn.metrics as `sMAPE`):\n",
    "    $sMAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{2 \\times |y_i - \\hat{y}_i|}{|y_i| + |\\hat{y}_i|} \\times 100\\%$\n",
    "    *(Note: Different formulas exist. This one ensures the result is between 0% and 200%).*\n",
    "* **Interpretation:**\n",
    "    * Represents a percentage error, adjusted for symmetry.\n",
    "    * Ranges from 0% to 200%. Lower is better.\n",
    "    * The interpretation is less direct than MAPE (it's roughly the absolute error as a percentage of the average magnitude of the true and predicted values).\n",
    "* **Pros:**\n",
    "    * **More symmetric** in penalizing over- and under-predictions compared to MAPE.\n",
    "    * **Avoids division by zero** unless *both* $y_i$ and $\\hat{y}_i$ are zero (in which case the term is typically defined as 0).\n",
    "    * Bounded range [0%, 200%].\n",
    "* **Cons:**\n",
    "    * **Less intuitive interpretation** than MAPE.\n",
    "    * Can produce **strange results** if one value is zero and the other is non-zero (the term becomes $\\frac{2|y_i|}{ |y_i|} = 2$, resulting in a 200% error for that point, which might be unexpected).\n",
    "    * Not as widely used or standardized as MAPE or RMSE.\n",
    "    * **Not directly available in `sklearn.metrics`** (as of common versions, always check documentation for updates).\n",
    "* **Example:**\n",
    "    Using the original house price data:\n",
    "    `y_true = [200, 350, 150, 500, 275]`\n",
    "    `y_pred = [210, 330, 165, 480, 280]`\n",
    "    Absolute Errors $|y_i - \\hat{y}_i|$: `[10, 20, 15, 20, 5]`\n",
    "    Sum of Abs Values $|y_i| + |\\hat{y}_i|$: `[410, 680, 315, 980, 555]`\n",
    "    Term $2 \\times |err| / (|y| + |\\hat{y}|)$:\n",
    "    `[2*10/410, 2*20/680, 2*15/315, 2*20/980, 2*5/555]`\n",
    "    `[0.0488, 0.0588, 0.0952, 0.0408, 0.0180]`\n",
    "    $sMAPE = \\frac{0.0488 + 0.0588 + 0.0952 + 0.0408 + 0.0180}{5} \\times 100\\%$\n",
    "    $sMAPE = \\frac{0.2616}{5} \\times 100\\% = 0.0523 \\times 100\\% = 5.23\\%$\n",
    "    (In this case, very similar to MAPE because predictions are close to actuals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27e402cc-74e3-4b78-8062-399038583ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sMAPE (Manual Calculation): 5.23%\n"
     ]
    }
   ],
   "source": [
    "# **Implementation (Manual):**\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "#\"\"\" Calculates Symmetric Mean Absolute Percentage Error (sMAPE) \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    numerator = np.abs(y_true - y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2 # Or use sum directly in formula\n",
    "# Handle case where both are zero (should be 0 error)\n",
    "    ratio = np.divide(numerator, denominator, out=np.zeros_like(numerator, dtype=float), where=denominator!=0)\n",
    "# Alternative formula structure as used in manual example\n",
    "# ratio = np.divide(2 * numerator, np.abs(y_true) + np.abs(y_pred), out=np.zeros_like(numerator, dtype=float), where=(np.abs(y_true) + np.abs(y_pred))!=0)\n",
    "    return np.mean(ratio) * 100\n",
    "\n",
    "y_true = np.array([200, 350, 150, 500, 275])\n",
    "y_pred = np.array([210, 330, 165, 480, 280])\n",
    "\n",
    "# Using the formula: mean( 2 * |y-yhat| / (|y| + |yhat|) ) * 100\n",
    "smape_val = np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100\n",
    "print(f\"sMAPE (Manual Calculation): {smape_val:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8608e0-16fc-41fe-a45f-b6647a7daa1a",
   "metadata": {},
   "source": [
    "* **Context:** Consider using sMAPE as an alternative to MAPE if dealing with potential zeros or near-zeros in your data, or if the asymmetry of MAPE is a significant concern. Be aware of its own definition variations and interpretation nuances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced0a323-69e8-41b1-b73b-2edd74dcd2f3",
   "metadata": {},
   "source": [
    "In summary for Regression Metrics:\n",
    "\n",
    "* Use **MAE** or **MedAE** for interpretable error in original units, especially if robustness to outliers is needed (MedAE being more robust).\n",
    "* Use **RMSE** if you want interpretable units but also want to penalize large errors more heavily (most common choice).\n",
    "* Use **MSE** primarily as a loss function during training or if the squared penalty is specifically desired for evaluation (less common for reporting due to units).\n",
    "* Use **Max Error** if the worst-case prediction error is critical.\n",
    "* Use **R²** for a quick relative measure of variance explained, but be wary of its increase with model complexity.\n",
    "* Use **Adjusted R²** when comparing models with different numbers of features.\n",
    "* Use **MAPE** for intuitive percentage errors in forecasting, but *only* if true values are reliably non-zero and its asymmetry is acceptable.\n",
    "* Consider **sMAPE** if MAPE's issues are problematic, but understand its own limitations.\n",
    "\n",
    "As with classification, relying on a single metric can be misleading. It's often best to evaluate regression models using a combination of metrics (e.g., RMSE and R², or MAE and R²) to get a more complete picture of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28739bd1-b0f1-4951-b5c0-dd1e0d1ea84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515913c5-f538-4eeb-9449-6a2971180e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dcde1-ae91-41b7-87b7-cfaa2f8ac4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
