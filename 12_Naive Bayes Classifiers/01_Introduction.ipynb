{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282eec13-f9cb-442a-aa08-2e474714bbf1",
   "metadata": {},
   "source": [
    "let's proceed with **Topic 12: Naive Bayes Classifiers**. These are a family of simple, yet often surprisingly effective, probabilistic classifiers based on Bayes' Theorem with a \"naive\" assumption of feature independence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11afa805-09d5-44fb-bc22-66a6c59d27b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**1. Introduction: What are Naive Bayes Classifiers?**\n",
    "\n",
    "* **Probabilistic Classifiers:** Naive Bayes classifiers calculate the probability of an instance belonging to each class and then predict the class with the highest probability.\n",
    "* **Based on Bayes' Theorem:** The core of the algorithm lies in applying Bayes' theorem.\n",
    "* **\"Naive\" Assumption:** The defining characteristic is the \"naive\" assumption that all features of an instance are **conditionally independent** of each other, given the class. This means the presence or value of one feature does not affect the presence or value of another feature *within the context of a specific class*. This assumption simplifies the calculations significantly.\n",
    "* **Efficiency:** Due to this simplification, Naive Bayes classifiers are very fast to train and predict, making them suitable for large datasets and high-dimensional problems like text classification.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Foundational Concept: Bayes' Theorem**\n",
    "\n",
    "Bayes' Theorem describes how to update the probability of a hypothesis based on new evidence. It's stated as:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "Where:\n",
    "* $P(A|B)$: **Posterior probability** – The probability of hypothesis $A$ being true, given that evidence $B$ has occurred.\n",
    "* $P(B|A)$: **Likelihood** – The probability of observing evidence $B$, given that hypothesis $A$ is true.\n",
    "* $P(A)$: **Prior probability** – The initial probability of hypothesis $A$ being true, before observing evidence $B$.\n",
    "* $P(B)$: **Marginal probability of evidence** – The total probability of observing evidence $B$. This acts as a normalizing constant.\n",
    "\n",
    "**Applying Bayes' Theorem to Classification:**\n",
    "\n",
    "Let $C_k$ be a particular class and $X = (x_1, x_2, \\dots, x_p)$ be a set of features for an instance. We want to find the probability of the instance belonging to class $C_k$ given its features $X$:\n",
    "\n",
    "$$P(C_k | X) = \\frac{P(X | C_k) \\cdot P(C_k)}{P(X)}$$\n",
    "\n",
    "Our goal is to choose the class $C_k$ that maximizes this posterior probability $P(C_k | X)$. Since $P(X)$ (the probability of observing the features) is the same for all classes when considering a single instance, we can ignore it for the purpose of finding the *most probable* class. Thus, we want to maximize:\n",
    "\n",
    "$$\\text{argmax}_{C_k} \\left( P(X | C_k) \\cdot P(C_k) \\right)$$\n",
    "\n",
    "* $P(C_k)$: **Prior probability of class $C_k$**. This is typically estimated from the training data as the proportion of training instances belonging to class $C_k$.\n",
    "* $P(X | C_k)$: **Likelihood of observing features $X$ given that the instance belongs to class $C_k$**. This is where the \"naive\" assumption comes in.\n",
    "\n",
    "---\n",
    "\n",
    "**3. The \"Naive\" Assumption of Feature Independence**\n",
    "\n",
    "Calculating the joint probability $P(X | C_k) = P(x_1, x_2, \\dots, x_p | C_k)$ directly is very difficult because it would require an enormous amount of data to estimate the probability for every possible combination of feature values.\n",
    "\n",
    "The **naive assumption** simplifies this by assuming that all features $x_1, x_2, \\dots, x_p$ are **conditionally independent** given the class $C_k$. This means:\n",
    "\n",
    "$$P(X | C_k) = P(x_1 | C_k) \\cdot P(x_2 | C_k) \\cdot \\dots \\cdot P(x_p | C_k) = \\prod_{i=1}^{p} P(x_i | C_k)$$\n",
    "\n",
    "* **Why \"Naive\"?** In most real-world scenarios, features are rarely perfectly independent. For example, in text classification, the presence of the word \"machine\" might make the presence of the word \"learning\" more likely.\n",
    "* **Practical Performance:** Despite this often unrealistic assumption, Naive Bayes classifiers frequently perform very well, especially in domains like text classification. The independence assumption doesn't need to hold perfectly for the classifier to make correct decisions; as long as the dependencies don't overwhelmingly favor incorrect classes, it can still work.\n",
    "\n",
    "With the naive assumption, the classification rule becomes:\n",
    "Choose class $C_k$ that maximizes $P(C_k) \\prod_{i=1}^{p} P(x_i | C_k)$.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Estimating Probabilities**\n",
    "\n",
    "* **$P(C_k)$ (Prior):**\n",
    "    $$P(C_k) = \\frac{\\text{Number of training samples in class } C_k}{\\text{Total number of training samples}}$$\n",
    "* **$P(x_i | C_k)$ (Likelihood):** The method for estimating this depends on the type of feature $x_i$ and thus the type of Naive Bayes classifier.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Types of Naive Bayes Classifiers**\n",
    "\n",
    "The main types differ in how they handle the $P(x_i | C_k)$ term, based on the assumed distribution of the features:\n",
    "\n",
    "**a) Gaussian Naive Bayes**\n",
    "* **Assumption:** Used when features $x_i$ are **continuous numerical values** and are assumed to follow a **Gaussian (normal) distribution** within each class $C_k$.\n",
    "* **Estimating $P(x_i | C_k)$:**\n",
    "    1.  For each class $C_k$ and each feature $x_i$, calculate the mean ($\\mu_{ik}$) and standard deviation ($\\sigma_{ik}$) of the values of $x_i$ from the training samples belonging to class $C_k$.\n",
    "    2.  Use the Probability Density Function (PDF) of the Gaussian distribution:\n",
    "        $$P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{ik}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{ik})^2}{2\\sigma_{ik}^2}\\right)$$\n",
    "* **Use Cases:** Problems with continuous features like sensor measurements, height, weight, etc.\n",
    "* **Scikit-learn:** `sklearn.naive_bayes.GaussianNB`\n",
    "\n",
    "**b) Multinomial Naive Bayes**\n",
    "* **Assumption:** Typically used when features represent **counts or frequencies** (usually non-negative integers). This is very common in text classification where features are word counts (e.g., Term Frequency - TF) or TF-IDF values.\n",
    "* **Estimating $P(x_i | C_k)$:**\n",
    "    For a feature $x_i$ (e.g., a specific word) and class $C_k$ (e.g., \"spam\"), this is the probability of that word appearing in a document of that class. It's typically calculated as:\n",
    "    $$P(x_i | C_k) = \\frac{\\text{count of feature } x_i \\text{ in samples of class } C_k + \\alpha}{\\text{total count of all features in samples of class } C_k + \\alpha \\cdot N_f}$$\n",
    "    Where:\n",
    "    * `count(feature $x_i$, class $C_k$)` is how many times feature $x_i$ appeared across all samples belonging to class $C_k$.\n",
    "    * `total count of all features in samples of class $C_k$` is the sum of all feature counts for that class.\n",
    "    * $N_f$ is the total number of unique features in the dataset (e.g., vocabulary size).\n",
    "    * **$\\alpha$ is a smoothing parameter (Laplace or Additive Smoothing).**\n",
    "        * If $\\alpha = 1$, it's Laplace smoothing.\n",
    "        * If $\\alpha = 0$, no smoothing.\n",
    "        * **Purpose of Smoothing:** To handle the \"zero-frequency problem.\" If a feature $x_i$ was not observed in any training sample of class $C_k$, its count would be 0, making $P(x_i | C_k) = 0$. This would cause the entire product $\\prod P(x_i | C_k)$ to become zero, regardless of other feature probabilities. Smoothing ensures that every feature has a small non-zero probability.\n",
    "* **Use Cases:** Text classification (spam filtering, document categorization, sentiment analysis).\n",
    "* **Scikit-learn:** `sklearn.naive_bayes.MultinomialNB` (has an `alpha` parameter for smoothing).\n",
    "\n",
    "**c) Bernoulli Naive Bayes**\n",
    "* **Assumption:** Used when features are **binary (0 or 1)**, indicating the presence or absence of a particular attribute.\n",
    "* **Estimating $P(x_i | C_k)$:**\n",
    "    For a feature $x_i$ and class $C_k$, it calculates the probability that feature $x_i$ is present (value 1) given class $C_k$.\n",
    "    $$P(x_i=1 | C_k) = \\frac{\\text{Number of samples in class } C_k \\text{ where feature } x_i=1 + \\alpha}{\\text{Total number of samples in class } C_k + 2\\alpha}$$\n",
    "    And $P(x_i=0 | C_k) = 1 - P(x_i=1 | C_k)$.\n",
    "    Smoothing (`alpha`) is also used here.\n",
    "* **Use Cases:** Text classification using a binary model (word is present/absent, rather than word count), or any problem with binary features.\n",
    "* **Scikit-learn:** `sklearn.naive_bayes.BernoulliNB` (has an `alpha` parameter).\n",
    "\n",
    "There are other variants like Complement Naive Bayes (good for imbalanced data) and Categorical Naive Bayes (for features that are inherently categorical, not just binary or counts).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec95656-3c13-43c6-8d32-d7bb5bc76a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
