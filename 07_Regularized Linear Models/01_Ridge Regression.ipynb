{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2412a281-e452-4c0f-a3dd-5ef2d625418b",
   "metadata": {},
   "source": [
    "- let's dive into **Topic 7: Regularized Linear Models**. \n",
    "- This is a crucial set of techniques that address some of the limitations of standard Linear Regression, particularly its tendency to overfit data, especially when you have many features or multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597947c4-3add-44a0-8e0e-42c04b80d638",
   "metadata": {},
   "source": [
    "**1. Why Do We Need Regularization?**\n",
    "\n",
    "Standard Linear Regression (Ordinary Least Squares - OLS) aims to find the coefficient values ($b_j$) that minimize the Mean Squared Error (MSE). This works well when you have a good number of samples, few features, and the model assumptions are reasonably met.\n",
    "\n",
    "However, OLS can run into problems:\n",
    "\n",
    "* **Overfitting:** If you have many features (high dimensionality), especially if the number of features is close to or exceeds the number of samples, the model can start fitting the *noise* in the training data rather than the underlying signal. This results in a model that performs exceptionally well on the training data but poorly on new, unseen data (poor generalization). Overfit models often have very large coefficient values.\n",
    "* **Multicollinearity:** When features are highly correlated, the coefficient estimates in OLS can become unstable and have high variance. Small changes in the training data can lead to large swings in the estimated coefficients, making them hard to interpret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc390ab-c338-4b8e-aacb-6b695f18bef2",
   "metadata": {},
   "source": [
    "**Regularization** is a technique used to combat these issues. It works by adding a **penalty term** to the cost function. This penalty discourages the model from learning overly complex patterns or assigning excessively large weights (coefficients) to features.\n",
    "\n",
    "**The Core Idea: Modifying the Cost Function**\n",
    "\n",
    "* Standard Linear Regression minimizes:\n",
    "    $$J_{OLS}(b) = MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "* Regularized Linear Regression minimizes a modified cost function:\n",
    "    $$J_{Regularized}(b) = MSE + \\text{Penalty Term}$$\n",
    "    The penalty term is a function of the magnitude of the coefficients. By penalizing large coefficients, regularization forces the model to be \"simpler\" and often improves its ability to generalize to new data.\n",
    "\n",
    "**Important Note on Feature Scaling:**\n",
    "For regularized models, it's **essential to scale your features** (e.g., using `StandardScaler` from Scikit-learn). This is because the penalty term is applied to the coefficients, and if features have different scales, their coefficients will naturally be on different scales, leading to uneven penalization. Scaling ensures that all features are treated fairly by the regularization process. The target variable `y` is generally not scaled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468c384-4fda-489c-98fc-be4292ef81be",
   "metadata": {},
   "source": [
    "Let's explore the three main types of regularized linear models. We'll use the **California Housing dataset** for our examples, as it has more features than the Advertising dataset and is readily available in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba30106-ee47-4ba7-99db-bd789d32354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) # To suppress some sklearn warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8be2c94b-041f-4bbb-825c-94738e37d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004db0b0-f9b7-4c0c-ad8e-edcc53627b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity, let's put X into a DataFrame with feature names\n",
    "X_df = pd.DataFrame(X, columns=housing.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7478100f-a3a2-475d-9ebc-3589c02a7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "858821d4-8291-4c83-9c5b-7c11c5e299e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw) # Fit on training and transform\n",
    "X_test = scaler.transform(X_test_raw)       # Transform test using training fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54250340-2fef-4a20-93ce-736befc3b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scaled arrays back to DataFrames for easier inspection of coefficients later\n",
    "X_train_scaled_df = pd.DataFrame(X_train, columns=X_train_raw.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test, columns=X_test_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f16219-5560-4d62-b4fe-c02b12b2f42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California Housing Dataset Loaded and Scaled.\n",
      "Training features shape: (16512, 8)\n",
      "Test features shape: (4128, 8)\n",
      "First 5 rows of X_train_raw (unscaled):\n",
      "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "14196  3.2596      33.0  5.017657   1.006421      2300.0  3.691814     32.71   \n",
      "8267   3.8125      49.0  4.473545   1.041005      1314.0  1.738095     33.77   \n",
      "17445  4.1563       4.0  5.645833   0.985119       915.0  2.723214     34.66   \n",
      "14265  1.9425      36.0  4.002817   1.033803      1418.0  3.994366     32.69   \n",
      "2271   3.5542      43.0  6.268421   1.134211       874.0  2.300000     36.78   \n",
      "\n",
      "       Longitude  \n",
      "14196    -117.03  \n",
      "8267     -118.16  \n",
      "17445    -120.48  \n",
      "14265    -117.11  \n",
      "2271     -119.80  \n",
      "\n",
      "First 5 rows of X_train_scaled_df (scaled):\n",
      "     MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0 -0.326196  0.348490 -0.174916  -0.208365    0.768276  0.051376 -1.372811   \n",
      "1 -0.035843  1.618118 -0.402835  -0.128530   -0.098901 -0.117362 -0.876696   \n",
      "2  0.144701 -1.952710  0.088216  -0.257538   -0.449818 -0.032280 -0.460146   \n",
      "3 -1.017864  0.586545 -0.600015  -0.145156   -0.007434  0.077507 -1.382172   \n",
      "4 -0.171488  1.142008  0.349007   0.086624   -0.485877 -0.068832  0.532084   \n",
      "\n",
      "   Longitude  \n",
      "0   1.272587  \n",
      "1   0.709162  \n",
      "2  -0.447603  \n",
      "3   1.232698  \n",
      "4  -0.108551  \n"
     ]
    }
   ],
   "source": [
    "print(\"California Housing Dataset Loaded and Scaled.\")\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"First 5 rows of X_train_raw (unscaled):\\n{X_train_raw.head()}\")\n",
    "print(f\"\\nFirst 5 rows of X_train_scaled_df (scaled):\\n{X_train_scaled_df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e411c-9a73-4dd6-b6fb-d03eac6e8888",
   "metadata": {},
   "source": [
    "---\n",
    "**2. Ridge Regression (L2 Regularization)**\n",
    "\n",
    "* **Penalty Term:** Ridge Regression adds a penalty proportional to the **sum of the squares of the coefficient magnitudes** (also known as the L2 norm of the coefficients). The intercept ($b_0$) is typically not regularized.\n",
    "    $$\\text{Penalty}_{L2} = \\alpha \\sum_{j=1}^{p} b_j^2$$\n",
    "* **Cost Function for Ridge:**\n",
    "    $$J_{Ridge}(b) = MSE + \\alpha \\sum_{j=1}^{p} b_j^2$$\n",
    "* **Hyperparameter $\\alpha$ (alpha):**\n",
    "    * Controls the strength of the regularization. It's a non-negative value.\n",
    "    * If $\\alpha = 0$: Ridge Regression becomes identical to OLS Linear Regression.\n",
    "    * As $\\alpha \\rightarrow \\infty$: The penalty becomes dominant, forcing all coefficients $b_j$ (for $j>0$) closer and closer to zero.\n",
    "    * The optimal value of $\\alpha$ is usually found using cross-validation.\n",
    "* **Effect of Ridge Regression:**\n",
    "    * It **shrinks** the coefficients towards zero but **rarely makes them exactly zero**. Thus, it keeps all features in the model but reduces their influence.\n",
    "    * Reduces model variance, which helps to prevent overfitting.\n",
    "    * Particularly effective when dealing with **multicollinearity** (highly correlated features), as it tends to distribute the coefficient weights more evenly among correlated features.\n",
    "* **Scikit-learn Implementation:** `sklearn.linear_model.Ridge` and `sklearn.linear_model.RidgeCV` (for built-in cross-validation to find alpha).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8e233f3-6df0-4789-9e16-428af50f0e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ridge Regression (L2 Regularization) ---\n",
      "Ridge MSE with alpha=1.0: 0.5559\n",
      "Ridge R-squared with alpha=1.0: 0.5758\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Ridge Regression (L2 Regularization) ---\")\n",
    "\n",
    "# --- Plain Ridge with a chosen alpha ---\n",
    "alpha_ridge = 1.0 # Example alpha value\n",
    "ridge_model = Ridge(alpha=alpha_ridge)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "ridge_predictions = ridge_model.predict(X_test)\n",
    "ridge_mse = mean_squared_error(y_test, ridge_predictions)\n",
    "print(f\"Ridge MSE with alpha={alpha_ridge}: {ridge_mse:.4f}\")\n",
    "print(f\"Ridge R-squared with alpha={alpha_ridge}: {ridge_model.score(X_test, y_test):.4f}\")\n",
    "# print(f\"Ridge Coefficients (alpha={alpha_ridge}): {ridge_model.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9413953f-20c6-48b7-9d3b-a04e0af8aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best alpha found by RidgeCV: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# --- RidgeCV to find the best alpha ---\n",
    "# Define a range of alphas to test\n",
    "alphas_to_test = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "# For RidgeCV, 'scoring' can be used to specify the metric for choosing alpha,\n",
    "# e.g., 'neg_mean_squared_error' (higher is better) or 'r2'. Default is model's score method.\n",
    "ridge_cv_model = RidgeCV(alphas=alphas_to_test, store_cv_values=True) # store_cv_values is useful for inspection\n",
    "ridge_cv_model.fit(X_train, y_train)\n",
    "\n",
    "best_alpha_ridge = ridge_cv_model.alpha_\n",
    "print(f\"\\nBest alpha found by RidgeCV: {best_alpha_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "849bd7bf-421e-48d1-80ff-02e9833db5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeCV MSE with best alpha: 0.5559\n",
      "RidgeCV R-squared with best alpha: 0.5758\n"
     ]
    }
   ],
   "source": [
    "ridge_cv_predictions = ridge_cv_model.predict(X_test)\n",
    "ridge_cv_mse = mean_squared_error(y_test, ridge_cv_predictions)\n",
    "print(f\"RidgeCV MSE with best alpha: {ridge_cv_mse:.4f}\")\n",
    "print(f\"RidgeCV R-squared with best alpha: {ridge_cv_model.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17f09aa8-9eb3-498b-b91c-e552b7742a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coefficients from RidgeCV model:\n",
      "MedInc        0.854327\n",
      "AveBedrms     0.339008\n",
      "HouseAge      0.122624\n",
      "Population   -0.002282\n",
      "AveOccup     -0.040833\n",
      "AveRooms     -0.294210\n",
      "Longitude    -0.869071\n",
      "Latitude     -0.896168\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCoefficients from RidgeCV model:\")\n",
    "ridge_coefs = pd.Series(ridge_cv_model.coef_, index=X_train_scaled_df.columns)\n",
    "print(ridge_coefs.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d225d34-1891-4e4e-a118-17f69425f4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For comparison: OLS Linear Regression MSE: 0.5559\n",
      "OLS Linear Regression R-squared: 0.5758\n"
     ]
    }
   ],
   "source": [
    "# Let's compare with OLS Linear Regression\n",
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X_train, y_train)\n",
    "ols_predictions = ols_model.predict(X_test)\n",
    "ols_mse = mean_squared_error(y_test, ols_predictions)\n",
    "print(f\"\\nFor comparison: OLS Linear Regression MSE: {ols_mse:.4f}\")\n",
    "print(f\"OLS Linear Regression R-squared: {ols_model.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8af115d9-f1ac-4403-9d8e-99f894240dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OLS Coefficients:\n",
      "MedInc        0.854383\n",
      "AveBedrms     0.339259\n",
      "HouseAge      0.122546\n",
      "Population   -0.002308\n",
      "AveOccup     -0.040829\n",
      "AveRooms     -0.294410\n",
      "Longitude    -0.869842\n",
      "Latitude     -0.896929\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOLS Coefficients:\")\n",
    "ols_coefs = pd.Series(ols_model.coef_, index=X_train_scaled_df.columns)\n",
    "print(ols_coefs.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e29942-0837-4559-9c5e-30d54a84e817",
   "metadata": {},
   "source": [
    "**Observations from Ridge:**\n",
    "You'll typically see that Ridge coefficients are smaller in magnitude compared to OLS coefficients, especially if OLS had very large ones. The MSE on the test set for Ridge might be slightly higher or lower than OLS, depending on whether OLS was overfitting. The primary benefit is often improved model stability and better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f88b98-2d30-4ced-bfdb-4ff87e435253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
