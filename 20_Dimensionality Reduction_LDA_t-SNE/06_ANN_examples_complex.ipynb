{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706bb837-e7dc-4d7c-befd-e884cb231a4e",
   "metadata": {},
   "source": [
    "- let's build some more complex Artificial Neural Network (ANN) examples using PyTorch. We'll increase the complexity by adding more layers, introducing techniques like dropout for regularization, and incorporating a validation loop into our training process.\n",
    "\n",
    "#### A. More Complex ANN for Classification: FashionMNIST Dataset\n",
    "\n",
    "The FashionMNIST dataset is a good step up from MNIST. It consists of 28x28 grayscale images of 10 different fashion categories (e.g., T-shirt, trouser, pullover, dress). It's more challenging than MNIST, requiring a slightly more capable network.\n",
    "\n",
    "Complexity Additions:\n",
    "\n",
    "- Deeper network (2 hidden layers).\n",
    "- nn.Dropout layer for regularization to prevent overfitting.\n",
    "- Validation loop within training to monitor performance on a validation set.\n",
    "- Plotting of training and validation loss/accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f859016d-f487-4939-bcbf-dfe7b82af210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Hyperparameters ---\n",
    "input_size = 784  # FashionMNIST images are 28x28 pixels -> flattened to 784\n",
    "hidden_size1 = 512 # Neurons in first hidden layer\n",
    "hidden_size2 = 256 # Neurons in second hidden layer\n",
    "num_classes = 10  # 10 fashion categories\n",
    "num_epochs = 20   # Train for more epochs\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "dropout_prob = 0.5 # Dropout probability\n",
    "\n",
    "# --- 3. Load and Prepare FashionMNIST Dataset ---\n",
    "print(\"Loading FashionMNIST dataset...\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,)) # Mean and std for FashionMNIST (approximate)\n",
    "])\n",
    "\n",
    "# Download and load the full training data\n",
    "full_train_dataset = torchvision.datasets.FashionMNIST(root='./data',\n",
    "                                                       train=True,\n",
    "                                                       transform=transform,\n",
    "                                                       download=True)\n",
    "\n",
    "# Download and load the test data\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data',\n",
    "                                                 train=False,\n",
    "                                                 transform=transform)\n",
    "\n",
    "# Split full training data into training and validation sets\n",
    "train_size = int(0.8 * len(full_train_dataset)) # 80% for training\n",
    "val_size = len(full_train_dataset) - train_size   # 20% for validation\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Dataset loaded. Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "fashion_mnist_classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                         'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# --- 4. Define the Deeper Neural Network Model with Dropout ---\n",
    "class DeeperANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes, dropout_prob):\n",
    "        super(DeeperANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_prob) # Dropout layer after first hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_prob) # Dropout layer after second hidden layer\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes) # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out) # Apply dropout\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out) # Apply dropout\n",
    "        out = self.fc3(out)\n",
    "        # No softmax here if using nn.CrossEntropyLoss\n",
    "        return out\n",
    "\n",
    "# --- 5. Instantiate the Model, Loss, and Optimizer ---\n",
    "model = DeeperANN(input_size, hidden_size1, hidden_size2, num_classes, dropout_prob).to(device)\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- 6. Training Loop with Validation ---\n",
    "print(\"\\nStarting Training...\")\n",
    "n_total_steps_train = len(train_loader)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train() # Set model to training mode (enables dropout)\n",
    "    running_train_loss = 0.0\n",
    "    n_correct_train = 0\n",
    "    n_samples_train = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, input_size).to(device) # Flatten images\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * images.size(0) # Multiply by batch size for weighted average\n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        n_samples_train += labels.size(0)\n",
    "        n_correct_train += (predicted_train == labels).sum().item()\n",
    "\n",
    "    epoch_train_loss = running_train_loss / n_samples_train\n",
    "    epoch_train_acc = 100.0 * n_correct_train / n_samples_train\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() # Set model to evaluation mode (disables dropout)\n",
    "    running_val_loss = 0.0\n",
    "    n_correct_val = 0\n",
    "    n_samples_val = 0\n",
    "    with torch.no_grad(): # No need to compute gradients during validation\n",
    "        for images_val, labels_val in val_loader:\n",
    "            images_val = images_val.reshape(-1, input_size).to(device)\n",
    "            labels_val = labels_val.to(device)\n",
    "\n",
    "            outputs_val = model(images_val)\n",
    "            loss_val = criterion(outputs_val, labels_val)\n",
    "            running_val_loss += loss_val.item() * images_val.size(0)\n",
    "\n",
    "            _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "            n_samples_val += labels_val.size(0)\n",
    "            n_correct_val += (predicted_val == labels_val).sum().item()\n",
    "\n",
    "    epoch_val_loss = running_val_loss / n_samples_val\n",
    "    epoch_val_acc = 100.0 * n_correct_val / n_samples_val\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_accuracies.append(epoch_val_acc)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, '\n",
    "          f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "\n",
    "# --- 7. Evaluation on Test Set ---\n",
    "print(\"\\nStarting Evaluation on Test Set...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    n_correct_test = 0\n",
    "    n_samples_test = 0\n",
    "    for images_test, labels_test in test_loader:\n",
    "        images_test = images_test.reshape(-1, input_size).to(device)\n",
    "        labels_test = labels_test.to(device)\n",
    "        outputs_test = model(images_test)\n",
    "        _, predicted_test = torch.max(outputs_test.data, 1)\n",
    "        n_samples_test += labels_test.size(0)\n",
    "        n_correct_test += (predicted_test == labels_test).sum().item()\n",
    "\n",
    "    accuracy_test = 100.0 * n_correct_test / n_samples_test\n",
    "    print(f'Accuracy of the network on the {len(test_dataset)} test images: {accuracy_test:.2f} %')\n",
    "\n",
    "# --- 8. Plot Training and Validation Loss and Accuracy ---\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, 'ro-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (CrossEntropy)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracies, 'ro-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bc93a-eb76-4cef-846d-71cc0ab727c8",
   "metadata": {},
   "source": [
    "# PyTorch \"Complex\" Classification ANN: Key Enhancements\n",
    "\n",
    "This document details the key enhancements made to an Artificial Neural Network (ANN) for a more complex image classification task, focusing on improvements for better generalization and performance monitoring.\n",
    "\n",
    "## 1. FashionMNIST Dataset\n",
    "- **Challenge**: Uses the `FashionMNIST` dataset (`torchvision.datasets.FashionMNIST`).\n",
    "- **Nature**: This dataset consists of 28x28 grayscale images of 10 different types of clothing (e.g., T-shirt, trousers, pullover, dress). It serves as a more challenging alternative to the simpler MNIST handwritten digit dataset, often used as a \"drop-in\" replacement to test model robustness.\n",
    "\n",
    "## 2. Deeper Network Architecture (`DeeperANN` class)\n",
    "- **Structure**: The neural network, `DeeperANN`, is designed with increased depth compared to a simpler MLP.\n",
    "- **Layer Configuration**: It now features two hidden layers, each followed by a ReLU activation and a Dropout layer:\n",
    "    - `Input -> Linear_1 (fc1) -> ReLU_1 (relu1) -> Dropout_1 (dropout1) -> Linear_2 (fc2) -> ReLU_2 (relu2) -> Dropout_2 (dropout2) -> Linear_3 (fc3) -> Output (Logits)`\n",
    "- **Rationale**: Adding more layers (depth) can potentially allow the network to learn more complex hierarchical features from the input data.\n",
    "\n",
    "## 3. Dropout (`nn.Dropout`)\n",
    "- **Implementation**: Dropout layers (`nn.Dropout(p=dropout_prob)`) are strategically placed after the ReLU activation function in each hidden layer.\n",
    "- **Mechanism**:\n",
    "    - **During Training (`model.train()` mode)**: For each training sample and for each forward pass, dropout randomly sets a fraction (`dropout_prob`) of the input units (neurons) from the preceding layer to zero. The outputs of the remaining active neurons are typically scaled up by a factor of `1/(1-dropout_prob)` to maintain the expected sum.\n",
    "    - **Purpose**: This technique acts as a form of regularization. By preventing neurons from co-adapting too much on the training data, it helps the network learn more robust and independent features. This, in turn, significantly reduces overfitting and improves the model's ability to generalize to unseen data.\n",
    "    - **During Evaluation (`model.eval()` mode)**: Dropout is automatically disabled. All neurons in the layer are used (no units are zeroed out), and the scaling factor is not applied. This ensures deterministic output during inference.\n",
    "\n",
    "## 4. Introduction of a Validation Set\n",
    "- **Data Splitting**: The original `FashionMNIST` training dataset is further divided into two subsets: a new, smaller training set and a validation set.\n",
    "- **Method**: `torch.utils.data.random_split` is used to perform this split, typically allocating a certain percentage (e.g., 80% for training, 20% for validation) of the original training data.\n",
    "- **Purpose**:\n",
    "    - The training set is used to update the model's weights (learn).\n",
    "    - The validation set is used to tune hyperparameters (like learning rate, network architecture, regularization strength) and to monitor the model's generalization performance during training without \"contaminating\" the final test set.\n",
    "\n",
    "## 5. Training Loop with Integrated Validation\n",
    "- **Epoch-wise Evaluation**: After each full epoch of training on the training data (where `model.train()` is active):\n",
    "    1.  **Switch to Evaluation Mode**: The model is set to evaluation mode using `model.eval()`. This ensures that layers like Dropout behave correctly for inference (i.e., are turned off).\n",
    "    2.  **No Gradient Calculation**: Performance on the validation set is computed within a `with torch.no_grad():` block. This disables gradient calculations, saving memory and computation time, as gradients are not needed for validation.\n",
    "    3.  **Performance Metrics**: Key metrics such as loss and accuracy are calculated on the `val_loader` (the DataLoader for the validation set).\n",
    "- **Overfitting Monitoring**: This regular validation step is crucial for detecting overfitting.\n",
    "    - **Signs of Overfitting**:\n",
    "        - Training loss continues to decrease while validation loss starts to increase or stagnates.\n",
    "        - Training accuracy continues to improve while validation accuracy stagnates or starts to decrease.\n",
    "    - **Action**: Observing these signs allows for early intervention, such as stopping training early (early stopping), adjusting regularization (like dropout rate or weight decay), or simplifying the model.\n",
    "\n",
    "## 6. Enhanced Plotting\n",
    "- **Comparative Visualization**: The plotting functionality is extended to visualize both training metrics and validation metrics (loss and accuracy) against the number of epochs on the same graph.\n",
    "- **Benefit**: This side-by-side comparison makes it much easier to:\n",
    "    - Observe the learning trends for both sets.\n",
    "    - Visually identify the onset of overfitting by comparing the divergence or stagnation of validation curves relative to training curves.\n",
    "    - Make informed decisions about the training process and model adjustments.\n",
    "\n",
    "## Summary\n",
    "These enhancements—using a more challenging dataset, deepening the network, incorporating dropout for regularization, splitting data into training/validation sets, and performing regular validation—represent common practices in developing more robust and well-generalized neural network models. The integrated validation and comparative plotting provide critical insights into the learning process, especially for managing the bias-variance tradeoff and preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a353ab-3d43-4d95-8d0a-443cfd1a08d8",
   "metadata": {},
   "source": [
    "---\n",
    "- ANN for Regression: Diabetes Dataset\n",
    "\n",
    "We'll use the Diabetes dataset from sklearn.datasets. It has 10 baseline variables (age, sex, body mass index, average blood pressure, and six blood serum measurements) for 442 diabetes patients, and the target is a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "Complexity Additions:\n",
    "\n",
    "- Deeper network (2-3 hidden layers).\n",
    "- nn.Dropout for regularization.\n",
    "- Validation loop within training.\n",
    "- Plotting of training and validation loss (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2211e4-c3d5-46ed-808d-85c938f8bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Hyperparameters ---\n",
    "# input_size will be determined by the dataset\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "# hidden_size3 = 16 # Optional third hidden layer\n",
    "output_size = 1    # Predicting a single continuous value\n",
    "num_epochs = 100   # More epochs for potentially better convergence\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "dropout_prob = 0.3 # Dropout probability\n",
    "\n",
    "# --- 3. Load and Prepare Diabetes Dataset ---\n",
    "print(\"Loading Diabetes dataset...\")\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "X_df = pd.DataFrame(X, columns=diabetes.feature_names)\n",
    "y_series = pd.Series(y, name='Progression')\n",
    "\n",
    "input_size = X.shape[1]\n",
    "print(f\"Dataset loaded. Number of features: {input_size}\")\n",
    "print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "\n",
    "# Split data into training and testing sets (overall split)\n",
    "X_train_val_raw, X_test_raw, y_train_val_raw, y_test_raw = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Further split training_validation set into training and validation\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(\n",
    "    X_train_val_raw, y_train_val_raw, test_size=0.2, random_state=42 # 0.2 of 0.8 = 0.16 of total\n",
    ")\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw) # Use transform on validation\n",
    "X_test_scaled = scaler.transform(X_test_raw) # Use transform on test\n",
    "\n",
    "# Convert data to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_raw, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_raw, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_raw, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Data prepared. Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# --- 4. Define the Deeper Neural Network Model for Regression with Dropout ---\n",
    "class DeeperANNRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, dropout_prob):\n",
    "        super(DeeperANNRegression, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        # No output activation for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# --- 5. Instantiate the Model, Loss, and Optimizer ---\n",
    "model = DeeperANNRegression(input_size, hidden_size1, hidden_size2, output_size, dropout_prob).to(device)\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- 6. Training Loop with Validation ---\n",
    "print(\"\\nStarting Training...\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for features, targets in train_loader:\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * features.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_train_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for features_val, targets_val in val_loader:\n",
    "            features_val = features_val.to(device)\n",
    "            targets_val = targets_val.to(device)\n",
    "            outputs_val = model(features_val)\n",
    "            loss_val = criterion(outputs_val, targets_val)\n",
    "            running_val_loss += loss_val.item() * features_val.size(0)\n",
    "            \n",
    "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss (MSE): {epoch_train_loss:.4f}, Val Loss (MSE): {epoch_val_loss:.4f}')\n",
    "\n",
    "print(\"Finished Training.\")\n",
    "\n",
    "# --- 7. Evaluation on Test Set ---\n",
    "print(\"\\nStarting Evaluation on Test Set...\")\n",
    "model.eval()\n",
    "all_predictions_test = []\n",
    "all_targets_test = []\n",
    "with torch.no_grad():\n",
    "    test_loss_final = 0.0\n",
    "    for features_test, targets_test in test_loader:\n",
    "        features_test = features_test.to(device)\n",
    "        targets_test = targets_test.to(device)\n",
    "        outputs_test = model(features_test)\n",
    "        loss_test = criterion(outputs_test, targets_test)\n",
    "        test_loss_final += loss_test.item() * features_test.size(0)\n",
    "        all_predictions_test.append(outputs_test.cpu().numpy())\n",
    "        all_targets_test.append(targets_test.cpu().numpy())\n",
    "\n",
    "avg_test_loss_final = test_loss_final / len(test_dataset)\n",
    "all_predictions_test = np.concatenate(all_predictions_test, axis=0)\n",
    "all_targets_test = np.concatenate(all_targets_test, axis=0)\n",
    "\n",
    "mse_test_final = mean_squared_error(all_targets_test, all_predictions_test)\n",
    "r2_test_final = r2_score(all_targets_test, all_predictions_test)\n",
    "\n",
    "print(f'Average Test Loss (MSE) from loop: {avg_test_loss_final:.4f}')\n",
    "print(f'Calculated Final Test MSE: {mse_test_final:.4f}')\n",
    "print(f'Final Test R-squared (R2): {r2_test_final:.4f}')\n",
    "\n",
    "# --- 8. Plot Training and Validation Loss ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, 'bo-', label='Training Loss (MSE)')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, 'ro-', label='Validation Loss (MSE)')\n",
    "plt.title('Training and Validation Loss per Epoch (Diabetes Regression)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- 9. Plot Actual vs. Predicted Values (Test Set) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(all_targets_test, all_predictions_test, alpha=0.5, edgecolors='k', label='Predictions')\n",
    "min_val_diag = min(all_targets_test.min(), all_predictions_test.min())\n",
    "max_val_diag = max(all_targets_test.max(), all_predictions_test.max())\n",
    "plt.plot([min_val_diag, max_val_diag], [min_val_diag, max_val_diag], 'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Disease Progression\")\n",
    "plt.ylabel(\"Predicted Disease Progression\")\n",
    "plt.title(\"Actual vs. Predicted Values (Test Set - Diabetes Regression)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed834c5-6cdf-4c27-b5a2-f5057f253498",
   "metadata": {},
   "source": [
    "# PyTorch \"Complex\" Regression ANN: Key Enhancements\n",
    "\n",
    "This document details the key enhancements applied to an Artificial Neural Network (ANN) designed for a regression task, focusing on a more robust architecture and a comprehensive training/evaluation workflow.\n",
    "\n",
    "## 1. Diabetes Dataset\n",
    "- **Source**: Utilizes the standard `load_diabetes` dataset from `sklearn.datasets`.\n",
    "- **Nature**: This is a regression dataset where the goal is to predict a quantitative measure of disease progression one year after baseline, based on ten baseline variables (age, sex, body mass index, average blood pressure, and six blood serum measurements).\n",
    "- **Preprocessing**: As with most regression tasks involving neural networks, feature scaling (e.g., `StandardScaler`) is typically applied to the input features, and the target variable might also be scaled or transformed depending on its distribution.\n",
    "\n",
    "## 2. Deeper Network Architecture with Dropout (`DeeperANNRegression` class)\n",
    "- **Structure**: The `DeeperANNRegression` model is designed with increased depth and regularization capabilities.\n",
    "- **Layer Configuration**: It features two hidden layers, each incorporating ReLU activation functions and Dropout layers:\n",
    "    - `Input -> Linear_1 (fc1) -> ReLU_1 (relu1) -> Dropout_1 (dropout1) -> Linear_2 (fc2) -> ReLU_2 (relu2) -> Dropout_2 (dropout2) -> Linear_3 (fc3) -> Output (Single Continuous Value)`\n",
    "- **Dropout (`nn.Dropout`)**:\n",
    "    - **Implementation**: Dropout layers are added after each ReLU activation in the hidden layers.\n",
    "    - **Purpose**: During training, dropout randomly deactivates a fraction of neurons. This regularization technique helps prevent overfitting by encouraging the network to learn more robust and less co-dependent features. During evaluation (`model.eval()`), dropout is automatically disabled.\n",
    "- **Output Layer**: The final layer (`fc3`) outputs a single neuron with no activation function, as is standard for regression tasks to predict a continuous value.\n",
    "\n",
    "## 3. Train/Validation/Test Split\n",
    "- **Data Partitioning**: The dataset is divided into three distinct subsets:\n",
    "    1.  **Training Set**: Used to fit the model parameters (i.e., learn the weights and biases).\n",
    "    2.  **Validation Set**: Used to monitor the model's performance on unseen data *during* the training process. This helps in:\n",
    "        - Tuning hyperparameters (e.g., learning rate, number of layers/neurons, dropout rate).\n",
    "        - Early stopping (deciding when to stop training to prevent overfitting if validation performance starts to degrade).\n",
    "    3.  **Test Set**: Used for a final, unbiased evaluation of the trained model's performance after all training and hyperparameter tuning are complete. This provides an estimate of how the model will perform on new, real-world data.\n",
    "- **Method**: Typically achieved using functions like `train_test_split` from `sklearn.model_selection` multiple times or by carefully indexing the data.\n",
    "\n",
    "## 4. Validation Loop Integrated with Training\n",
    "- **Epoch-wise Evaluation**: Similar to the \"complex\" classification setup, a validation step is performed after each training epoch:\n",
    "    1.  **Switch to Evaluation Mode**: The model is set to `model.eval()`.\n",
    "    2.  **No Gradient Calculation**: Calculations on the validation set are performed within a `with torch.no_grad():` block.\n",
    "    3.  **Performance Metric**: The Mean Squared Error (MSE) loss is calculated on the `val_loader` (DataLoader for the validation set).\n",
    "- **Overfitting Monitoring**: Tracking validation MSE alongside training MSE allows for the detection of overfitting. If training MSE continues to decrease while validation MSE stagnates or increases, it indicates that the model is learning the training data too well (including its noise) and losing its ability to generalize.\n",
    "\n",
    "## 5. Enhanced Plotting\n",
    "- **Loss Visualization**:\n",
    "    - Both training MSE and validation MSE are plotted against the number of epochs. This comparative plot is critical for visualizing:\n",
    "        - The learning progress on both datasets.\n",
    "        - The point at which overfitting might begin (divergence of training and validation loss curves).\n",
    "- **Prediction Quality (Test Set)**:\n",
    "    - An \"Actual vs. Predicted\" scatter plot is generated using the test set.\n",
    "    - **Purpose**: This provides a visual assessment of the regression model's performance. For a good model, the points should cluster tightly around the diagonal line (where actual values equal predicted values).\n",
    "    - Additional regression metrics like R-squared ($R^2$) score are typically reported alongside this plot for a quantitative evaluation.\n",
    "\n",
    "## Summary\n",
    "These enhancements—employing a standard regression dataset, building a deeper network with dropout for regularization, and implementing a rigorous train/validation/test split with integrated validation monitoring—lead to a more robust and reliable workflow for developing regression ANNs. This approach not only aims for better predictive performance by mitigating overfitting but also provides clearer insights into the model's learning behavior and generalization capabilities through comprehensive evaluation and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126707a-699b-4426-b29f-cb1728e38436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
