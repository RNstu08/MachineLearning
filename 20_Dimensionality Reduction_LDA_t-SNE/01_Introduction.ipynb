{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58021ecb-eb04-43d8-8b85-4f86d71fcddf",
   "metadata": {},
   "source": [
    "**Linear Discriminant Analysis (LDA)** and **t-Distributed Stochastic Neighbor Embedding (t-SNE)**.\n",
    "\n",
    "These methods offer different approaches and are suited for different purposes compared to PCA.\n",
    "\n",
    "**1. Linear Discriminant Analysis (LDA)**\n",
    "\n",
    "* **Type:** **Supervised** dimensionality reduction technique.\n",
    "    * This is a key difference from PCA, which is unsupervised. LDA uses the class labels of your data during the dimensionality reduction process.\n",
    "* **Primary Goal:** To find a lower-dimensional subspace that **maximizes the separability between classes**.\n",
    "    * It projects the data onto axes (called linear discriminants) that maximize the ratio of between-class variance to within-class variance. In simpler terms, it tries to find dimensions that push different classes far apart while keeping points within the same class close together.\n",
    "* **How it Works (Conceptual):**\n",
    "    1.  Computes the mean of each class for all features.\n",
    "    2.  Computes two scatter matrices:\n",
    "        * **Within-class scatter matrix ($S_W$):** Represents how scattered the data is within each class. LDA tries to minimize this.\n",
    "        * **Between-class scatter matrix ($S_B$):** Represents how scattered the means of the different classes are from each other. LDA tries to maximize this.\n",
    "    3.  It then solves an eigenvalue problem (related to $S_W^{-1}S_B$) to find the linear discriminants (eigenvectors) that achieve this maximization of class separability.\n",
    "* **Number of Components:** The maximum number of linear discriminants (components) LDA can find is $c-1$, where $c$ is the number of classes. For a binary classification problem, LDA will find only 1 discriminant.\n",
    "* **Use Cases:**\n",
    "    * Primarily used as a feature extraction technique for **classification tasks**. By reducing dimensions while maximizing class separability, it can sometimes improve the performance of subsequent classifiers and reduce computational cost.\n",
    "    * Can also be used for visualization if reduced to 2 or 3 components, showing how classes are separated.\n",
    "* **Contrast with PCA:**\n",
    "    * **PCA (Unsupervised):** Finds directions of maximum variance in the data *without* considering class labels.\n",
    "    * **LDA (Supervised):** Finds directions that maximize class separability *using* class labels. The \"best\" directions for PCA might not be the best for separating classes, and vice-versa.\n",
    "* **Assumptions:** LDA assumes that features are normally distributed (Gaussian) and that classes have identical covariance matrices. However, it often works reasonably well even if these assumptions are not perfectly met.\n",
    "* **Scikit-learn:** `sklearn.discriminant_analysis.LinearDiscriminantAnalysis`\n",
    "\n",
    "---\n",
    "\n",
    "**2. t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "\n",
    "* **Type:** **Non-linear** dimensionality reduction technique.\n",
    "* **Primary Goal:** **Visualization** of high-dimensional datasets in low-dimensional space (typically 2D or 3D). It's exceptionally good at revealing local structure and clusters.\n",
    "* **How it Works (Conceptual):**\n",
    "    1.  **High-Dimensional Similarities:** t-SNE models the similarity between pairs of high-dimensional data points as conditional probabilities. Similar points are assigned a higher probability of being picked as neighbors. It uses a Gaussian distribution centered on each point to model these probabilities.\n",
    "    2.  **Low-Dimensional Similarities:** It then tries to create a low-dimensional embedding of these points (e.g., in 2D) and models the similarities between these low-dimensional points using a t-distribution (which has heavier tails than a Gaussian, helping to separate dissimilar points further apart and reduce crowding of points in the center of the map).\n",
    "    3.  **Minimizing Divergence:** The algorithm iteratively adjusts the positions of the points in the low-dimensional embedding to minimize the Kullback-Leibler (KL) divergence between the two distributions of pairwise similarities (the high-dimensional one and the low-dimensional one). Essentially, it tries to make the low-dimensional representation reflect the neighborhood structure of the high-dimensional data.\n",
    "* **Key Characteristics & Considerations:**\n",
    "    * **Preserves Local Structure:** t-SNE is excellent at showing which points are \"neighbors\" in the high-dimensional space, often revealing distinct clusters very clearly.\n",
    "    * **Global Structure Not Always Preserved:** The relative sizes of clusters and the distances *between* clusters in a t-SNE plot are often not meaningful. You should not interpret these aspects too literally. The primary focus is on which points group together.\n",
    "    * **Computationally Intensive:** Can be slow on large datasets (though variants like UMAP are faster).\n",
    "    * **Hyperparameters are Important:**\n",
    "        * `perplexity`: Roughly related to the number of nearest neighbors that are considered for each point. Typical values are between 5 and 50. The output can be sensitive to this.\n",
    "        * `n_iter`: Number of iterations for optimization.\n",
    "        * `learning_rate`: Step size for optimization.\n",
    "    * **Not for Dimensionality Reduction for ML Models:** t-SNE is primarily a visualization tool. The transformed features are generally not suitable as input for subsequent supervised learning tasks because the mapping is complex and doesn't preserve global distances or variance in a way that's useful for most classifiers/regressors.\n",
    "* **Use Cases:**\n",
    "    * Visualizing high-dimensional datasets to explore potential clusters, groups, or manifolds.\n",
    "    * Common in fields like bioinformatics, image analysis, and natural language processing for visualizing embeddings.\n",
    "* **Scikit-learn:** `sklearn.manifold.TSNE`\n",
    "\n",
    "---\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "| Feature          | PCA (Principal Component Analysis)                 | LDA (Linear Discriminant Analysis)              | t-SNE (t-Distributed Stochastic Neighbor Embedding) |\n",
    "| :--------------- | :------------------------------------------------- | :---------------------------------------------- | :------------------------------------------------ |\n",
    "| **Type** | Unsupervised, Linear                             | Supervised, Linear                              | Unsupervised, Non-linear                          |\n",
    "| **Goal** | Maximize variance, find orthogonal components      | Maximize class separability                     | Preserve local neighborhood structure             |\n",
    "| **Uses Labels?** | No                                                 | Yes                                             | No                                                |\n",
    "| **Primary Use** | Dimensionality reduction for ML, visualization, noise reduction | Dimensionality reduction for classification, feature extraction for classification | Visualization of high-dimensional data            |\n",
    "| **Output** | Principal Components (uncorrelated)                | Linear Discriminants (maximize class separation) | Low-dimensional embedding (usually 2D or 3D)      |\n",
    "| **Interpretability** | Loadings can be interpreted                      | Discriminants relate to class separation        | Distances between clusters often not meaningful   |\n",
    "| **Scaling** | Crucial (Standardization)                        | Often recommended                               | Often recommended                                 |\n",
    "\n",
    "This brief overview should give you a sense of what LDA and t-SNE are and how they differ from PCA. They are valuable tools for specific types of dimensionality reduction and visualization tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c74ad-54a2-479a-8bab-24cd0a1f5611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87e3d6-0b62-4843-a054-d104a4910252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37335fb-b74f-4a10-982e-c797d69e6211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d5ac2-f330-4b2b-b631-9639df1c1345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211947c-44df-48d8-b931-33d2e716c69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb107b2-0417-4cbf-a9c8-1fda2076607b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
