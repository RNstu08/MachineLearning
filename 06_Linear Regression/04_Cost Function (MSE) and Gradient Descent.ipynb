{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e156f181-f765-4433-8269-0eca47feb142",
   "metadata": {},
   "source": [
    "Alright, let's move on to two fundamental concepts that explain *how* our linear regression model (and many other machine learning models) actually \"learns\" the best parameters: the **Cost Function (specifically MSE)** and the optimization algorithm **Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6857b57-ab76-41aa-86fc-8309ea66cd3b",
   "metadata": {},
   "source": [
    "**Part 4: Cost Function (Mean Squared Error - MSE)**\n",
    "\n",
    "1.  **What is a Cost Function?**\n",
    "    * In supervised learning, our goal is to create a model that makes predictions ($\\hat{y}$) that are as close as possible to the actual target values ($y$).\n",
    "    * A **cost function** (also called a loss function or objective function) quantifies how \"bad\" or \"wrong\" our model's predictions are for a given set of model parameters (like $b_0$ and $b_1$ in simple linear regression).\n",
    "    * The core idea of training a model is to find the specific values for its parameters that **minimize** this cost function. The lower the cost, the better our model fits the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200830df-7bb5-47f4-a074-dfe4b9b9c40e",
   "metadata": {},
   "source": [
    "2.  **Mean Squared Error (MSE) for Linear Regression**\n",
    "    * For a single data point $i$, the error (or residual) is the difference between the actual value $y_i$ and the predicted value $\\hat{y}_i$:\n",
    "        $$\\text{error}_i = y_i - \\hat{y}_i$$\n",
    "    * We want to combine these errors from all $n$ data points in our training set into a single cost value.\n",
    "    * **Why square the error?**\n",
    "        1.  **Direction doesn't matter for cost:** An error of -2 is just as bad as an error of +2. Squaring makes all errors positive: $(-2)^2 = 4$ and $(2)^2 = 4$.\n",
    "        2.  **Penalizes larger errors more:** An error of 4 (squared is 16) is penalized more than twice an error of 2 (squared is 4). This often makes sense as large errors are usually more problematic.\n",
    "        3.  **Mathematical properties:** The squared error function is smooth and convex (for linear regression), meaning it has a single global minimum and its derivative is easy to calculate, which is essential for optimization algorithms like Gradient Descent.\n",
    "    * **Sum of Squared Errors (SSE) or Residual Sum of Squares (RSS):** This is the sum of all the squared errors:\n",
    "        $$\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "    * **Mean Squared Error (MSE):** This is simply the average of the squared errors. It's often preferred over SSE because it's independent of the number of data points, making it easier to compare across datasets of different sizes.\n",
    "        $$\\text{MSE} = J(b_0, b_1, ..., b_p) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "        The notation $J(b_0, b_1, ..., b_p)$ indicates that the MSE is a function of our model's parameters (the coefficients $b_j$). For Simple Linear Regression ($ \\hat{y}_i = b_0 + b_1 x_i $):\n",
    "        $$\\text{MSE} = J(b_0, b_1) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (b_0 + b_1 x_i))^2$$\n",
    "        For Multiple Linear Regression ($ \\hat{y}_i = b_0 + b_1 x_{i1} + ... + b_p x_{ip} $):\n",
    "        $$\\text{MSE} = J(b_0, ..., b_p) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (b_0 + b_1 x_{i1} + ... + b_p x_{ip}))^2$$\n",
    "    * **Visualizing the Cost Function (Conceptual):**\n",
    "        * For Simple Linear Regression (with parameters $b_0$ and $b_1$), if you plot MSE against possible values of $b_0$ and $b_1$, you'd get a 3D bowl shape. Our goal is to find the coordinates ($b_0, b_1$) at the very bottom of this bowl.\n",
    "        * For Multiple Linear Regression, this \"bowl\" exists in higher dimensions, but the principle is the same: it's a convex shape, meaning there's one global minimum we're trying to find.\n",
    "\n",
    "    *(Self-correction: The factor for MSE is often $1/n$ or $1/(2n)$ for mathematical convenience in derivatives, especially when using gradient descent. Scikit-learn's `mean_squared_error` uses $1/n$. The $1/2$ factor sometimes simplifies the gradient calculation by canceling out the '2' from differentiating the squared term. The fundamental shape and location of the minimum remain the same.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae0ba4-fadc-41cc-8268-075e2c636851",
   "metadata": {},
   "source": [
    "3.  **Why is MSE the Standard for (Basic) Linear Regression?**\n",
    "    * When the assumptions of linear regression hold (particularly normally distributed and homoscedastic errors), minimizing the MSE is equivalent to finding the **Maximum Likelihood Estimate (MLE)** for the coefficients. This gives the MSE a strong statistical grounding.\n",
    "    * As mentioned, its mathematical properties (convexity and differentiability) make it well-suited for optimization.\n",
    "\n",
    "Now that we know what we're trying to minimize (the MSE), let's look at *how* we can find the parameters that achieve this minimum. This brings us to Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d91a38-b02d-4481-9a6a-2641390cc2e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Part 5: Gradient Descent**\n",
    "\n",
    "1.  **The Optimization Problem:**\n",
    "    Our goal is to find the values of $b_0, b_1, ..., b_p$ that minimize the cost function $J(b_0, b_1, ..., b_p)$.\n",
    "    * One way to solve this for linear regression is using the **Normal Equation**: $b = (X^T X)^{-1} X^T y$. This is an analytical solution that directly calculates the optimal coefficients in one step. Scikit-learn's `LinearRegression` often uses this (or a similar method based on matrix decomposition like SVD).\n",
    "    * However, the Normal Equation can be computationally expensive if you have a very large number of features (calculating $(X^T X)^{-1}$ is roughly $O(p^3)$ where $p$ is the number of features). Also, for many other machine learning models, an analytical solution like the Normal Equation doesn't exist.\n",
    "    * This is where **Gradient Descent**, an iterative optimization algorithm, comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0547b1-1d81-4209-a666-68d8cfb220af",
   "metadata": {},
   "source": [
    "2.  **Intuition behind Gradient Descent:**\n",
    "    * Imagine you're standing on a foggy mountain (the cost function surface) and you want to get to the lowest point (the minimum cost).\n",
    "    * You can't see the whole landscape, but you can feel the slope of the ground under your feet.\n",
    "    * To go down, you'd take a step in the steepest downhill direction.\n",
    "    * You repeat this process, taking small steps, and eventually, you should reach the valley floor.\n",
    "    * The \"slope\" in mathematical terms is the **gradient** of the cost function. The gradient is a vector of partial derivatives of $J$ with respect to each parameter ($ \\frac{\\partial J}{\\partial b_0}, \\frac{\\partial J}{\\partial b_1}, ..., \\frac{\\partial J}{\\partial b_p} $).\n",
    "    * The gradient points in the direction of the **steepest ascent** (uphill). So, to go downhill, we move in the **opposite direction** of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862e99b-9242-4b39-9f55-a45c780f4b44",
   "metadata": {},
   "source": [
    "3.  **Gradient Descent Algorithm Steps:**\n",
    "    1.  **Initialize Parameters:** Start with some initial guesses for the coefficients $b_0, b_1, ..., b_p$ (e.g., all zeros or small random numbers).\n",
    "    2.  **Define a Learning Rate $\\alpha$**: This determines the size of the steps we take (more on this later).\n",
    "    3.  **Repeat** for a specified number of iterations or until the cost function stops decreasing significantly (convergence):\n",
    "        a.  **Calculate the Gradient:** Compute the partial derivative of the cost function $J$ with respect to each parameter $b_j$. For MSE and linear regression, the gradient components are:\n",
    "            For a single training example $(x_i, y_i)$ and prediction $\\hat{y}_i$:\n",
    "            $$\\frac{\\partial J}{\\partial b_0} = \\frac{1}{n} \\sum_{i=1}^{n} -2(y_i - \\hat{y}_i) = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$$\n",
    "            $$\\frac{\\partial J}{\\partial b_j} = \\frac{1}{n} \\sum_{i=1}^{n} -2(y_i - \\hat{y}_i)x_{ij} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)x_{ij} \\quad (\\text{for feature } j > 0)$$\n",
    "            (*Note: If MSE is defined as $\\frac{1}{2n}\\sum(...)^2$, the '2's cancel out. We'll stick to the $1/n$ definition and keep the '2' for now, or adjust the learning rate accordingly.*)\n",
    "            In vector form for all parameters $b$:\n",
    "            $$\\nabla J(b) = \\frac{2}{n} X^T (Xb - y)$$\n",
    "            where $X$ is the feature matrix (with an intercept column), $b$ is the vector of coefficients, and $y$ is the vector of actual target values. $Xb - y$ is the vector of errors $(\\hat{y} - y)$. (Note: $Xb-y$ vs $y-Xb$ just flips the sign of the gradient which is absorbed by the update rule's minus sign). Let's use $y - Xb$ to be consistent with $y_i - \\hat{y}_i$. So $\\nabla J(b) = -\\frac{2}{n} X^T (y - Xb)$.\n",
    "\n",
    "        b.  **Update Parameters:** Adjust each parameter by moving it in the opposite direction of its corresponding gradient component, scaled by the learning rate $\\alpha$:\n",
    "            $$b_j := b_j - \\alpha \\frac{\\partial J}{\\partial b_j}$$\n",
    "            This means: $ b_{\\text{new}} = b_{\\text{old}} - \\alpha \\cdot \\text{gradient} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb04979c-989c-41a1-828d-46a46736a88e",
   "metadata": {},
   "source": [
    "4.  **The Learning Rate ($\\alpha$)**\n",
    "    * This is a **hyperparameter** that controls how big of a step we take downhill in each iteration.\n",
    "    * **Its choice is critical:**\n",
    "        * **Too small $\\alpha$**: Gradient descent will be very slow to converge, requiring many iterations. It might also get stuck in a shallow local minimum if the cost function isn't perfectly convex (though for linear regression with MSE, it is).\n",
    "        * **Too large $\\alpha$**: The algorithm might overshoot the minimum. Instead of converging, the cost might oscillate wildly or even diverge (increase indefinitely).\n",
    "    * Finding a good learning rate often involves some experimentation. A common practice is to try several values (e.g., 0.001, 0.01, 0.1, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1676ba-9562-4f24-ba7a-2d59d9a16dca",
   "metadata": {},
   "source": [
    "5.  **Feature Scaling for Gradient Descent**\n",
    "    * It's **highly recommended, often essential**, to scale your features before applying gradient descent.\n",
    "    * **Why?** If features have vastly different scales (e.g., 'age' from 20-70 and 'income' from 20000-100000), the cost function surface becomes very elongated and narrow (like a stretched ellipse instead of a circle). Gradient descent will then take a very inefficient, zig-zag path to the minimum and converge very slowly.\n",
    "    * **Common scaling methods:**\n",
    "        * **Standardization (Z-score normalization):** $x' = (x - \\mu) / \\sigma$. Transforms data to have mean 0 and standard deviation 1. (Scikit-learn's `StandardScaler`)\n",
    "        * **Normalization (Min-Max scaling):** $x' = (x - \\text{min}) / (\\text{max} - \\text{min})$. Scales data to a fixed range, usually [0, 1]. (Scikit-learn's `MinMaxScaler`)\n",
    "    * Scaling ensures that the cost function surface is more symmetrical, allowing gradient descent to find the minimum much more quickly and directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20b853b-01a1-44c7-9188-9562258b6866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a basic gradient descent for SLR. We'll use a small synthetic dataset.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression # To compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7844826e-f2ea-4733-95b9-f61882e4c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Generate synthetic data for SLR\n",
    "np.random.seed(42)\n",
    "X_synth = 2 * np.random.rand(100, 1) # Single feature\n",
    "y_synth = 4 + 3 * X_synth + np.random.randn(100, 1) # y = 4 + 3x + noise\n",
    "y_synth = y_synth.ravel() # Flatten y to a 1D array for easier handling\n",
    "\n",
    "# --- Feature Scaling (Important for Gradient Descent!) ---\n",
    "# Although for one feature it might still work without, it's good practice.\n",
    "# For this simple 1D case, X_synth is already in a decent range (0-2).\n",
    "# If X_synth was, e.g., 2000 * np.random.rand(100,1), scaling would be crucial.\n",
    "# Let's proceed without explicit scaling here to keep it simpler,\n",
    "# but acknowledge its importance for multi-feature or poorly scaled data.\n",
    "\n",
    "# Add a column of ones for the intercept term (b0)\n",
    "# X_b will be [x0, x1] where x0 is always 1\n",
    "X_b_synth = np.c_[np.ones((100, 1)), X_synth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7bf98b-a120-44c0-a5c3-e4bffef6a9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial b0 (intercept): 0.0130, Initial b1 (slope): 1.4535\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Parameters\n",
    "b_manual = np.random.randn(2) # b0, b1 (intercept, slope)\n",
    "print(f\"Initial b0 (intercept): {b_manual[0]:.4f}, Initial b1 (slope): {b_manual[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66303cbe-1604-4dd8-b9de-bb2f60b048c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Hyperparameters\n",
    "learning_rate = 0.1\n",
    "n_iterations = 1000\n",
    "n_samples = len(X_synth)\n",
    "\n",
    "cost_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ada1093-2853-4048-8fd8-857fb18d4d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: MSE = 31.0093, b0 = 1.1010, b1 = 2.5690\n",
      "Iteration 100: MSE = 0.8075, b0 = 4.1629, b1 = 2.8162\n",
      "Iteration 200: MSE = 0.8066, b0 = 4.2134, b1 = 2.7716\n",
      "Iteration 300: MSE = 0.8066, b0 = 4.2150, b1 = 2.7702\n",
      "Iteration 400: MSE = 0.8066, b0 = 4.2151, b1 = 2.7701\n",
      "Iteration 500: MSE = 0.8066, b0 = 4.2151, b1 = 2.7701\n",
      "Iteration 600: MSE = 0.8066, b0 = 4.2151, b1 = 2.7701\n",
      "Iteration 700: MSE = 0.8066, b0 = 4.2151, b1 = 2.7701\n",
      "Iteration 800: MSE = 0.8066, b0 = 4.2151, b1 = 2.7701\n",
      "Iteration 900: MSE = 0.8066, b0 = 4.2151, b1 = 2.7701\n",
      "\n",
      "After 1000 iterations:\n",
      "Learned b0 (intercept): 4.2151\n",
      "Learned b1 (slope): 2.7701\n"
     ]
    }
   ],
   "source": [
    "# 3. Gradient Descent Loop\n",
    "for iteration in range(n_iterations):\n",
    "    predictions = X_b_synth.dot(b_manual) # y_hat = X_b * b\n",
    "    errors = predictions - y_synth       # y_hat - y (sign depends on gradient formula used)\n",
    "                                         # If J = (y - y_hat)^2, gradient uses (y_hat - y)\n",
    "                                         # If J = (y_hat - y)^2, gradient uses (y_hat - y)\n",
    "                                         # Let's use J = 1/n * sum((y_hat - y)^2)\n",
    "                                         # dJ/db = 2/n * X_b.T * (X_b * b - y)\n",
    "    \n",
    "    gradients = (2/n_samples) * X_b_synth.T.dot(errors) # dJ/db\n",
    "    b_manual = b_manual - learning_rate * gradients   # Update rule\n",
    "\n",
    "    mse = np.mean(errors**2)\n",
    "    cost_history.append(mse)\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print(f\"Iteration {iteration}: MSE = {mse:.4f}, b0 = {b_manual[0]:.4f}, b1 = {b_manual[1]:.4f}\")\n",
    "\n",
    "print(f\"\\nAfter {n_iterations} iterations:\")\n",
    "print(f\"Learned b0 (intercept): {b_manual[0]:.4f}\")\n",
    "print(f\"Learned b1 (slope): {b_manual[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eff94bf-0b27-4295-b718-cc4b323f3346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot cost history\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(n_iterations), cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Cost Function (MSE) During Gradient Descent\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e5ddc0-744c-44fd-a96b-abbdc5164523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scikit-learn LinearRegression results:\n",
      "Intercept (b0): 4.2151\n",
      "Coefficient (b1): 2.7701\n"
     ]
    }
   ],
   "source": [
    "# Compare with Scikit-learn's LinearRegression (which uses Normal Equation)\n",
    "lr_sklearn = LinearRegression()\n",
    "lr_sklearn.fit(X_synth, y_synth) # X_synth is original single feature here\n",
    "print(\"\\nScikit-learn LinearRegression results:\")\n",
    "print(f\"Intercept (b0): {lr_sklearn.intercept_:.4f}\")\n",
    "print(f\"Coefficient (b1): {lr_sklearn.coef_[0]:.4f}\")\n",
    "\n",
    "# We expect our manual b0 and b1 to be close to Scikit-learn's results.\n",
    "# True values were b0=4, b1=3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2653c7b-4a61-480b-946b-43d8d30a9bd4",
   "metadata": {},
   "source": [
    "**Explanation of the Code:**\n",
    "* We generate simple data where the true intercept is 4 and the true slope is 3.\n",
    "* `X_b_synth` is created by adding a column of ones to our feature `X_synth`. This is a common trick so that the intercept $b_0$ can be treated as a coefficient for this \"dummy\" feature $x_0=1$, simplifying matrix calculations.\n",
    "* We initialize $b_0$ and $b_1$ randomly.\n",
    "* In each iteration:\n",
    "    * We calculate predictions using current $b_0, b_1$.\n",
    "    * We calculate errors.\n",
    "    * We compute the gradients for $b_0$ and $b_1$.\n",
    "    * We update $b_0$ and $b_1$ using the learning rate and gradients.\n",
    "    * We store the MSE to see how it decreases.\n",
    "* The plot of MSE over iterations should show a steep drop initially and then a plateau as it converges to the minimum.\n",
    "* The final `b_manual` values should be close to the true values (4 and 3) and also to what Scikit-learn's `LinearRegression` finds.\n",
    "\n",
    "**Effect of Learning Rate (Conceptual):**\n",
    "* If you set `learning_rate` to a very small value (e.g., 0.001), you'll see the cost decrease much slower, and it might not reach the minimum in 1000 iterations.\n",
    "* If you set `learning_rate` too high (e.g., 0.9 or 1.0 for this data), you might see the MSE bounce around erratically or even increase, indicating divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46e411-8384-4d1f-ac7b-8e095e58b83f",
   "metadata": {},
   "source": [
    "**6. Types of Gradient Descent (Brief Overview)**\n",
    "\n",
    "* **Batch Gradient Descent (what we implemented above):**\n",
    "    * Calculates the gradient using the *entire* training dataset in each iteration.\n",
    "    * Pros: Smooth, reliable convergence towards the global minimum (for convex functions).\n",
    "    * Cons: Can be very slow and memory-intensive if the dataset is massive, as all data must be processed for each tiny step.\n",
    "* **Stochastic Gradient Descent (SGD):**\n",
    "    * Updates the parameters using the gradient calculated from only *one randomly selected training sample* per iteration.\n",
    "    * Pros: Much faster iterations. Can escape shallow local minima due to noisy updates. Good for very large datasets and online learning (where data arrives sequentially).\n",
    "    * Cons: Updates are noisy, so the cost function will fluctuate more. Typically requires a gradually decreasing learning rate (learning rate schedule) to eventually settle near the minimum. May not converge to the exact minimum but gets very close.\n",
    "* **Mini-Batch Gradient Descent:**\n",
    "    * A compromise between Batch GD and SGD.\n",
    "    * Updates parameters using a small, randomly selected *batch* of training samples (e.g., 32, 64, 128 samples) per iteration.\n",
    "    * Pros: Balances the stability of Batch GD with the speed of SGD. More efficient computation than SGD due to vectorization over the mini-batch. Most commonly used in practice, especially in deep learning.\n",
    "    * Cons: Adds another hyperparameter (batch size)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e407521-7a8b-48bb-af61-61a1e661fb83",
   "metadata": {},
   "source": [
    "**7. Gradient Descent in Scikit-learn**\n",
    "* `sklearn.linear_model.LinearRegression`: Typically uses an analytical solver (like the Normal Equation via `scipy.linalg.lstsq`), not Gradient Descent. It's very efficient for datasets that fit in memory and don't have an astronomical number of features.\n",
    "* `sklearn.linear_model.SGDRegressor`: Implements linear regression (and other models) using Stochastic Gradient Descent.\n",
    "    * It's very useful for large datasets (supports out-of-core learning with the `partial_fit` method).\n",
    "    * **Requires feature scaling.**\n",
    "    * Has hyperparameters like `eta0` (initial learning rate), `learning_rate` schedule (e.g., 'constant', 'optimal', 'invscaling'), `max_iter`, `tol`.\n",
    "\n",
    "This covers the Cost Function (MSE) and Gradient Descent. These are foundational concepts for understanding how many machine learning algorithms are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174de7d8-2cde-4600-a28c-c0d698059abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
