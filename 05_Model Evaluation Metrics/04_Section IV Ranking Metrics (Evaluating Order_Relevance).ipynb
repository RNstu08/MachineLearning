{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b2625b-2a9a-4585-a8b6-557364cfb7b5",
   "metadata": {},
   "source": [
    "These metrics are crucial when your model's output is an *ordered list* of items, and the quality of this ordering is what you want to evaluate. This is common in:\n",
    "\n",
    "* **Information Retrieval:** Search engines ranking web pages.\n",
    "* **Recommendation Systems:** Recommending movies, products, or articles in a ranked order.\n",
    "* **Document Retrieval:** Finding relevant documents for a query from a large corpus.\n",
    "\n",
    "The key here is that simply predicting *if* an item is relevant isn't enough; its *position* in the ranked list matters greatly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890fc462-02f8-471e-9a2b-30b9a12264a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32f3599c-48ea-4717-8f37-8b764b2345f7",
   "metadata": {},
   "source": [
    "## 34. Mean Average Precision (MAP)\n",
    "\n",
    "MAP is a popular metric that evaluates the overall quality of a ranked list across multiple queries or users. To understand MAP, we first need to understand Precision@K and Average Precision (AP).\n",
    "\n",
    "### A. Precision@K (P@K)\n",
    "\n",
    "* **Concept:** Measures the precision (proportion of relevant items) within the top K items of a ranked list.\n",
    "* **Formula:**\n",
    "    $P@K = \\frac{\\text{Number of relevant items in the top K positions}}{K}$\n",
    "* **Interpretation:** A score between 0 and 1. $P@K = 0.6$ means 60% of the top K items presented were relevant.\n",
    "* **Pros:** Simple to calculate and interpret. Focuses on the quality of the very top results, which are often the most visible to users.\n",
    "* **Cons:**\n",
    "    * Ignores the ranking *within* the top K (a relevant item at rank 1 has the same impact as one at rank K).\n",
    "    * Doesn't consider recall (how many of *all* relevant items were found).\n",
    "    * Can be sensitive to the choice of K.\n",
    "* **Example:**\n",
    "    A search query returns a ranked list of 10 documents. Let 'R' denote a relevant document and 'N' an irrelevant one.\n",
    "    Ranked List: `[R, N, R, R, N, N, R, N, N, N]`\n",
    "    Total relevant documents for this query = 4 (at positions 1, 3, 4, 7).\n",
    "\n",
    "    * $P@1 = 1/1 = 1.0$ (1 relevant in top 1)\n",
    "    * $P@3 = 2/3 \\approx 0.67$ (2 relevant in top 3: R at 1, R at 3)\n",
    "    * $P@5 = 3/5 = 0.6$ (3 relevant in top 5: R at 1, R at 3, R at 4)\n",
    "    * $P@10 = 4/10 = 0.4$ (4 relevant in top 10)\n",
    "* **Implementation:** Usually calculated manually as part of AP.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834ad6d-8f63-4c90-b08a-6867008aebe8",
   "metadata": {},
   "source": [
    "\n",
    "### B. Average Precision (AP)\n",
    "\n",
    "* **Concept:** AP summarizes the precision-recall trade-off for a *single query* or user. It's the average of Precision@K values, but calculated *only at the ranks where a relevant item is found*. It rewards models that rank relevant items higher in the list.\n",
    "* **Formula:**\n",
    "    $AP = \\frac{\\sum_{k=1}^{N} (P@k \\times rel(k))}{\\text{Total number of relevant items for the query}}$\n",
    "    Where:\n",
    "    * $N$ is the total number of items in the ranked list for the query.\n",
    "    * $P@k$ is the Precision at cut-off $k$.\n",
    "    * $rel(k)$ is an indicator function: 1 if the item at rank $k$ is relevant, 0 otherwise.\n",
    "* **Interpretation:** A score between 0 and 1 for a single query. A higher AP means relevant items are generally ranked higher and found consistently. An AP of 1.0 means all relevant items were found and ranked at the very top of the list.\n",
    "* **Pros:**\n",
    "    * Considers the order of relevant items.\n",
    "    * Rewards finding many relevant items and placing them early.\n",
    "    * More stable and comprehensive than a single P@K value for a query.\n",
    "* **Cons:**\n",
    "    * Assumes binary relevance (an item is either relevant or not; no degrees of relevance).\n",
    "* **Example:**\n",
    "    Using the same ranked list: `[R, N, R, R, N, N, R, N, N, N]`\n",
    "    Relevant items are at ranks 1, 3, 4, 7. Total relevant items = 4.\n",
    "\n",
    "    1.  Item at rank 1 is **R**: $P@1 = 1/1 = 1.0$. $rel(1)=1$. Term = $1.0 \\times 1 = 1.0$.\n",
    "    2.  Item at rank 2 is N: $rel(2)=0$. Term = $P@2 \\times 0 = 0$.\n",
    "    3.  Item at rank 3 is **R**: $P@3 = 2/3 \\approx 0.667$. $rel(3)=1$. Term = $0.667 \\times 1 \\approx 0.667$.\n",
    "    4.  Item at rank 4 is **R**: $P@4 = 3/4 = 0.75$. $rel(4)=1$. Term = $0.75 \\times 1 = 0.75$.\n",
    "    5.  Item at rank 5 is N: $rel(5)=0$. Term = $0$.\n",
    "    6.  Item at rank 6 is N: $rel(6)=0$. Term = $0$.\n",
    "    7.  Item at rank 7 is **R**: $P@7 = 4/7 \\approx 0.571$. $rel(7)=1$. Term = $0.571 \\times 1 \\approx 0.571$.\n",
    "    (No more relevant items)\n",
    "\n",
    "    $AP = \\frac{(1.0 \\times 1) + (0 \\times P@2) + (0.667 \\times 1) + (0.75 \\times 1) + (0 \\times P@5) + (0 \\times P@6) + (0.571 \\times 1)}{\\text{4}}$\n",
    "    $AP = \\frac{1.0 + 0.667 + 0.75 + 0.571}{4} = \\frac{2.988}{4} \\approx 0.747$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a67afe01-6013-4073-b57f-70809d70a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Implementation (Conceptual Manual Python Calculation):**\n",
    "\n",
    "# Scikit-learn's `average_precision_score` is designed for binary classification true labels and continuous scores. For AP in ranking from an already ranked list, manual calculation is clearer:\n",
    "import sklearn\n",
    "def calculate_ap(ranked_relevance_list, total_relevant_docs):\n",
    "# \"\"\"\n",
    "# Calculates Average Precision (AP) for a single query.\n",
    "# ranked_relevance_list: list of 0s and 1s, where 1 means relevant.\n",
    "# total_relevant_docs: total number of truly relevant docs for this query.\n",
    "# \"\"\"\n",
    "    if total_relevant_docs == 0:\n",
    "        return 0.0\n",
    "\n",
    "    hits = 0\n",
    "    sum_precision_at_k = 0.0\n",
    "    for k, is_relevant in enumerate(ranked_relevance_list):\n",
    "        if is_relevant:\n",
    "            hits += 1\n",
    "            precision_at_k = hits / (k + 1)\n",
    "            sum_precision_at_k += precision_at_k\n",
    "\n",
    "    return sum_precision_at_k / total_relevant_docs\n",
    "\n",
    "# Example from above\n",
    "ranked_relevance = [1, 0, 1, 1, 0, 0, 1, 0, 0, 0] # R=1, N=0\n",
    "total_relevant = 4\n",
    "ap = calculate_ap(ranked_relevance, total_relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc1796-8594-46df-9066-6b85df7ab2a9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501338b-d84b-4d12-ab91-8d521ad4070e",
   "metadata": {},
   "source": [
    "### C. Mean Average Precision (MAP)\n",
    "\n",
    "* **Concept:** The MAP is the mean of Average Precision (AP) scores calculated over a set of multiple queries or users. It provides a single figure measure of quality across all queries.\n",
    "* **Formula:**\n",
    "    $MAP = \\frac{1}{|Q|} \\sum_{q=1}^{|Q|} AP_q$\n",
    "    Where:\n",
    "    * $|Q|$ is the total number of queries.\n",
    "    * $AP_q$ is the Average Precision for query $q$.\n",
    "* **Interpretation:** A score between 0 and 1. A higher MAP indicates better overall ranking performance across the entire set of queries. An MAP of 0.75 means that, on average, a query's ranked list scores 0.75 on AP.\n",
    "* **Pros:**\n",
    "    * Provides a single, comprehensive metric for evaluating ranking systems over multiple queries.\n",
    "    * Widely used and understood in information retrieval and recommendation literature.\n",
    "    * Penalizes systems that perform poorly on many queries.\n",
    "* **Cons:**\n",
    "    * Still assumes binary relevance (relevant/not relevant).\n",
    "    * Can be influenced by the number of relevant documents per query. Queries with very few relevant documents might have unstable AP scores.\n",
    "* **Example:**\n",
    "    Suppose we have AP scores for 3 queries:\n",
    "    * Query 1 AP: 0.747\n",
    "    * Query 2 AP: 0.500\n",
    "    * Query 3 AP: 0.950 (Corrected from previous example's calculation)\n",
    "    $MAP = \\frac{0.747 + 0.500 + 0.950}{3} = \\frac{2.197}{3} \\approx 0.732$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d25170a1-6552-44f0-aa4a-095980ee41ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP Scores: [0.747, 0.5, 0.95]\n",
      "MAP: 0.732\n"
     ]
    }
   ],
   "source": [
    "# Implementation:** There's no direct `map_score` in scikit-learn. It involves calculating AP for each query and then averaging these AP scores.\n",
    "\n",
    "# Continuing from the AP example:\n",
    "ap_scores = []\n",
    "\n",
    "# Query 1\n",
    "ranked_relevance_q1 = [1, 0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
    "total_relevant_q1 = 4\n",
    "ap_scores.append(calculate_ap(ranked_relevance_q1, total_relevant_q1)) # Expected 0.747\n",
    "\n",
    "# Query 2 (example data)\n",
    "ranked_relevance_q2 = [0, 1, 0, 1, 0] # Relevant at rank 2 and 4\n",
    "total_relevant_q2 = 2\n",
    "# P@2 (1st relevant) = 1/2 = 0.5\n",
    "# P@4 (2nd relevant) = 2/4 = 0.5\n",
    "# AP = (0.5 + 0.5) / 2 = 0.5\n",
    "ap_scores.append(calculate_ap(ranked_relevance_q2, total_relevant_q2)) # Expected 0.5\n",
    "\n",
    "# Query 3 (example data)\n",
    "ranked_relevance_q3 = [1, 1, 1, 0, 1] # Relevant at 1, 2, 3, 5\n",
    "total_relevant_q3 = 4\n",
    "# P@1 (1st relevant) = 1/1 = 1.0\n",
    "# P@2 (2nd relevant) = 2/2 = 1.0\n",
    "# P@3 (3rd relevant) = 3/3 = 1.0\n",
    "# P@5 (4th relevant) = 4/5 = 0.8\n",
    "# AP = (1.0 + 1.0 + 1.0 + 0.8) / 4 = 3.8 / 4 = 0.95\n",
    "ap_scores.append(calculate_ap(ranked_relevance_q3, total_relevant_q3)) # Expected 0.95\n",
    "\n",
    "if ap_scores:\n",
    "    map_score = sum(ap_scores) / len(ap_scores)\n",
    "    print(f\"AP Scores: {[round(s, 3) for s in ap_scores]}\")\n",
    "    print(f\"MAP: {map_score:.3f}\") # Expected MAP: (0.747 + 0.5 + 0.95) / 3 = 0.732\n",
    "else:\n",
    "    print(\"No AP scores to calculate MAP.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f02c87-5483-4179-afa7-6d39b428a986",
   "metadata": {},
   "source": [
    "* **Context:** MAP is a standard evaluation metric in academic information retrieval (e.g., TREC conferences) and is often used to compare search algorithms or recommendation models that produce ranked lists based on binary relevance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483aa75e-7297-4def-a63d-a666a7685bfd",
   "metadata": {},
   "source": [
    "## 35. Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "NDCG is designed to evaluate rankings where items have *graded relevance* (e.g., not just relevant/irrelevant, but \"highly relevant,\" \"somewhat relevant,\" \"irrelevant\"). It also emphasizes placing highly relevant items at the top of the list. We need to understand CG, DCG, and IDCG first.\n",
    "\n",
    "### A. Cumulative Gain (CG@K)\n",
    "\n",
    "* **Concept:** The sum of the relevance scores of the items in the top K positions of a ranked list.\n",
    "* **Formula:**\n",
    "    $CG@K = \\sum_{i=1}^{K} rel_i$\n",
    "    Where $rel_i$ is the graded relevance score of the item at rank $i$.\n",
    "* **Interpretation:** The total relevance accumulated within the top K items. Higher CG@K indicates more relevant items are in the top K.\n",
    "* **Pros:** Simple to understand and compute. Handles graded relevance scores.\n",
    "* **Cons:** Ignores the position of items within the top K. A highly relevant item at rank K contributes the same as if it were at rank 1.\n",
    "* **Example:**\n",
    "    Ranked list of documents with graded relevance scores (0=irrelevant, 1=somewhat, 2=relevant, 3=highly relevant):\n",
    "    Relevance scores in ranked order: `rel = [3, 0, 2, 2, 1]`\n",
    "\n",
    "    * $CG@1 = 3$\n",
    "    * $CG@3 = 3 + 0 + 2 = 5$\n",
    "    * $CG@5 = 3 + 0 + 2 + 2 + 1 = 8$\n",
    "* **Implementation:** Manual sum.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4defb4-e32b-4c60-898a-273ccb0358f9",
   "metadata": {},
   "source": [
    "### B. Discounted Cumulative Gain (DCG@K)\n",
    "\n",
    "* **Concept:** Similar to CG, but it applies a logarithmic discount to the relevance scores based on their rank. Relevant items appearing lower in the list contribute less to the DCG.\n",
    "* **Formula:**\n",
    "    $DCG@K = \\sum_{i=1}^{K} \\frac{rel_i}{\\log_2(i+1)}$\n",
    "    (Scikit-learn uses this formula where the discount $\\log_2(i+1)$ means $\\log_2(rank+1)$ effectively.)\n",
    "* **Interpretation:** The total discounted relevance accumulated in the top K items. Higher DCG@K is better.\n",
    "* **Pros:**\n",
    "    * Values item position: Highly relevant items are rewarded more if ranked higher.\n",
    "    * Handles graded relevance.\n",
    "* **Cons:**\n",
    "    * Absolute DCG values are not easily comparable across different queries or different K.\n",
    "    * Not normalized.\n",
    "* **Example:**\n",
    "    Using `rel = [3, 0, 2, 2, 1]`\n",
    "    Discount factors $\\log_2(i+1)$: $\\log_2(2)=1$, $\\log_2(3)\\approx1.585$, $\\log_2(4)=2$, $\\log_2(5)\\approx2.322$, $\\log_2(6)\\approx2.585$.\n",
    "\n",
    "    * $DCG@1 = 3 / 1 = 3.0$\n",
    "    * $DCG@3 = (3/1) + (0/1.585) + (2/2) = 3 + 0 + 1 = 4.0$\n",
    "    * $DCG@5 = (3/1) + (0/1.585) + (2/2) + (2/2.322) + (1/2.585) \\approx 3 + 0 + 1 + 0.861 + 0.387 \\approx 5.248$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "662ce736-845f-4007-89cf-41eace89fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation (Scikit-learn `dcg_score`):**\n",
    "\n",
    "from sklearn.metrics import dcg_score\n",
    "import numpy as np\n",
    "\n",
    "# True relevance scores in the order they were ranked by the model\n",
    "true_relevance_ranked_by_model = np.asarray([[3, 0, 2, 2, 1]]) # Needs 2D array\n",
    "\n",
    "# print(f\"DCG@1: {dcg_score(true_relevance_ranked_by_model, k=1):.3f}\")\n",
    "# print(f\"DCG@3: {dcg_score(true_relevance_ranked_by_model, k=3):.3f}\")\n",
    "# print(f\"DCG@5: {dcg_score(true_relevance_ranked_by_model, k=5):.3f}\")\n",
    "# Expected Outputs: DCG@1: 3.000, DCG@3: 4.000, DCG@5: 5.248"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89010bd6-0b6d-4824-b463-59084a37b0bf",
   "metadata": {},
   "source": [
    "### C. Ideal Discounted Cumulative Gain (IDCG@K)\n",
    "\n",
    "* **Concept:** The DCG score of a \"perfect\" or ideal ranking for the top K items. This is achieved by taking all known items for a query, sorting them by their true relevance scores in descending order, and then calculating DCG@K on this ideal list.\n",
    "* **Formula:** Same as DCG@K, but applied to the ideally ranked relevance scores:\n",
    "    $IDCG@K = \\sum_{i=1}^{K} \\frac{rel_i^{ideal}}{\\log_2(i+1)}$\n",
    "* **Interpretation:** The maximum possible DCG@K for a given query and set of documents at cut-off K.\n",
    "* **Pros:** Provides the normalization factor needed for NDCG.\n",
    "* **Cons:** Requires knowing all true relevance scores for all documents relevant to a query to construct the ideal list.\n",
    "* **Example:**\n",
    "    Suppose for our query, the full set of available items and their true relevance scores are: DocA (3), DocB (0), DocC (2), DocD (2), DocE (1), DocF(3).\n",
    "    Sorted ideally: `[3, 3, 2, 2, 1, 0]`\n",
    "    Ideal relevance list for top 5: `ideal_rel = [3, 3, 2, 2, 1]`\n",
    "\n",
    "    * $IDCG@1 = 3 / 1 = 3.0$\n",
    "    * $IDCG@3 = (3/1) + (3/1.585) + (2/2) \\approx 3 + 1.893 + 1 = 5.893$\n",
    "    * $IDCG@5 = (3/1) + (3/1.585) + (2/2) + (2/2.322) + (1/2.585) \\approx 3 + 1.893 + 1 + 0.861 + 0.387 \\approx 7.141$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb276f9c-6a89-43ad-a0d1-6026497349ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Implementation:** Calculate DCG on the ideally sorted list of true relevance scores.\n",
    "\n",
    "# Full set of true relevances for all docs for the query, then sorted.\n",
    "ideal_true_relevance_scores = np.asarray([[3, 3, 2, 2, 1]]) # Sorted ideally\n",
    "\n",
    "    # print(f\"IDCG@1: {dcg_score(ideal_true_relevance_scores, k=1):.3f}\")\n",
    "    # print(f\"IDCG@3: {dcg_score(ideal_true_relevance_scores, k=3):.3f}\")\n",
    "    # print(f\"IDCG@5: {dcg_score(ideal_true_relevance_scores, k=5):.3f}\")\n",
    "    # Expected Outputs: IDCG@1: 3.000, IDCG@3: 5.893, IDCG@5: 7.141"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a1887-7af0-4b39-aca7-c1f0021c8e0a",
   "metadata": {},
   "source": [
    "### D. Normalized Discounted Cumulative Gain (NDCG@K)\n",
    "\n",
    "* **Concept:** DCG@K normalized by IDCG@K. This scales the DCG score to a range between 0 and 1, making it comparable across different queries and different numbers of results.\n",
    "* **Formula:**\n",
    "    $NDCG@K = \\frac{DCG@K}{IDCG@K}$\n",
    "    (If IDCG@K is 0, NDCG@K is typically defined as 0.)\n",
    "* **Interpretation:** A score between 0 and 1.\n",
    "    * NDCG@K = 1 means the model's ranking is identical to the ideal ranking for the top K items.\n",
    "    * NDCG@K = 0 means the model found no relevant items in the top K (or DCG@K was 0).\n",
    "    * Higher values indicate better ranking performance.\n",
    "* **Pros:**\n",
    "    * Considers both the relevance of items and their position in the list.\n",
    "    * Handles graded relevance scores.\n",
    "    * **Normalized:** Allows for fair comparison across queries.\n",
    "    * Widely used and robust.\n",
    "* **Cons:**\n",
    "    * Can be more complex to calculate manually.\n",
    "    * IDCG calculation requires knowing the complete set of relevance judgments for a query.\n",
    "* **Example:**\n",
    "    Using our calculated values:\n",
    "    * $DCG@3 = 4.0$, $IDCG@3 = 5.893$ (based on global ideal `[3,3,2]`)\n",
    "        $NDCG@3 = 4.0 / 5.893 \\approx 0.679$\n",
    "    * $DCG@5 = 5.248$, $IDCG@5 = 7.141$ (based on global ideal `[3,3,2,2,1]`)\n",
    "        $NDCG@5 = 5.248 / 7.141 \\approx 0.735$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ad9d311-851a-4d1e-8dd0-f427e9c33e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Implementation (Scikit-learn `ndcg_score`):**\n",
    "# `ndcg_score` calculates DCG for `y_true` based on the order induced by `y_score`. For IDCG, it sorts `y_true` by its own values to get the ideal order of those specific items.\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "\n",
    "    # Case 1: Manual calculation using pre-computed DCG and IDCG\n",
    "dcg_at_5 = 5.248\n",
    "idcg_at_5_global = 7.141 # Based on a globally defined ideal list\n",
    "ndcg_at_5_manual = dcg_at_5 / idcg_at_5_global if idcg_at_5_global > 0 else 0.0\n",
    "    # print(f\"Manual NDCG@5 (using global IDCG): {ndcg_at_5_manual:.3f}\") # Expected: 0.735\n",
    "\n",
    "    # Case 2: Using sklearn.metrics.ndcg_score\n",
    "    # y_true are the true relevances of the items.\n",
    "    # y_score are the scores given by the model to these items.\n",
    "    # Example:\n",
    "    # True relevances of items in their original (e.g., database) order:\n",
    "true_relevances_all = np.asarray([[3, 0, 2, 2, 1, 3]]) # DocA, DocB, DocC, DocD, DocE, DocF\n",
    "    # Model's scores for these items:\n",
    "model_prediction_scores = np.asarray([[0.9, 0.1, 0.7, 0.6, 0.3, 0.8]])\n",
    "    # Based on these scores, model would rank: A(3), F(3), C(2), D(2), E(1), B(0)\n",
    "    # Model's ranked true relevances: [3, 3, 2, 2, 1, 0]\n",
    "\n",
    "    # ndcg_score sorts y_true by y_score to get DCG,\n",
    "    # and sorts y_true by its own values to get IDCG.\n",
    "    # print(f\"Sklearn NDCG@5: {ndcg_score(true_relevances_all, model_prediction_scores, k=5):.3f}\")\n",
    "    # This will use the top 5 items from true_relevances_all based on model_prediction_scores\n",
    "    # For DCG: items with scores 0.9,0.8,0.7,0.6,0.3 -> relevances [3 (A), 3 (F), 2 (C), 2 (D), 1 (E)]\n",
    "    # DCG@5 for [3,3,2,2,1] is 7.141 (from our IDCG@5 example)\n",
    "    # For IDCG: it sorts true_relevances_all = [3,0,2,2,1,3] to [3,3,3,2,2,1] and takes top 5: [3,3,3,2,2]\n",
    "    # IDCG@5 for [3,3,3,2,2] is 3/1 + 3/1.585 + 3/2 + 2/2.322 + 2/2.585\n",
    "    # = 3 + 1.893 + 1.5 + 0.861 + 0.774 = 8.028\n",
    "    # Sklearn NDCG@5 = 7.141 / 8.028 = 0.889 (approximately)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea44212-6351-4d3e-b9b3-c46832f913ce",
   "metadata": {},
   "source": [
    "**Note on `ndcg_score`:** If you provide `y_true` as the actual relevance scores of the items in the order they were presented by your model, and `y_score` as dummy scores that reflect this order (e.g., `[num_items, num_items-1, ..., 1]`), then the DCG part will be correct. However, the IDCG part will be based on sorting *only those presented items*. If the ideal ranking involves items *not* presented by your model (but known for the query), `ndcg_score` might not give the globally correct NDCG. In such cases, calculate DCG (for model's list) and IDCG (for global ideal list) separately and then divide.\n",
    "\n",
    "* **Context:** NDCG is one of the most popular and robust metrics for evaluating ranked search results and recommendation lists, especially when relevance is not binary. It's often the primary metric in academic research and industry benchmarks for ranking tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary for Ranking Metrics:\n",
    "\n",
    "* **MAP** is excellent for tasks with **binary relevance** judgments, where the order of relevant items matters significantly. It averages performance across multiple queries.\n",
    "* **NDCG** is preferred for tasks with **graded relevance** judgments (e.g., 0-5 stars) and where the position of highly relevant items is paramount.\n",
    "\n",
    "Both metrics are more complex than simple accuracy or precision but provide a much more nuanced view of ranking quality. The choice depends on the nature of your relevance data and the specific aspects of ranking performance you want to prioritize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d05e5a-e63c-4dbf-866c-183e2aa6768f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
