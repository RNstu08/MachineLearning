{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82cc88c6-2e94-4589-a0ba-86ae0c31cac3",
   "metadata": {},
   "source": [
    "### X. Further Topics\n",
    "\n",
    "This section briefly covers additional important topics related to `Scikit-learn` and the broader machine learning ecosystem.\n",
    "\n",
    "#### 1. Handling Imbalanced Datasets\n",
    "\n",
    "* **Problem:** In many real-world classification problems (e.g., fraud detection, medical diagnosis), one class is much rarer than others. Standard accuracy can be misleading, and models might learn to simply predict the majority class.\n",
    "* **Techniques (Brief Overview):**\n",
    "    * **Resampling:**\n",
    "        * Oversampling: Duplicating samples from the minority class (e.g., using `SMOTE` - Synthetic Minority Over-sampling Technique, often found in the `imbalanced-learn` library).\n",
    "        * Undersampling: Removing samples from the majority class.\n",
    "    * **Class Weighting:** Many `Scikit-learn` classifiers (like `LogisticRegression`, `SVC`) have a `class_weight='balanced'` parameter that automatically adjusts weights inversely proportional to class frequencies.\n",
    "    * **Different Metrics:** Focus on metrics less sensitive to imbalance, like Precision, Recall, F1-score (especially for the minority class), AUC-ROC, or Precision-Recall AUC.\n",
    "* **Library:** The `imbalanced-learn` library provides implementations for many resampling techniques compatible with `Scikit-learn`.\n",
    "\n",
    "#### 2. Feature Importance\n",
    "\n",
    "* **Concept:** Understanding which features contribute most to a model's predictions. This helps interpret the model and potentially perform feature selection.\n",
    "* **Accessing Importance:**\n",
    "    * **Tree-based models** (like `DecisionTreeClassifier`, `RandomForestClassifier`, `GradientBoostingClassifier`): Have a `feature_importances_` attribute after fitting, indicating the relative importance (often based on impurity reduction) of each feature.\n",
    "    * **Linear models** (like `LinearRegression`, `LogisticRegression`, `Lasso`, `Ridge`): Have a `coef_` attribute. The magnitude of the coefficients (after scaling features) can indicate feature importance (larger absolute value = more importance). For `Lasso`, non-zero coefficients indicate selected features.\n",
    "    * **Permutation Importance:** A model-agnostic technique (`sklearn.inspection.permutation_importance`) that measures the decrease in model score when a single feature's values are randomly shuffled.\n",
    "\n",
    "#### 3. Partial Fit (Incremental/Online Learning)\n",
    "\n",
    "* **Concept:** For datasets that are too large to fit into memory at once, some `Scikit-learn` estimators support incremental learning via the `partial_fit` method. You can train the model on mini-batches of data sequentially.\n",
    "* **Estimators:** Look for estimators that implement `partial_fit` in their documentation (e.g., `SGDClassifier`, `SGDRegressor`, `PassiveAggressiveClassifier`, `MultinomialNB`, `MiniBatchKMeans`).\n",
    "\n",
    "#### 4. Beyond Scikit-learn: Other ML Libraries\n",
    "\n",
    "While `Scikit-learn` is fantastic for general-purpose ML, other libraries excel in specific areas:\n",
    "\n",
    "* **Gradient Boosting Machines** (often outperform Random Forests):\n",
    "    * `XGBoost`: Highly optimized, widely used in competitions. Offers speed and performance advantages.\n",
    "    * `LightGBM`: Another fast, high-performance gradient boosting framework, particularly efficient with large datasets.\n",
    "    * `CatBoost`: Handles categorical features directly and often performs well with default parameters.\n",
    "    * (These libraries often have `Scikit-learn` compatible wrappers).\n",
    "* **Deep Learning:** For tasks involving complex patterns in unstructured data (images, text, audio), deep learning frameworks are the standard:\n",
    "    * `TensorFlow`: Developed by Google, extensive ecosystem (`Keras` is its high-level API).\n",
    "    * `PyTorch`: Developed by Facebook AI Research, known for its Pythonic feel and flexibility in research.\n",
    "    * (`Scikit-learn` is often still used for preprocessing or evaluating results alongside these frameworks).\n",
    "* **Statistical Modeling:**\n",
    "    * `Statsmodels`: Focuses more on traditional statistical modeling, inference, and hypothesis testing, providing detailed statistical summaries (p-values, confidence intervals) not always available in `Scikit-learn`.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "`Scikit-learn` provides a robust and comprehensive foundation for most traditional machine learning tasks. Understanding these further topics and knowing when to explore specialized libraries like `XGBoost`, `TensorFlow`, or `PyTorch` will equip you for a wider range of data science challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a2c82-84c6-4256-af9e-cb6c691e74f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
