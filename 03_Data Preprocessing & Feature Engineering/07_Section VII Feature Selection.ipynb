{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c7059b-b774-4f03-b139-05ce100243a6",
   "metadata": {},
   "source": [
    "After potentially creating many new features during feature engineering, or if your initial dataset has a large number of features, you might want to select only the most relevant ones. Feature selection aims to choose a subset of the original features that are most useful for predicting the target variable.\n",
    "\n",
    "**Goals of Feature Selection:**\n",
    "\n",
    "* **Improve Model Performance:** Reduce overfitting by removing irrelevant or redundant features (noise).\n",
    "* **Reduce Training Time:** Fewer features mean models train faster.\n",
    "* **Enhance Interpretability:** Simpler models with fewer features are often easier to understand.\n",
    "* **Reduce Dimensionality:** Mitigate the \"curse of dimensionality\".\n",
    "\n",
    "`Scikit-learn` provides several methods, broadly categorized as Filter, Wrapper, and Embedded methods.\n",
    "\n",
    "## Feature Selection Techniques\n",
    "\n",
    "This document covers:\n",
    "\n",
    "* **Goal:** Explains why feature selection is important (improving performance, reducing complexity, etc.).\n",
    "* **Filter Methods:** Demonstrates `VarianceThreshold` (removing low/zero variance features) and `SelectKBest` (using univariate statistical tests like `f_classif` or `f_regression`).\n",
    "* **Wrapper Methods:** Shows `RFE` (Recursive Feature Elimination) using a model's coefficients or importances to iteratively select features. Mentions `RFECV` for automatic selection of the number of features.\n",
    "* **Embedded Methods:** Illustrates how L1 regularization (`Lasso` for regression) inherently performs feature selection by shrinking some coefficients to zero, and how tree-based models (`RandomForestClassifier`) provide `feature_importances_`. Mentions `SelectFromModel`.\n",
    "* **Considerations:** Emphasizes fitting selectors only on training data and the pros/cons of different method categories.\n",
    "\n",
    "---\n",
    "\n",
    "Choosing the right features can significantly impact your model's success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fffbab6a-202c-40cd-b051-9cb60bcc1708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data ---\n",
      "Iris Dataset Features:\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      "\n",
      "Housing Dataset Features (with added noise):\n",
      "    MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  NoisyFeature  \n",
      "0    -122.23      0.049671  \n",
      "1    -122.22     -0.013826  \n",
      "2    -122.24      0.064769  \n",
      "3    -122.25      0.152303  \n",
      "4    -122.25     -0.023415  \n",
      "------------------------------\n",
      "--- Filter Methods ---\n",
      "\n",
      "--- a) Variance Threshold ---\n",
      "Shape before VarianceThreshold: (14448, 10)\n",
      "Features kept after VarianceThreshold(0): ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'NoisyFeature']\n",
      "Shape after VarianceThreshold(0): (14448, 9)\n",
      "--------------------\n",
      "\n",
      "--- b) Univariate Selection (SelectKBest) ---\n",
      "\n",
      "Iris feature scores (f_classif): [ 95.76  33.06 814.73 791.35]\n",
      "Top 2 Iris features selected by SelectKBest: ['petal length (cm)', 'petal width (cm)']\n",
      "Shape after SelectKBest(k=2): (105, 2)\n",
      "\n",
      "Top 5 Housing features selected by SelectKBest (f_regression): ['MedInc', 'HouseAge', 'AveRooms', 'Latitude', 'Longitude']\n",
      "------------------------------\n",
      "--- Wrapper Methods ---\n",
      "\n",
      "--- a) Recursive Feature Elimination (RFE) ---\n",
      "Features selected by RFE (Logistic Regression, k=2): ['petal length (cm)', 'petal width (cm)']\n",
      "Feature ranking (lower is better): [3 2 1 1]\n",
      "Shape after RFE: (105, 2)\n",
      "\n",
      "(RFECV automatically finds the best number of features using CV - see commented code)\n",
      "------------------------------\n",
      "--- Embedded Methods ---\n",
      "\n",
      "--- a) L1 Regularization (Lasso) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\learning\\Machine_learning\\Pandas\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SelectKBest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Coefficients:\n",
      " MedInc          0.738636\n",
      "HouseAge        0.138543\n",
      "AveRooms       -0.000000\n",
      "AveBedrms       0.000000\n",
      "Population      0.000000\n",
      "AveOccup       -0.000000\n",
      "Latitude       -0.263678\n",
      "Longitude      -0.222547\n",
      "NoisyFeature   -0.000000\n",
      "dtype: float64\n",
      "\n",
      "Features selected by Lasso (non-zero coefs): ['MedInc', 'HouseAge', 'Latitude', 'Longitude']\n",
      "\n",
      "--- b) Tree-based Feature Importance ---\n",
      "Feature Importances from RandomForest:\n",
      "              Feature  Importance\n",
      "3   petal width (cm)    0.454892\n",
      "2  petal length (cm)    0.400227\n",
      "0  sepal length (cm)    0.120608\n",
      "1   sepal width (cm)    0.024273\n",
      "\n",
      "(SelectFromModel can automatically select based on importance - see commented code)\n",
      "------------------------------\n",
      "--- Final Considerations ---\n",
      "- Feature selection should generally be done *after* train-test split.\n",
      "- Fit selectors/models ONLY on the training data.\n",
      "- Transform both training and test sets using the *same* fitted selector.\n",
      "- Filter methods are fast but don't consider feature interactions.\n",
      "- Wrapper methods are more thorough but computationally expensive.\n",
      "- Embedded methods offer a balance, integrating selection into training.\n",
      "- The best method depends on the dataset, model, and goals.\n",
      "- Often beneficial to include feature selection within a Pipeline, especially when using cross-validation.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler # For scaling before some methods\n",
    "# Filter Methods\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression, mutual_info_classif\n",
    "# Wrapper Methods\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.linear_model import LogisticRegression, Lasso # Lasso is also embedded\n",
    "# Embedded Methods\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# For visualization\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "print(\"--- Loading Data ---\")\n",
    "# Classification Example: Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y_iris = iris.target\n",
    "print(\"Iris Dataset Features:\\n\", X_iris.head())\n",
    "\n",
    "# Regression Example: California Housing\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "# Add a noisy feature for demonstration\n",
    "np.random.seed(42)\n",
    "X_housing['NoisyFeature'] = np.random.randn(X_housing.shape[0]) * 0.1\n",
    "print(\"\\nHousing Dataset Features (with added noise):\\n\", X_housing.head())\n",
    "\n",
    "# Split data (important to fit selectors only on training data)\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris)\n",
    "X_housing_train, X_housing_test, y_housing_train, y_housing_test = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale data (needed for some models used in selection, like Lasso, RFE with linear models)\n",
    "scaler_housing = StandardScaler()\n",
    "X_housing_train_scaled = scaler_housing.fit_transform(X_housing_train)\n",
    "X_housing_test_scaled = scaler_housing.transform(X_housing_test)\n",
    "# Convert back to DataFrame for clarity\n",
    "X_housing_train_scaled = pd.DataFrame(X_housing_train_scaled, columns=X_housing.columns, index=X_housing_train.index)\n",
    "\n",
    "scaler_iris = StandardScaler()\n",
    "X_iris_train_scaled = scaler_iris.fit_transform(X_iris_train)\n",
    "X_iris_test_scaled = scaler_iris.transform(X_iris_test)\n",
    "X_iris_train_scaled = pd.DataFrame(X_iris_train_scaled, columns=X_iris.columns, index=X_iris_train.index)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 2. Filter Methods ---\n",
    "# Select features based on statistical properties, independent of any specific model.\n",
    "# Fast and computationally inexpensive.\n",
    "\n",
    "print(\"--- Filter Methods ---\")\n",
    "\n",
    "# a) Variance Threshold: Remove features with low variance.\n",
    "print(\"\\n--- a) Variance Threshold ---\")\n",
    "# Remove features with zero variance (constant features) - default threshold=0\n",
    "# Add a constant feature for demonstration\n",
    "X_housing_train_const = X_housing_train_scaled.copy()\n",
    "X_housing_train_const['ConstantFeat'] = 0\n",
    "print(f\"Shape before VarianceThreshold: {X_housing_train_const.shape}\")\n",
    "\n",
    "selector_var = VarianceThreshold(threshold=0.0) # Removes constant features\n",
    "selector_var.fit(X_housing_train_const) # Fit on training data\n",
    "\n",
    "# Get boolean mask of features to keep\n",
    "features_to_keep_mask = selector_var.get_support()\n",
    "kept_features = X_housing_train_const.columns[features_to_keep_mask]\n",
    "print(f\"Features kept after VarianceThreshold(0): {list(kept_features)}\")\n",
    "\n",
    "# Transform data (selects the columns)\n",
    "X_train_high_variance = selector_var.transform(X_housing_train_const)\n",
    "print(f\"Shape after VarianceThreshold(0): {X_train_high_variance.shape}\")\n",
    "# Can also set a higher threshold to remove quasi-constant features\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "# b) Univariate Selection: Select features based on statistical tests against the target variable.\n",
    "print(\"\\n--- b) Univariate Selection (SelectKBest) ---\")\n",
    "# SelectKBest: Selects the top 'k' features.\n",
    "# SelectPercentile: Selects the top 'percentile' features.\n",
    "# Common scoring functions:\n",
    "# - For Regression: f_regression, mutual_info_regression\n",
    "# - For Classification: f_classif (ANOVA F-value), chi2 (for non-negative features), mutual_info_classif\n",
    "\n",
    "# Example: Select top 2 features for Iris classification using f_classif\n",
    "k_best = 2\n",
    "selector_kbest_iris = SelectKBest(score_func=f_classif, k=k_best)\n",
    "selector_kbest_iris.fit(X_iris_train_scaled, y_iris_train) # Fit on training data\n",
    "\n",
    "# Get scores and selected features\n",
    "scores_iris = selector_kbest_iris.scores_\n",
    "selected_indices_iris = selector_kbest_iris.get_support(indices=True)\n",
    "selected_features_iris = X_iris_train.columns[selected_indices_iris]\n",
    "\n",
    "print(f\"\\nIris feature scores (f_classif): {scores_iris.round(2)}\")\n",
    "print(f\"Top {k_best} Iris features selected by SelectKBest: {list(selected_features_iris)}\")\n",
    "\n",
    "# Transform data to keep only selected features\n",
    "X_train_iris_kbest = selector_kbest_iris.transform(X_iris_train_scaled)\n",
    "X_test_iris_kbest = selector_kbest_iris.transform(X_iris_test_scaled) # Use same fitted selector\n",
    "print(f\"Shape after SelectKBest(k=2): {X_train_iris_kbest.shape}\")\n",
    "\n",
    "# Example: Select top 5 features for Housing regression using f_regression\n",
    "k_best_reg = 5\n",
    "selector_kbest_housing = SelectKBest(score_func=f_regression, k=k_best_reg)\n",
    "selector_kbest_housing.fit(X_housing_train_scaled, y_housing_train)\n",
    "selected_features_housing = X_housing_train.columns[selector_kbest_housing.get_support(indices=True)]\n",
    "print(f\"\\nTop {k_best_reg} Housing features selected by SelectKBest (f_regression): {list(selected_features_housing)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 3. Wrapper Methods ---\n",
    "# Use a specific machine learning model to evaluate the usefulness of feature subsets.\n",
    "# More computationally expensive than filter methods but can lead to better performance.\n",
    "\n",
    "print(\"--- Wrapper Methods ---\")\n",
    "\n",
    "# a) Recursive Feature Elimination (RFE)\n",
    "# Iteratively trains a model, removes the least important feature(s), and repeats.\n",
    "# Requires an estimator with `coef_` or `feature_importances_`.\n",
    "print(\"\\n--- a) Recursive Feature Elimination (RFE) ---\")\n",
    "\n",
    "# Example: Use RFE with Logistic Regression to select 2 features for Iris\n",
    "model_for_rfe = LogisticRegression(solver='liblinear', random_state=42)\n",
    "# n_features_to_select: Number of features to keep.\n",
    "# step: Number of features to remove at each iteration.\n",
    "rfe_selector = RFE(estimator=model_for_rfe, n_features_to_select=2, step=1)\n",
    "rfe_selector.fit(X_iris_train_scaled, y_iris_train) # Fit on training data\n",
    "\n",
    "selected_features_rfe = X_iris_train.columns[rfe_selector.support_]\n",
    "print(f\"Features selected by RFE (Logistic Regression, k=2): {list(selected_features_rfe)}\")\n",
    "print(f\"Feature ranking (lower is better): {rfe_selector.ranking_}\")\n",
    "\n",
    "# Transform data\n",
    "X_train_iris_rfe = rfe_selector.transform(X_iris_train_scaled)\n",
    "print(f\"Shape after RFE: {X_train_iris_rfe.shape}\")\n",
    "\n",
    "# b) RFECV: RFE with cross-validation to automatically find the optimal number of features.\n",
    "# print(\"\\n--- b) RFECV (RFE with Cross-Validation) ---\")\n",
    "# rfecv_selector = RFECV(estimator=model_for_rfe, step=1, cv=StratifiedKFold(3), scoring='accuracy')\n",
    "# rfecv_selector.fit(X_iris_train_scaled, y_iris_train)\n",
    "# print(f\"Optimal number of features found by RFECV: {rfecv_selector.n_features_}\")\n",
    "# selected_features_rfecv = X_iris_train.columns[rfecv_selector.support_]\n",
    "# print(f\"Features selected by RFECV: {list(selected_features_rfecv)}\")\n",
    "# plt.figure()\n",
    "# plt.plot(range(1, len(rfecv_selector.cv_results_['mean_test_score']) + 1), rfecv_selector.cv_results_['mean_test_score'])\n",
    "# plt.xlabel(\"Number of features selected\")\n",
    "# plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "# plt.title(\"RFECV Performance\")\n",
    "# plt.show()\n",
    "print(\"\\n(RFECV automatically finds the best number of features using CV - see commented code)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 4. Embedded Methods ---\n",
    "# Feature selection is an intrinsic part of the model training process.\n",
    "\n",
    "print(\"--- Embedded Methods ---\")\n",
    "\n",
    "# a) L1 Regularization (Lasso)\n",
    "# The L1 penalty forces some feature coefficients to become exactly zero.\n",
    "print(\"\\n--- a) L1 Regularization (Lasso) ---\")\n",
    "# Use Lasso for regression (housing data)\n",
    "lasso = Lasso(alpha=0.05, random_state=42) # Alpha controls regularization strength\n",
    "lasso.fit(X_housing_train_scaled, y_housing_train)\n",
    "\n",
    "lasso_coefs = pd.Series(lasso.coef_, index=X_housing_train.columns)\n",
    "print(\"Lasso Coefficients:\\n\", lasso_coefs)\n",
    "selected_features_lasso = lasso_coefs[lasso_coefs != 0].index\n",
    "print(f\"\\nFeatures selected by Lasso (non-zero coefs): {list(selected_features_lasso)}\")\n",
    "# Note: The noisy feature might be eliminated depending on alpha.\n",
    "\n",
    "# Can use LogisticRegression with penalty='l1' for classification.\n",
    "\n",
    "# b) Tree-based Feature Importance\n",
    "# Models like RandomForest calculate importance based on how much each feature\n",
    "# contributes to reducing impurity (e.g., Gini impurity) across all trees.\n",
    "print(\"\\n--- b) Tree-based Feature Importance ---\")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_iris_train_scaled, y_iris_train) # Fit on training data\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_iris_train.columns, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances from RandomForest:\\n\", feature_importance_df)\n",
    "\n",
    "# Select features based on importance threshold (e.g., keep features contributing > X%)\n",
    "# Or use SelectFromModel which does this automatically\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# sfm = SelectFromModel(rf, threshold=0.1, prefit=True) # Use prefit=True as rf is already fitted\n",
    "# X_train_iris_sfm = sfm.transform(X_iris_train_scaled)\n",
    "# print(f\"\\nShape after SelectFromModel (threshold=0.1): {X_train_iris_sfm.shape}\")\n",
    "# selected_features_sfm = X_iris_train.columns[sfm.get_support()]\n",
    "# print(f\"Features selected by SelectFromModel: {list(selected_features_sfm)}\")\n",
    "print(\"\\n(SelectFromModel can automatically select based on importance - see commented code)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 5. Final Considerations ---\n",
    "print(\"--- Final Considerations ---\")\n",
    "print(\"- Feature selection should generally be done *after* train-test split.\")\n",
    "print(\"- Fit selectors/models ONLY on the training data.\")\n",
    "print(\"- Transform both training and test sets using the *same* fitted selector.\")\n",
    "print(\"- Filter methods are fast but don't consider feature interactions.\")\n",
    "print(\"- Wrapper methods are more thorough but computationally expensive.\")\n",
    "print(\"- Embedded methods offer a balance, integrating selection into training.\")\n",
    "print(\"- The best method depends on the dataset, model, and goals.\")\n",
    "print(\"- Often beneficial to include feature selection within a Pipeline, especially when using cross-validation.\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7064327-d9d0-4db1-bd6f-1f32ce68d77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
